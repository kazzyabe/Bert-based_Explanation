{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../cases/cases used in study.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>problem.1</th>\n",
       "      <th>problem.2</th>\n",
       "      <th>problem.3</th>\n",
       "      <th>problem.4</th>\n",
       "      <th>problem.5</th>\n",
       "      <th>problem.6</th>\n",
       "      <th>problem.7</th>\n",
       "      <th>solution/binary classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query paper i</td>\n",
       "      <td>selected citations ij</td>\n",
       "      <td>W2V</td>\n",
       "      <td>D2V</td>\n",
       "      <td>surface similarity</td>\n",
       "      <td>paper sizein Bytes</td>\n",
       "      <td>publication type pair; 1 same type;0 different...</td>\n",
       "      <td>in which percentile (1 to 8) the citation occurs</td>\n",
       "      <td>final decision: 1 is B, 0 is S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q01</td>\n",
       "      <td>C0101</td>\n",
       "      <td>2.098</td>\n",
       "      <td>0.256289676</td>\n",
       "      <td>0.030031591</td>\n",
       "      <td>36240</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q01</td>\n",
       "      <td>C0102</td>\n",
       "      <td>2.117</td>\n",
       "      <td>0.096844971</td>\n",
       "      <td>0.028035853</td>\n",
       "      <td>32202</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q01</td>\n",
       "      <td>C0103</td>\n",
       "      <td>1.973</td>\n",
       "      <td>0.214411901</td>\n",
       "      <td>0.033382998</td>\n",
       "      <td>38899</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q01</td>\n",
       "      <td>C0104</td>\n",
       "      <td>2.235</td>\n",
       "      <td>0.115689246</td>\n",
       "      <td>0.021607683</td>\n",
       "      <td>18009</td>\n",
       "      <td>1</td>\n",
       "      <td>7000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         problem              problem.1 problem.2    problem.3  \\\n",
       "0  query paper i  selected citations ij       W2V          D2V   \n",
       "1            Q01                  C0101     2.098  0.256289676   \n",
       "2            Q01                  C0102     2.117  0.096844971   \n",
       "3            Q01                  C0103     1.973  0.214411901   \n",
       "4            Q01                  C0104     2.235  0.115689246   \n",
       "\n",
       "            problem.4           problem.5  \\\n",
       "0  surface similarity  paper sizein Bytes   \n",
       "1         0.030031591               36240   \n",
       "2         0.028035853               32202   \n",
       "3         0.033382998               38899   \n",
       "4         0.021607683               18009   \n",
       "\n",
       "                                           problem.6  \\\n",
       "0  publication type pair; 1 same type;0 different...   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  0   \n",
       "4                                                  1   \n",
       "\n",
       "                                          problem.7  \\\n",
       "0  in which percentile (1 to 8) the citation occurs   \n",
       "1                                              1000   \n",
       "2                                              1000   \n",
       "3                                              1000   \n",
       "4                                              7000   \n",
       "\n",
       "   solution/binary classification  \n",
       "0  final decision: 1 is B, 0 is S  \n",
       "1                               1  \n",
       "2                               1  \n",
       "3                               0  \n",
       "4                               0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../10 whole query articles txt/'\n",
    "# path2 = '../100 cited articles/'\n",
    "\n",
    "# files = []\n",
    "# # r=root, d=directories, f = files\n",
    "# for r, d, f in os.walk(path2):\n",
    "#     print(r)\n",
    "#     print(d)\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"../100 cited articles/C1005.txt\")\n",
    "# tmp = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.unique(data['problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'query paper i'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     Q01\n",
       "2     Q01\n",
       "3     Q01\n",
       "4     Q01\n",
       "5     Q01\n",
       "6     Q01\n",
       "7     Q01\n",
       "8     Q01\n",
       "9     Q01\n",
       "10    Q01\n",
       "Name: problem, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['problem'] == 'Q01']['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      query paper i\n",
       "1              hello\n",
       "2              hello\n",
       "3              hello\n",
       "4              hello\n",
       "           ...      \n",
       "96               Q10\n",
       "97               Q10\n",
       "98               Q10\n",
       "99               Q10\n",
       "100              Q10\n",
       "Name: problem, Length: 101, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['problem'].replace('Q01', 'hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace query papers with actual contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_problem = pd.unique(data['problem'])\n",
    "for u in uni_problem[1:]:\n",
    "    f = open('../10 whole query articles txt/'+u+'.txt')\n",
    "    tmp = f.read()\n",
    "    data['problem'] = data['problem'].replace(u, tmp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>problem.1</th>\n",
       "      <th>problem.2</th>\n",
       "      <th>problem.3</th>\n",
       "      <th>problem.4</th>\n",
       "      <th>problem.5</th>\n",
       "      <th>problem.6</th>\n",
       "      <th>problem.7</th>\n",
       "      <th>solution/binary classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query paper i</td>\n",
       "      <td>selected citations ij</td>\n",
       "      <td>W2V</td>\n",
       "      <td>D2V</td>\n",
       "      <td>surface similarity</td>\n",
       "      <td>paper sizein Bytes</td>\n",
       "      <td>publication type pair; 1 same type;0 different...</td>\n",
       "      <td>in which percentile (1 to 8) the citation occurs</td>\n",
       "      <td>final decision: 1 is B, 0 is S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>C0101</td>\n",
       "      <td>2.098</td>\n",
       "      <td>0.256289676</td>\n",
       "      <td>0.030031591</td>\n",
       "      <td>36240</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>C0102</td>\n",
       "      <td>2.117</td>\n",
       "      <td>0.096844971</td>\n",
       "      <td>0.028035853</td>\n",
       "      <td>32202</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>C0103</td>\n",
       "      <td>1.973</td>\n",
       "      <td>0.214411901</td>\n",
       "      <td>0.033382998</td>\n",
       "      <td>38899</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>C0104</td>\n",
       "      <td>2.235</td>\n",
       "      <td>0.115689246</td>\n",
       "      <td>0.021607683</td>\n",
       "      <td>18009</td>\n",
       "      <td>1</td>\n",
       "      <td>7000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>C1006</td>\n",
       "      <td>2.247</td>\n",
       "      <td>0.276550423</td>\n",
       "      <td>0.028751456</td>\n",
       "      <td>30707</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>C1007</td>\n",
       "      <td>2.218</td>\n",
       "      <td>0.188079829</td>\n",
       "      <td>0.030792555</td>\n",
       "      <td>34563</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>C1008</td>\n",
       "      <td>2.206</td>\n",
       "      <td>0.214281735</td>\n",
       "      <td>0.032667587</td>\n",
       "      <td>31950</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>C1009</td>\n",
       "      <td>2.261</td>\n",
       "      <td>0.137207324</td>\n",
       "      <td>0.034078566</td>\n",
       "      <td>54274</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>C1010</td>\n",
       "      <td>2.263</td>\n",
       "      <td>0.256527846</td>\n",
       "      <td>0.033144238</td>\n",
       "      <td>33624</td>\n",
       "      <td>1</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               problem              problem.1  \\\n",
       "0                                        query paper i  selected citations ij   \n",
       "1     Lexicon-Based Methods for Sentiment Analysis ...                  C0101   \n",
       "2     Lexicon-Based Methods for Sentiment Analysis ...                  C0102   \n",
       "3     Lexicon-Based Methods for Sentiment Analysis ...                  C0103   \n",
       "4     Lexicon-Based Methods for Sentiment Analysis ...                  C0104   \n",
       "..                                                 ...                    ...   \n",
       "96    Learning Subjective Language  Theresa Wilson ...                  C1006   \n",
       "97    Learning Subjective Language  Theresa Wilson ...                  C1007   \n",
       "98    Learning Subjective Language  Theresa Wilson ...                  C1008   \n",
       "99    Learning Subjective Language  Theresa Wilson ...                  C1009   \n",
       "100   Learning Subjective Language  Theresa Wilson ...                  C1010   \n",
       "\n",
       "    problem.2    problem.3           problem.4           problem.5  \\\n",
       "0         W2V          D2V  surface similarity  paper sizein Bytes   \n",
       "1       2.098  0.256289676         0.030031591               36240   \n",
       "2       2.117  0.096844971         0.028035853               32202   \n",
       "3       1.973  0.214411901         0.033382998               38899   \n",
       "4       2.235  0.115689246         0.021607683               18009   \n",
       "..        ...          ...                 ...                 ...   \n",
       "96      2.247  0.276550423         0.028751456               30707   \n",
       "97      2.218  0.188079829         0.030792555               34563   \n",
       "98      2.206  0.214281735         0.032667587               31950   \n",
       "99      2.261  0.137207324         0.034078566               54274   \n",
       "100     2.263  0.256527846         0.033144238               33624   \n",
       "\n",
       "                                             problem.6  \\\n",
       "0    publication type pair; 1 same type;0 different...   \n",
       "1                                                    1   \n",
       "2                                                    1   \n",
       "3                                                    0   \n",
       "4                                                    1   \n",
       "..                                                 ...   \n",
       "96                                                   0   \n",
       "97                                                   1   \n",
       "98                                                   1   \n",
       "99                                                   0   \n",
       "100                                                  1   \n",
       "\n",
       "                                            problem.7  \\\n",
       "0    in which percentile (1 to 8) the citation occurs   \n",
       "1                                                1000   \n",
       "2                                                1000   \n",
       "3                                                1000   \n",
       "4                                                7000   \n",
       "..                                                ...   \n",
       "96                                               2000   \n",
       "97                                               2000   \n",
       "98                                               2000   \n",
       "99                                               1000   \n",
       "100                                              6000   \n",
       "\n",
       "     solution/binary classification  \n",
       "0    final decision: 1 is B, 0 is S  \n",
       "1                                 1  \n",
       "2                                 1  \n",
       "3                                 0  \n",
       "4                                 0  \n",
       "..                              ...  \n",
       "96                                1  \n",
       "97                                1  \n",
       "98                                0  \n",
       "99                                1  \n",
       "100                               1  \n",
       "\n",
       "[101 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace citations with actual contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_citation = pd.unique(data['problem.1'])\n",
    "for u in uni_citation[1:]:\n",
    "    f = open('../100 cited articles/'+u+'.txt')\n",
    "    tmp = f.read()\n",
    "    data['problem.1'] = data['problem.1'].replace(u, tmp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>problem.1</th>\n",
       "      <th>problem.2</th>\n",
       "      <th>problem.3</th>\n",
       "      <th>problem.4</th>\n",
       "      <th>problem.5</th>\n",
       "      <th>problem.6</th>\n",
       "      <th>problem.7</th>\n",
       "      <th>solution/binary classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query paper i</td>\n",
       "      <td>selected citations ij</td>\n",
       "      <td>W2V</td>\n",
       "      <td>D2V</td>\n",
       "      <td>surface similarity</td>\n",
       "      <td>paper sizein Bytes</td>\n",
       "      <td>publication type pair; 1 same type;0 different...</td>\n",
       "      <td>in which percentile (1 to 8) the citation occurs</td>\n",
       "      <td>final decision: 1 is B, 0 is S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>Amazon Mechanical Turk for Subjectivity Word ...</td>\n",
       "      <td>2.098</td>\n",
       "      <td>0.256289676</td>\n",
       "      <td>0.030031591</td>\n",
       "      <td>36240</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>Mining WordNet for Fuzzy Sentiment:  Sentimen...</td>\n",
       "      <td>2.117</td>\n",
       "      <td>0.096844971</td>\n",
       "      <td>0.028035853</td>\n",
       "      <td>32202</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>When Specialists and Generalists Work Togethe...</td>\n",
       "      <td>1.973</td>\n",
       "      <td>0.214411901</td>\n",
       "      <td>0.033382998</td>\n",
       "      <td>38899</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>Distilling Opinion in Discourse: A Preliminar...</td>\n",
       "      <td>2.235</td>\n",
       "      <td>0.115689246</td>\n",
       "      <td>0.021607683</td>\n",
       "      <td>18009</td>\n",
       "      <td>1</td>\n",
       "      <td>7000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Similarity-Based Estimation of Word Cooccurre...</td>\n",
       "      <td>2.247</td>\n",
       "      <td>0.276550423</td>\n",
       "      <td>0.028751456</td>\n",
       "      <td>30707</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Recognizing Expressions of Commonsense Psycho...</td>\n",
       "      <td>2.218</td>\n",
       "      <td>0.188079829</td>\n",
       "      <td>0.030792555</td>\n",
       "      <td>34563</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Automatic Detection of Text Genre  Xerox Palo...</td>\n",
       "      <td>2.206</td>\n",
       "      <td>0.214281735</td>\n",
       "      <td>0.032667587</td>\n",
       "      <td>31950</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Building a Large Annotated Corpus of English:...</td>\n",
       "      <td>2.261</td>\n",
       "      <td>0.137207324</td>\n",
       "      <td>0.034078566</td>\n",
       "      <td>54274</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Proceedings of the 40th Annual Meeting of the...</td>\n",
       "      <td>2.263</td>\n",
       "      <td>0.256527846</td>\n",
       "      <td>0.033144238</td>\n",
       "      <td>33624</td>\n",
       "      <td>1</td>\n",
       "      <td>6000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               problem  \\\n",
       "0                                        query paper i   \n",
       "1     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "2     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "3     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "4     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "..                                                 ...   \n",
       "96    Learning Subjective Language  Theresa Wilson ...   \n",
       "97    Learning Subjective Language  Theresa Wilson ...   \n",
       "98    Learning Subjective Language  Theresa Wilson ...   \n",
       "99    Learning Subjective Language  Theresa Wilson ...   \n",
       "100   Learning Subjective Language  Theresa Wilson ...   \n",
       "\n",
       "                                             problem.1 problem.2    problem.3  \\\n",
       "0                                selected citations ij       W2V          D2V   \n",
       "1     Amazon Mechanical Turk for Subjectivity Word ...     2.098  0.256289676   \n",
       "2     Mining WordNet for Fuzzy Sentiment:  Sentimen...     2.117  0.096844971   \n",
       "3     When Specialists and Generalists Work Togethe...     1.973  0.214411901   \n",
       "4     Distilling Opinion in Discourse: A Preliminar...     2.235  0.115689246   \n",
       "..                                                 ...       ...          ...   \n",
       "96    Similarity-Based Estimation of Word Cooccurre...     2.247  0.276550423   \n",
       "97    Recognizing Expressions of Commonsense Psycho...     2.218  0.188079829   \n",
       "98    Automatic Detection of Text Genre  Xerox Palo...     2.206  0.214281735   \n",
       "99    Building a Large Annotated Corpus of English:...     2.261  0.137207324   \n",
       "100   Proceedings of the 40th Annual Meeting of the...     2.263  0.256527846   \n",
       "\n",
       "              problem.4           problem.5  \\\n",
       "0    surface similarity  paper sizein Bytes   \n",
       "1           0.030031591               36240   \n",
       "2           0.028035853               32202   \n",
       "3           0.033382998               38899   \n",
       "4           0.021607683               18009   \n",
       "..                  ...                 ...   \n",
       "96          0.028751456               30707   \n",
       "97          0.030792555               34563   \n",
       "98          0.032667587               31950   \n",
       "99          0.034078566               54274   \n",
       "100         0.033144238               33624   \n",
       "\n",
       "                                             problem.6  \\\n",
       "0    publication type pair; 1 same type;0 different...   \n",
       "1                                                    1   \n",
       "2                                                    1   \n",
       "3                                                    0   \n",
       "4                                                    1   \n",
       "..                                                 ...   \n",
       "96                                                   0   \n",
       "97                                                   1   \n",
       "98                                                   1   \n",
       "99                                                   0   \n",
       "100                                                  1   \n",
       "\n",
       "                                            problem.7  \\\n",
       "0    in which percentile (1 to 8) the citation occurs   \n",
       "1                                                1000   \n",
       "2                                                1000   \n",
       "3                                                1000   \n",
       "4                                                7000   \n",
       "..                                                ...   \n",
       "96                                               2000   \n",
       "97                                               2000   \n",
       "98                                               2000   \n",
       "99                                               1000   \n",
       "100                                              6000   \n",
       "\n",
       "     solution/binary classification  \n",
       "0    final decision: 1 is B, 0 is S  \n",
       "1                                 1  \n",
       "2                                 1  \n",
       "3                                 0  \n",
       "4                                 0  \n",
       "..                              ...  \n",
       "96                                1  \n",
       "97                                1  \n",
       "98                                0  \n",
       "99                                1  \n",
       "100                               1  \n",
       "\n",
       "[101 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b = data.loc[1:, ['problem', 'problem.1', 'solution/binary classification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>problem.1</th>\n",
       "      <th>solution/binary classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query paper i</td>\n",
       "      <td>selected citations ij</td>\n",
       "      <td>final decision: 1 is B, 0 is S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         problem              problem.1  solution/binary classification\n",
       "0  query paper i  selected citations ij  final decision: 1 is B, 0 is S"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0:0,['problem', 'problem.1', 'solution/binary classification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b.columns = ['query', 'citation', 'decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>citation</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>Amazon Mechanical Turk for Subjectivity Word ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>Mining WordNet for Fuzzy Sentiment:  Sentimen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>When Specialists and Generalists Work Togethe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>Distilling Opinion in Discourse: A Preliminar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lexicon-Based Methods for Sentiment Analysis ...</td>\n",
       "      <td>SENTIWORDNET 3.0: An Enhanced Lexical Resourc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Similarity-Based Estimation of Word Cooccurre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Recognizing Expressions of Commonsense Psycho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Automatic Detection of Text Genre  Xerox Palo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Building a Large Annotated Corpus of English:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Learning Subjective Language  Theresa Wilson ...</td>\n",
       "      <td>Proceedings of the 40th Annual Meeting of the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 query  \\\n",
       "1     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "2     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "3     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "4     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "5     Lexicon-Based Methods for Sentiment Analysis ...   \n",
       "..                                                 ...   \n",
       "96    Learning Subjective Language  Theresa Wilson ...   \n",
       "97    Learning Subjective Language  Theresa Wilson ...   \n",
       "98    Learning Subjective Language  Theresa Wilson ...   \n",
       "99    Learning Subjective Language  Theresa Wilson ...   \n",
       "100   Learning Subjective Language  Theresa Wilson ...   \n",
       "\n",
       "                                              citation decision  \n",
       "1     Amazon Mechanical Turk for Subjectivity Word ...        1  \n",
       "2     Mining WordNet for Fuzzy Sentiment:  Sentimen...        1  \n",
       "3     When Specialists and Generalists Work Togethe...        0  \n",
       "4     Distilling Opinion in Discourse: A Preliminar...        0  \n",
       "5     SENTIWORDNET 3.0: An Enhanced Lexical Resourc...        1  \n",
       "..                                                 ...      ...  \n",
       "96    Similarity-Based Estimation of Word Cooccurre...        1  \n",
       "97    Recognizing Expressions of Commonsense Psycho...        1  \n",
       "98    Automatic Detection of Text Genre  Xerox Palo...        0  \n",
       "99    Building a Large Annotated Corpus of English:...        1  \n",
       "100   Proceedings of the 40th Annual Meeting of the...        1  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_b.loc(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit = test[41]['query']\n",
    "type(cit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  Introduction  The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews. Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining. Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships. This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions in online news articles.  Identifying opinion holders is important especially in news articles. Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries). By grouping opinion holders of different stance on diverse social and political issues, we can a have better understanding of the relationships among countries or among organizations.  An opinion topic can be considered as an object an opinion is about. In product reviews, for example, such opinion topics are often the product itself or its specific features, such as design and quality (e.g. I like the design of iPod video, The sound quality is amazing). In news articles, opinion topics can be social issues, government\\'s acts, new events, or someone\\'s opinions. (e.g., \\'Democrats in Congress accused vice president Dick Cheney\\'s shooting accident., Shiite leaders accused Sunnis of a mass killing of Shiites in Madaen, south of Baghdad.)  As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews. In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points. In this study, we extract opinion topics in news articles. Also, we do not pre-limit topics in advance. We first identify an opinion and then find its holder and topic. We define holder as an entity who holds an opinion, and topic, as what the opinion is about.  In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence. We decompose the overall task into the following steps:  Identify opinions.  Label semantic roles related to the opinions.  Find holders and topics of opinions among the identified semantic roles.  Store <opinion, holder, topic> triples into a database.  In this paper, we focus on the first three subtasks.  The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics. To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training. We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification.  This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling. Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet1 data for our task. Section 4 reports our experiments and results with discussions and finally Section 5 concludes.  2 Related Work  This section reviews previous works in both sentiment detection and semantic role labeling.  2.1 Subjectivity and Sentiment Detection  Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002).  Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their  work is similar to ours but different because their opinion is restricted to propositional opinion and mostly to verbs. Another related works are (Choi et al., 2005; Kim and Hovy, 2005). Both of them use the MPQA corpus2 but they only identify opinion holders, not topics.  As for opinion topic identification, few researches have been conducted, and only in a very limited domain, product reviews. (Hu and Liu, 2004; Popescu and Etzioni, 2005) present product mining algorithms with extracting certain product features given specific product types. Our paper aims at extracting topics of opinion in general news media text.  2.2 Semantic Role Labeling  Semantic role labeling is the task of identifying semantic roles such as Agent, Patient, Speaker, or Topic, in a sentence. A statistical approach for semantic role labeling was introduced by (Gildea and Jurafsky, 2002). Their system learned semantic relationship among constituents in a sentence from FrameNet, a large corpus of semantically hand-annotated data. The FrameNet annotation scheme is based on Frame Semantics (Fillmore, 1976). Frames are defined as schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles. For example, given a sentence Jack built a new house out of bricks, a semantic role labeling system should identify the roles for the verb built such as [Agent Jack] built [Created_entity a new house] [Component out of bricks]3. In our study, we build a semantic role labeling system as an intermediate step to label opinion holders and topics by training it on opinion-bearing frames and their frame elements in FrameNet.  3 Finding Opinions and Their Holders and Topics  For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data. The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence. Given a sentence and an opinion bearing word, our method identifies frame elements in the sentence and  2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 3 The verb build is defined under the frame Building in which Agent, Created_entity, and Components are defined as frame elements.  Table 1: Example of opinion related frames and lexical units  want, wish, hope, Event,  interested, Location_of_event  Emotion _directed agitated, amused, anguish, ashamed, angry, annoyed, Event, Topic Experiencer, Expressor,  Mental _property absurd, brilliant, careless, crazy, cunning, foolish Behavior, Protagonist, Domain, Degree  Subject _stimulus delightful, amazing, annoying, amusing, aggravating, Stimulus, Degree Experiencer, Circumstances,  Figure 1: An overview of our algorithm  searches which frame element corresponds to the opinion holder and which to the topic. The example in Figure 1 shows the intuition of our algorithm.  We decompose our task in 3 subtasks: (1) collect opinion words and opinion-related frames,  semantic role labeling for those frames, and  finally map semantic roles to holder and topic. Following subsections describe each subtask.  3.1 Opinion Words and Related Frames  We describe the subtask of collecting opinion words and related frames in 3 phases.  Phase 1: Collect Opinion Words  In this study, we consider an opinion-bearing (positive/negative) word is a key indicator of an opinion. Therefore, we first identify opinion-bearing word from a given sentence and extract its holder and topic. Since previous studies indicate that opinion-bearing verbs and adjectives are especially efficient for opinion identification, we focus on creating a set of opinion-bearing verbs and adjectives. We annotated 1860 adjectives and 2011 verbs4 by classifying them into positive, negative, and neutral classes. Words in the positive class carry positive valence whereas those in negative class carry negative valence. Words that are not opinion-bearing are classified as neutral.  Note that in our study we treat word sentiment classification as a three-way classification problem instead of a two-way classification problem  (i.e. positive and negative). By adding the third class, neutral, we can prevent the classifier assigning either positive or negative sentiment to weak opinion-bearing word. For example, the word central that Hatzivassiloglou and McKeown (1997) marked as a positive adjective is not classified as positive by our system. Instead we mark it as neutral, since it is a weak clue for an opinion. For the same reason, we did not consider able classified as a positive word by General Inquirer5, a sentiment word lexicon, as a positive opinion indicator. Finally, we collected 69 positive and 151 negative verbs and 199 positive and 304 negative adjectives.  Phase 2: Find Opinion-related Frames  We collected frames related to opinion words from the FrameNet corpus. We used FrameNet II (Baker et al., 2003) which contains 450 semantic frames and more than 3000 frame elements (FE). A frame consists of lexical items, called Lexical Unit (LU), and related frame elements. For instance, LUs in ATTACK frame are verbs such as assail, assault, and attack, and nouns such as invasion, raid, and strike. FrameNet II contains  4 These were randomly selected from 8011 English verbs and 19748 English adjectives. 5 http://www.wjh.harvard.edu/~inquirer/homecat.htm  approximately 7500 lexical units and over 100,000 annotated sentences.  For each word in our opinion word set described in Phase 1, we find a frame to which the word belongs. 49 frames for verbs and 43 frames for adjectives are collected. Table 1 shows examples of selected frames with some of the lexical units those frames cover. For example, our system found the frame Desiring from opinion-bearing words want, wish, hope, etc. Finally, we collected 8256 and 11877 sentences related to selected opinion bearing frames for verbs and adjectives respectively.  Phase 3: FrameNet expansion  Even though Phase 2 searches for a correlated frame for each verb and adjective in our opinion-bearing word list, not all of them are defined in FrameNet data. Some words such as criticize and harass in our list have associated frames (Case 1), whereas others such as vilify and maltreat do not have those (Case 2). For a word in Case 2, we use a clustering algorithms CBC (Clustering By Committee) to predict the closest (most reasonable) frame of undefined word from existing frames. CBC (Pantel and Lin, 2002) was developed based on the distributional hypothesis (Harris, 1954) that words which occur in the same contexts tend to be similar. Using CBC, for example, our clustering module computes lexical similarity between the word vilify in Case 2 and all words in Case 1. Then it picks criticize as a similar word, so that we can use for vilify the frame Judgment_communication to which criticize belongs and all frame elements defined under Judgment_ communication.  3.2 Semantic Role Labeling  To find a potential holder and topic of an opinion word in a sentence, we first label semantic roles in a sentence.  Modeling: We follow the statistical approaches for semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et. al, 2003) which separate the task into two steps: identify candidates of frame elements (Step 1) and assign semantic roles for those candidates (Step 2). Like their intuition, we treated both steps as classification problems. We first collected all constituents of the given sentence by parsing it using the Charniak parser. Then, in Step 1, we classified candidate constituents of frame elements from non-candidates. In Step 2, each selected candidate was thus classified into one of frame ele-Table 2: Features used for our semantic role labeling model.  target word A predicate whose meaning represents the frame (a verb or an adjective in our task)  phrase type Syntactic type of the frame element (e.g. NP, PP)  head word Syntactic head of the frame element phrase  parse tree path A path between the frame element and target word in the parse tree  position Whether the element phrase occurs before or after the target word  voice The voice of the sentence (active or passive)  frame name one of our opinion-related frames  ment types (e.g. Stimulus, Degree, Experiencer, etc.). As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al., 1996). For system development, we used MEGA model optimization package6, an implementation of ME models.  Data: We collected 8256 and 11877 sentences which were associated to opinion bearing frames for verbs and adjectives from FrameNet annotation data. Each sentence in our dataset contained a frame name, a target predicate (a word whose meaning represents aspects of the frame), and frame elements labeled with element types. We divided the data into 90% for training and 10% for test.  Features used: Table 2 describes features that we used for our classification model. The target word is an opinion-bearing verb or adjective which is associated to a frame. We used the Charniak parser to get a phrase type feature of a frame element and the parse tree path feature. We determined a head word of a phrase by an algorithm using a tree head table7, position feature by the order of surface words of a frame element and the target word, and the voice feature by a simple pattern. Frame name for a target  Table 3. Precision (P), Recall (R), and F-score (F) of Topic and Holder identification for opinion verbs (V) and adjectives (A) on Testset 1.  Topic Holder  Topic Holder  word was selected by methods described in Phase 2 and Phase 3 in Subsection 3.1.  3.3 Map Semantic Roles to Holder and Topic  After identifying frame elements in a sentence, our system finally selects holder and topic from those frame elements. In the example in Table 1, the frame Desiring has frame elements such as Event (The change that the Experiencer would like to see), Experiencer (the person or sentient being who wishes for the Event to occur), Location_of_event (the place involved in the desired Event), Focal_participant (entity that the Experiencer wishes to be affected by some Event). Among these FEs, we can consider that Experiencer can be a holder and Focal_participant can be a topic (if any exists in a sentence). We manually built a mapping table to map FEs to holder or topic using as support the FE definitions in each opinion related frame and the annotated sample sentences.  The goal of our experiment is first, to see how our holder and topic labeling system works on the FrameNet data, and second, to examine how it performs on online news media text. The first data set (Testset 1) consists of 10% of data described in Subsection 3.2 and the second (Testset 2) is manually annotated by 2 humans. (see Subsection 4.2). We report experimental results for both test sets.  Gold Standard: In total, Testset 1 contains 2028 annotated sentences collected from FrameNet data set. (834 from frames related to opinion verb and 1194 from opinion adjectives) We measure the system performance using precision (the percentage of correct holders/topics among system\\'s labeling results), recall (the percentage of correct holders/topics that system retrieved), and F-score.  Baseline: For the baseline system, we applied two different algorithms for sentences which have opinion-bearing verbs as target words and for those that have opinion-bearing adjectives as target words. For verbs, baseline system labeled a subject of a verb as a holder and an object as a topic. (e.g. [holder He] condemned [topic the lawyer].) For adjectives, the baseline marked the subject of a predicate adjective as a holder (e.g. [holder I] was happy). For the topics of adjectives, the baseline picks a modified word if the target adjective is a modifier (e.g. That was a stupid [topic mistake].) and a subject word if the adjective is a predicate. ([topic The view] is breathtaking in January.)  Result: Table 3 and 4 show evaluation results of our system and the baseline system respectively. Our system performed much better than the baseline system in identifying topic and holder for both sets of sentences with verb target words and those with adjectives. Especially in recognizing topics of target opinion-bearing words, our system improved F-score from 30.4% to 66.5% for verb target words and from 38.2% to 70.3% for adjectives. It was interesting to see that the intuition that A subject of opinion-bearing verb is a holder and an object is a topic which we applied for the baseline achieved relatively good F-score (56.9%). However, our system obtained much higher F-score (78.7%). Holder identification task achieved higher F-score than topic identification which implies that identifying topics of opinion is a harder task.  We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics that simple relations such as subject and object relations are not able to capture. For example, in a sentence Her letter upset me, simply looking for the subjective and objective of the verb upset is not enough to recognize the holder and topic. It is necessary to see a deeper level of semantic relaTable 5. Opinion-bearing sentence identification on Testset 2. (P: precision, R: recall, F: F-score, A: Accuracy, H1: Human1, H2: Human2)  Table 6: Results of Topic and Holder identification on Testset 2. (Sys: our system, BL: baseline)  Topic Holder  tions: Her letter is a stimulus and me is an experiencer of the verb upset.  Gold Standard: Two humans 8 annotated 100 sentences randomly selected from news media texts. Those news data is collected from online news sources such as The New York Times, UN Office for the Coordination of Humanitarian Affairs, and BBC News9, which contain articles about various international affaires. Annotators identified opinion-bearing sentences with marking opinion word with its holder and topic if they existed. The inter-annotator agreement in identifying opinion sentences was 82%.  Baseline: In order to identify opinion-bearing sentences for our baseline system, we used the opinion-bearing word set introduced in Phase 1 in Subsection 3.1. If a sentence contains an opinion-bearing verb or adjective, the baseline system started looking for its holder and topic. For holder and topic identification, we applied the  8 We refer them as Human1 and Human2 for the rest of this paper.9 www.nytimes.com, www.irinnews.org, and www.bbc.co.uk  same baseline algorithm as described in Subsection 4.1 to Testset 2.  Result: Note that Testset 1 was collected from sentences of opinion-related frames in FrameNet and therefore all sentences in the set contained either opinion-bearing verb or adjective. (i.e. All sentences are opinion-bearing) However, sentences in Testset 2 were randomly collected from online news media pages and therefore not all of them are opinion-bearing. We first evaluated the task of opinion-bearing sentence identification. Table 5 shows the system results. When we mark all sentences as opinion-bearing, it achieved 43% and 38% of accuracy for the annotation result of Human1 and Human2 respectively. Our system performance (64% and 55%) is comparable with the unique assignment.  We measured the holder and topic identification system with precision, recall, and F-score. As we can see from Table 6, our system achieved much higher precision than the baseline system for both Topic and Holder identification tasks. However, we admit that there is still a lot of room for improvement.  The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification. In overall, our system attained higher F-score in holder identification task, including the baseline system. Based on F-score, we believe that identifying topics of opinion is much more difficult than identifying holders. It was interesting to see the same phenomenon that the baseline system mainly assuming that subject and object of a sentence are likely to be opinion holder and topic, achieved lower scores for both holder and topic identification tasks in Testset 2 as in Testset 1. This implies that more sophisticated analysis of the relationship between opinion words (e.g. verbs and adjectives) and their topics and holders is crucial.  4.3 Difficulties in evaluation  We observed several difficulties in evaluating holder and topic identification. First, the boundary of an entity of holder or topic can be flexible. For example, in sentence Senator Titus Olupitan who sponsored the bill wants the permission., not only Senator Titus Olupitan but also Senator Titus Olupitan who sponsored the bill is an eligible answer. Second, some correct holders and topics which our system found were evaluated wrong even if they referred the same entities in the gold standard because human annotators marked only one of them as an answer.  In the future, we need more annotated data for improved evaluation.  Conclusion and Future Work  This paper presented a methodology to identify an opinion with its holder and topic given a sentence in online news media texts. We introduced an approach of exploiting semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. Our method first identifies an opinion-bearing word, labels semantic roles related to the word in the sentence, and then finds a holder and a topic of the opinion word among labeled semantic roles.  There has been little previous study in identifying opinion holders and topics partly because it requires a great amount of annotated data. To overcome this barrier, we utilized FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics. However, FrameNet has a limited number of words in its annotated corpus. For a broader coverage, we used a clustering technique to predict a most probable frame for an unseen word.  Our experimental results showed that our system performs significantly better than the baseline. The baseline system results imply that opinion holder and topic identification is a hard task. We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics which simple relations such as subject and object relations are not able to capture.  In the future, we plan to extend our list of opinion-bearing verbs and adjectives so that we can discover and apply more opinion-related frames. Also, it would be interesting to see how other types of part of speech such as adverbs and nouns affect the performance of the system.  Reference  Baker, Collin F. and Hiroaki Sato. 2003. The FrameNet Data and Software. Poster and Demonstration at Association for Computational Linguistics. Sapporo, Japan.  Berger, Adam, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing, Computational Linguistics, (22-1).  Bethard, Steven, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky.  2004. Automatic Extraction of Opinion Propositions and their Holders, AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.  Choi, Y., Cardie, C., Riloff, E., and Patwardhan,  S. 2005. Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns. Proceedings of HLT/EMNLP-05.  Esuli, Andrea and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classification. Proceedings of CIKM-05, 14th ACM International Conference on Information and Knowledge Management, Bremen, DE, pp. 617-624.  Fillmore, C. Frame semantics and the nature of language. 1976. In Annals of the New York Academy of Sciences: Conferences on the Origin and Development of Language and Speech, Volume 280: 20-32.  Fleischman, Michael, Namhee Kwon, and Eduard Hovy. 2003. Maximum Entropy Models for FrameNet Classification. Proceedings of EMNLP, Sapporo, Japan.  Gildea, D. and Jurafsky, D. Automatic Labeling of semantic roles. 2002. In Computational Linguistics. 28(3), 245-288.  Harris, Zellig, 1954. Distributional structure. Word, 10(23) :146--162.  Hatzivassiloglou, Vasileios and Kathleen McKeown. 1997. Predicting the Semantic Orientation of Adjectives. Proceedings of 35th Annual Meeting of the Assoc. for Computational Linguistics (ACL-97): 174-181  Hatzivassiloglou, Vasileios and Wiebe, Janyce. 2000. Effects of Adjective Orientation and Gradability on Sentence Subjectivity. Proceedings of International Conference on Computational Linguistics (COLING-2000). Saarbrcken, Germany.  Hu, Minqing and Bing Liu. 2004. Mining and summarizing customer reviews\". Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD-2004), Seattle, Washington, USA.  Kim, Soo-Min and Eduard Hovy. 2004. Determining the Sentiment of Opinions. Proceedings of COLING-04. pp. 1367-1373. Geneva, Switzerland.  Kim, Soo-Min and Eduard Hovy. 2005. Identifying Opinion Holders for Question Answering  in Opinion Texts. Proceedings of AAAI-05 Workshop on Question Answering in Restricted Domains  Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques, Proceedings of EMNLP-2002.  Pantel, Patrick and Dekang Lin. 2002. Discovering Word Senses from Text. Proceedings of ACM Conference on Knowledge Discovery and Data Mining. (KDD-02). pp. 613-619. Edmonton, Canada.  Popescu, Ana-Maria and Oren Etzioni. 2005. Extracting Product Features and Opinions from Reviews , Proceedings of HLT-EMNLP 2005.  Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 2003. Learning Subjective Nouns Using Extraction Pattern Bootstrapping. Proceedings of Seventh Conference on Natural Language Learning (CoNLL-03). ACL SIGNLL. Pages 25-32.  Turney, Peter D. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews, Proceedings of ACL-02, Philadelphia, Pennsylvania, 417-424  Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P. O\\'Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. Proceedings of ACL-99. University of Maryland, June, pp. 246-253.  Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proceedings of HLT/EMNLP 2005, Vancouver, Canada '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = re.split(\"\\d*[^a-zA-z] Introduction\", cit)\n",
    "# s = re.findall(\" ?\\d?[^a-zA-z]? ?Introduction\", cit)\n",
    "s = re.split(\"  \", cit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text',\n",
       " 'Soo-Min Kim and Eduard Hovy',\n",
       " 'USC Information Sciences Institute',\n",
       " '4676 Admiralty Way',\n",
       " 'This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.',\n",
       " 'Introduction',\n",
       " 'The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews. Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining. Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships. This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions in online news articles.',\n",
       " 'Identifying opinion holders is important especially in news articles. Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries). By grouping opinion holders of different stance on diverse social and political issues, we can a have better understanding of the relationships among countries or among organizations.',\n",
       " \"An opinion topic can be considered as an object an opinion is about. In product reviews, for example, such opinion topics are often the product itself or its specific features, such as design and quality (e.g. I like the design of iPod video, The sound quality is amazing). In news articles, opinion topics can be social issues, government's acts, new events, or someone's opinions. (e.g., 'Democrats in Congress accused vice president Dick Cheney's shooting accident., Shiite leaders accused Sunnis of a mass killing of Shiites in Madaen, south of Baghdad.)\",\n",
       " 'As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews. In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points. In this study, we extract opinion topics in news articles. Also, we do not pre-limit topics in advance. We first identify an opinion and then find its holder and topic. We define holder as an entity who holds an opinion, and topic, as what the opinion is about.',\n",
       " 'In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence. We decompose the overall task into the following steps:',\n",
       " 'Identify opinions.',\n",
       " 'Label semantic roles related to the opinions.',\n",
       " 'Find holders and topics of opinions among the identified semantic roles.',\n",
       " 'Store <opinion, holder, topic> triples into a database.',\n",
       " 'In this paper, we focus on the first three subtasks.',\n",
       " 'The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics. To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training. We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification.',\n",
       " 'This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling. Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet1 data for our task. Section 4 reports our experiments and results with discussions and finally Section 5 concludes.',\n",
       " '2 Related Work',\n",
       " 'This section reviews previous works in both sentiment detection and semantic role labeling.',\n",
       " '2.1 Subjectivity and Sentiment Detection',\n",
       " 'Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002).',\n",
       " 'Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their',\n",
       " 'work is similar to ours but different because their opinion is restricted to propositional opinion and mostly to verbs. Another related works are (Choi et al., 2005; Kim and Hovy, 2005). Both of them use the MPQA corpus2 but they only identify opinion holders, not topics.',\n",
       " 'As for opinion topic identification, few researches have been conducted, and only in a very limited domain, product reviews. (Hu and Liu, 2004; Popescu and Etzioni, 2005) present product mining algorithms with extracting certain product features given specific product types. Our paper aims at extracting topics of opinion in general news media text.',\n",
       " '2.2 Semantic Role Labeling',\n",
       " 'Semantic role labeling is the task of identifying semantic roles such as Agent, Patient, Speaker, or Topic, in a sentence. A statistical approach for semantic role labeling was introduced by (Gildea and Jurafsky, 2002). Their system learned semantic relationship among constituents in a sentence from FrameNet, a large corpus of semantically hand-annotated data. The FrameNet annotation scheme is based on Frame Semantics (Fillmore, 1976). Frames are defined as schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles. For example, given a sentence Jack built a new house out of bricks, a semantic role labeling system should identify the roles for the verb built such as [Agent Jack] built [Created_entity a new house] [Component out of bricks]3. In our study, we build a semantic role labeling system as an intermediate step to label opinion holders and topics by training it on opinion-bearing frames and their frame elements in FrameNet.',\n",
       " '3 Finding Opinions and Their Holders and Topics',\n",
       " 'For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data. The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence. Given a sentence and an opinion bearing word, our method identifies frame elements in the sentence and',\n",
       " '2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 3 The verb build is defined under the frame Building in which Agent, Created_entity, and Components are defined as frame elements.',\n",
       " 'Table 1: Example of opinion related frames and lexical units',\n",
       " 'want, wish, hope, Event,',\n",
       " 'interested, Location_of_event',\n",
       " 'Emotion _directed agitated, amused, anguish, ashamed, angry, annoyed, Event, Topic Experiencer, Expressor,',\n",
       " 'Mental _property absurd, brilliant, careless, crazy, cunning, foolish Behavior, Protagonist, Domain, Degree',\n",
       " 'Subject _stimulus delightful, amazing, annoying, amusing, aggravating, Stimulus, Degree Experiencer, Circumstances,',\n",
       " 'Figure 1: An overview of our algorithm',\n",
       " 'searches which frame element corresponds to the opinion holder and which to the topic. The example in Figure 1 shows the intuition of our algorithm.',\n",
       " 'We decompose our task in 3 subtasks: (1) collect opinion words and opinion-related frames,',\n",
       " 'semantic role labeling for those frames, and',\n",
       " 'finally map semantic roles to holder and topic. Following subsections describe each subtask.',\n",
       " '3.1 Opinion Words and Related Frames',\n",
       " 'We describe the subtask of collecting opinion words and related frames in 3 phases.',\n",
       " 'Phase 1: Collect Opinion Words',\n",
       " 'In this study, we consider an opinion-bearing (positive/negative) word is a key indicator of an opinion. Therefore, we first identify opinion-bearing word from a given sentence and extract its holder and topic. Since previous studies indicate that opinion-bearing verbs and adjectives are especially efficient for opinion identification, we focus on creating a set of opinion-bearing verbs and adjectives. We annotated 1860 adjectives and 2011 verbs4 by classifying them into positive, negative, and neutral classes. Words in the positive class carry positive valence whereas those in negative class carry negative valence. Words that are not opinion-bearing are classified as neutral.',\n",
       " 'Note that in our study we treat word sentiment classification as a three-way classification problem instead of a two-way classification problem',\n",
       " '(i.e. positive and negative). By adding the third class, neutral, we can prevent the classifier assigning either positive or negative sentiment to weak opinion-bearing word. For example, the word central that Hatzivassiloglou and McKeown (1997) marked as a positive adjective is not classified as positive by our system. Instead we mark it as neutral, since it is a weak clue for an opinion. For the same reason, we did not consider able classified as a positive word by General Inquirer5, a sentiment word lexicon, as a positive opinion indicator. Finally, we collected 69 positive and 151 negative verbs and 199 positive and 304 negative adjectives.',\n",
       " 'Phase 2: Find Opinion-related Frames',\n",
       " 'We collected frames related to opinion words from the FrameNet corpus. We used FrameNet II (Baker et al., 2003) which contains 450 semantic frames and more than 3000 frame elements (FE). A frame consists of lexical items, called Lexical Unit (LU), and related frame elements. For instance, LUs in ATTACK frame are verbs such as assail, assault, and attack, and nouns such as invasion, raid, and strike. FrameNet II contains',\n",
       " '4 These were randomly selected from 8011 English verbs and 19748 English adjectives. 5 http://www.wjh.harvard.edu/~inquirer/homecat.htm',\n",
       " 'approximately 7500 lexical units and over 100,000 annotated sentences.',\n",
       " 'For each word in our opinion word set described in Phase 1, we find a frame to which the word belongs. 49 frames for verbs and 43 frames for adjectives are collected. Table 1 shows examples of selected frames with some of the lexical units those frames cover. For example, our system found the frame Desiring from opinion-bearing words want, wish, hope, etc. Finally, we collected 8256 and 11877 sentences related to selected opinion bearing frames for verbs and adjectives respectively.',\n",
       " 'Phase 3: FrameNet expansion',\n",
       " 'Even though Phase 2 searches for a correlated frame for each verb and adjective in our opinion-bearing word list, not all of them are defined in FrameNet data. Some words such as criticize and harass in our list have associated frames (Case 1), whereas others such as vilify and maltreat do not have those (Case 2). For a word in Case 2, we use a clustering algorithms CBC (Clustering By Committee) to predict the closest (most reasonable) frame of undefined word from existing frames. CBC (Pantel and Lin, 2002) was developed based on the distributional hypothesis (Harris, 1954) that words which occur in the same contexts tend to be similar. Using CBC, for example, our clustering module computes lexical similarity between the word vilify in Case 2 and all words in Case 1. Then it picks criticize as a similar word, so that we can use for vilify the frame Judgment_communication to which criticize belongs and all frame elements defined under Judgment_ communication.',\n",
       " '3.2 Semantic Role Labeling',\n",
       " 'To find a potential holder and topic of an opinion word in a sentence, we first label semantic roles in a sentence.',\n",
       " 'Modeling: We follow the statistical approaches for semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et. al, 2003) which separate the task into two steps: identify candidates of frame elements (Step 1) and assign semantic roles for those candidates (Step 2). Like their intuition, we treated both steps as classification problems. We first collected all constituents of the given sentence by parsing it using the Charniak parser. Then, in Step 1, we classified candidate constituents of frame elements from non-candidates. In Step 2, each selected candidate was thus classified into one of frame ele-Table 2: Features used for our semantic role labeling model.',\n",
       " 'target word A predicate whose meaning represents the frame (a verb or an adjective in our task)',\n",
       " 'phrase type Syntactic type of the frame element (e.g. NP, PP)',\n",
       " 'head word Syntactic head of the frame element phrase',\n",
       " 'parse tree path A path between the frame element and target word in the parse tree',\n",
       " 'position Whether the element phrase occurs before or after the target word',\n",
       " 'voice The voice of the sentence (active or passive)',\n",
       " 'frame name one of our opinion-related frames',\n",
       " 'ment types (e.g. Stimulus, Degree, Experiencer, etc.). As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al., 1996). For system development, we used MEGA model optimization package6, an implementation of ME models.',\n",
       " 'Data: We collected 8256 and 11877 sentences which were associated to opinion bearing frames for verbs and adjectives from FrameNet annotation data. Each sentence in our dataset contained a frame name, a target predicate (a word whose meaning represents aspects of the frame), and frame elements labeled with element types. We divided the data into 90% for training and 10% for test.',\n",
       " 'Features used: Table 2 describes features that we used for our classification model. The target word is an opinion-bearing verb or adjective which is associated to a frame. We used the Charniak parser to get a phrase type feature of a frame element and the parse tree path feature. We determined a head word of a phrase by an algorithm using a tree head table7, position feature by the order of surface words of a frame element and the target word, and the voice feature by a simple pattern. Frame name for a target',\n",
       " 'Table 3. Precision (P), Recall (R), and F-score (F) of Topic and Holder identification for opinion verbs (V) and adjectives (A) on Testset 1.',\n",
       " 'Topic Holder',\n",
       " 'Topic Holder',\n",
       " 'word was selected by methods described in Phase 2 and Phase 3 in Subsection 3.1.',\n",
       " '3.3 Map Semantic Roles to Holder and Topic',\n",
       " 'After identifying frame elements in a sentence, our system finally selects holder and topic from those frame elements. In the example in Table 1, the frame Desiring has frame elements such as Event (The change that the Experiencer would like to see), Experiencer (the person or sentient being who wishes for the Event to occur), Location_of_event (the place involved in the desired Event), Focal_participant (entity that the Experiencer wishes to be affected by some Event). Among these FEs, we can consider that Experiencer can be a holder and Focal_participant can be a topic (if any exists in a sentence). We manually built a mapping table to map FEs to holder or topic using as support the FE definitions in each opinion related frame and the annotated sample sentences.',\n",
       " 'The goal of our experiment is first, to see how our holder and topic labeling system works on the FrameNet data, and second, to examine how it performs on online news media text. The first data set (Testset 1) consists of 10% of data described in Subsection 3.2 and the second (Testset 2) is manually annotated by 2 humans. (see Subsection 4.2). We report experimental results for both test sets.',\n",
       " \"Gold Standard: In total, Testset 1 contains 2028 annotated sentences collected from FrameNet data set. (834 from frames related to opinion verb and 1194 from opinion adjectives) We measure the system performance using precision (the percentage of correct holders/topics among system's labeling results), recall (the percentage of correct holders/topics that system retrieved), and F-score.\",\n",
       " 'Baseline: For the baseline system, we applied two different algorithms for sentences which have opinion-bearing verbs as target words and for those that have opinion-bearing adjectives as target words. For verbs, baseline system labeled a subject of a verb as a holder and an object as a topic. (e.g. [holder He] condemned [topic the lawyer].) For adjectives, the baseline marked the subject of a predicate adjective as a holder (e.g. [holder I] was happy). For the topics of adjectives, the baseline picks a modified word if the target adjective is a modifier (e.g. That was a stupid [topic mistake].) and a subject word if the adjective is a predicate. ([topic The view] is breathtaking in January.)',\n",
       " 'Result: Table 3 and 4 show evaluation results of our system and the baseline system respectively. Our system performed much better than the baseline system in identifying topic and holder for both sets of sentences with verb target words and those with adjectives. Especially in recognizing topics of target opinion-bearing words, our system improved F-score from 30.4% to 66.5% for verb target words and from 38.2% to 70.3% for adjectives. It was interesting to see that the intuition that A subject of opinion-bearing verb is a holder and an object is a topic which we applied for the baseline achieved relatively good F-score (56.9%). However, our system obtained much higher F-score (78.7%). Holder identification task achieved higher F-score than topic identification which implies that identifying topics of opinion is a harder task.',\n",
       " 'We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics that simple relations such as subject and object relations are not able to capture. For example, in a sentence Her letter upset me, simply looking for the subjective and objective of the verb upset is not enough to recognize the holder and topic. It is necessary to see a deeper level of semantic relaTable 5. Opinion-bearing sentence identification on Testset 2. (P: precision, R: recall, F: F-score, A: Accuracy, H1: Human1, H2: Human2)',\n",
       " 'Table 6: Results of Topic and Holder identification on Testset 2. (Sys: our system, BL: baseline)',\n",
       " 'Topic Holder',\n",
       " 'tions: Her letter is a stimulus and me is an experiencer of the verb upset.',\n",
       " 'Gold Standard: Two humans 8 annotated 100 sentences randomly selected from news media texts. Those news data is collected from online news sources such as The New York Times, UN Office for the Coordination of Humanitarian Affairs, and BBC News9, which contain articles about various international affaires. Annotators identified opinion-bearing sentences with marking opinion word with its holder and topic if they existed. The inter-annotator agreement in identifying opinion sentences was 82%.',\n",
       " 'Baseline: In order to identify opinion-bearing sentences for our baseline system, we used the opinion-bearing word set introduced in Phase 1 in Subsection 3.1. If a sentence contains an opinion-bearing verb or adjective, the baseline system started looking for its holder and topic. For holder and topic identification, we applied the',\n",
       " '8 We refer them as Human1 and Human2 for the rest of this paper.9 www.nytimes.com, www.irinnews.org, and www.bbc.co.uk',\n",
       " 'same baseline algorithm as described in Subsection 4.1 to Testset 2.',\n",
       " 'Result: Note that Testset 1 was collected from sentences of opinion-related frames in FrameNet and therefore all sentences in the set contained either opinion-bearing verb or adjective. (i.e. All sentences are opinion-bearing) However, sentences in Testset 2 were randomly collected from online news media pages and therefore not all of them are opinion-bearing. We first evaluated the task of opinion-bearing sentence identification. Table 5 shows the system results. When we mark all sentences as opinion-bearing, it achieved 43% and 38% of accuracy for the annotation result of Human1 and Human2 respectively. Our system performance (64% and 55%) is comparable with the unique assignment.',\n",
       " 'We measured the holder and topic identification system with precision, recall, and F-score. As we can see from Table 6, our system achieved much higher precision than the baseline system for both Topic and Holder identification tasks. However, we admit that there is still a lot of room for improvement.',\n",
       " 'The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification. In overall, our system attained higher F-score in holder identification task, including the baseline system. Based on F-score, we believe that identifying topics of opinion is much more difficult than identifying holders. It was interesting to see the same phenomenon that the baseline system mainly assuming that subject and object of a sentence are likely to be opinion holder and topic, achieved lower scores for both holder and topic identification tasks in Testset 2 as in Testset 1. This implies that more sophisticated analysis of the relationship between opinion words (e.g. verbs and adjectives) and their topics and holders is crucial.',\n",
       " '4.3 Difficulties in evaluation',\n",
       " 'We observed several difficulties in evaluating holder and topic identification. First, the boundary of an entity of holder or topic can be flexible. For example, in sentence Senator Titus Olupitan who sponsored the bill wants the permission., not only Senator Titus Olupitan but also Senator Titus Olupitan who sponsored the bill is an eligible answer. Second, some correct holders and topics which our system found were evaluated wrong even if they referred the same entities in the gold standard because human annotators marked only one of them as an answer.',\n",
       " 'In the future, we need more annotated data for improved evaluation.',\n",
       " 'Conclusion and Future Work',\n",
       " 'This paper presented a methodology to identify an opinion with its holder and topic given a sentence in online news media texts. We introduced an approach of exploiting semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. Our method first identifies an opinion-bearing word, labels semantic roles related to the word in the sentence, and then finds a holder and a topic of the opinion word among labeled semantic roles.',\n",
       " 'There has been little previous study in identifying opinion holders and topics partly because it requires a great amount of annotated data. To overcome this barrier, we utilized FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics. However, FrameNet has a limited number of words in its annotated corpus. For a broader coverage, we used a clustering technique to predict a most probable frame for an unseen word.',\n",
       " 'Our experimental results showed that our system performs significantly better than the baseline. The baseline system results imply that opinion holder and topic identification is a hard task. We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics which simple relations such as subject and object relations are not able to capture.',\n",
       " 'In the future, we plan to extend our list of opinion-bearing verbs and adjectives so that we can discover and apply more opinion-related frames. Also, it would be interesting to see how other types of part of speech such as adverbs and nouns affect the performance of the system.',\n",
       " 'Reference',\n",
       " 'Baker, Collin F. and Hiroaki Sato. 2003. The FrameNet Data and Software. Poster and Demonstration at Association for Computational Linguistics. Sapporo, Japan.',\n",
       " 'Berger, Adam, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing, Computational Linguistics, (22-1).',\n",
       " 'Bethard, Steven, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky.',\n",
       " '2004. Automatic Extraction of Opinion Propositions and their Holders, AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.',\n",
       " 'Choi, Y., Cardie, C., Riloff, E., and Patwardhan,',\n",
       " 'S. 2005. Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns. Proceedings of HLT/EMNLP-05.',\n",
       " 'Esuli, Andrea and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classification. Proceedings of CIKM-05, 14th ACM International Conference on Information and Knowledge Management, Bremen, DE, pp. 617-624.',\n",
       " 'Fillmore, C. Frame semantics and the nature of language. 1976. In Annals of the New York Academy of Sciences: Conferences on the Origin and Development of Language and Speech, Volume 280: 20-32.',\n",
       " 'Fleischman, Michael, Namhee Kwon, and Eduard Hovy. 2003. Maximum Entropy Models for FrameNet Classification. Proceedings of EMNLP, Sapporo, Japan.',\n",
       " 'Gildea, D. and Jurafsky, D. Automatic Labeling of semantic roles. 2002. In Computational Linguistics. 28(3), 245-288.',\n",
       " 'Harris, Zellig, 1954. Distributional structure. Word, 10(23) :146--162.',\n",
       " 'Hatzivassiloglou, Vasileios and Kathleen McKeown. 1997. Predicting the Semantic Orientation of Adjectives. Proceedings of 35th Annual Meeting of the Assoc. for Computational Linguistics (ACL-97): 174-181',\n",
       " 'Hatzivassiloglou, Vasileios and Wiebe, Janyce. 2000. Effects of Adjective Orientation and Gradability on Sentence Subjectivity. Proceedings of International Conference on Computational Linguistics (COLING-2000). Saarbrcken, Germany.',\n",
       " 'Hu, Minqing and Bing Liu. 2004. Mining and summarizing customer reviews\". Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD-2004), Seattle, Washington, USA.',\n",
       " 'Kim, Soo-Min and Eduard Hovy. 2004. Determining the Sentiment of Opinions. Proceedings of COLING-04. pp. 1367-1373. Geneva, Switzerland.',\n",
       " 'Kim, Soo-Min and Eduard Hovy. 2005. Identifying Opinion Holders for Question Answering',\n",
       " 'in Opinion Texts. Proceedings of AAAI-05 Workshop on Question Answering in Restricted Domains',\n",
       " 'Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques, Proceedings of EMNLP-2002.',\n",
       " 'Pantel, Patrick and Dekang Lin. 2002. Discovering Word Senses from Text. Proceedings of ACM Conference on Knowledge Discovery and Data Mining. (KDD-02). pp. 613-619. Edmonton, Canada.',\n",
       " 'Popescu, Ana-Maria and Oren Etzioni. 2005. Extracting Product Features and Opinions from Reviews , Proceedings of HLT-EMNLP 2005.',\n",
       " 'Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 2003. Learning Subjective Nouns Using Extraction Pattern Bootstrapping. Proceedings of Seventh Conference on Natural Language Learning (CoNLL-03). ACL SIGNLL. Pages 25-32.',\n",
       " 'Turney, Peter D. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews, Proceedings of ACL-02, Philadelphia, Pennsylvania, 417-424',\n",
       " \"Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P. O'Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. Proceedings of ACL-99. University of Maryland, June, pp. 246-253.\",\n",
       " 'Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proceedings of HLT/EMNLP 2005, Vancouver, Canada ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_at_Introduction(d, i=0):\n",
    "    col = ['query', 'citation']\n",
    "    tmp = d[col[i]]\n",
    "    tmp = re.split(\"Introduction\", tmp)\n",
    "    # print(len(tmp))\n",
    "    return tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       Amazon Mechanical Turk for Subjectivity Word ...\n",
       "2       Mining WordNet for Fuzzy Sentiment:  Sentimen...\n",
       "3       When Specialists and Generalists Work Togethe...\n",
       "4       Distilling Opinion in Discourse: A Preliminar...\n",
       "5       SENTIWORDNET 3.0: An Enhanced Lexical Resourc...\n",
       "                             ...                        \n",
       "96      Similarity-Based Estimation of Word Cooccurre...\n",
       "97      Recognizing Expressions of Commonsense Psycho...\n",
       "98      Automatic Detection of Text Genre  Xerox Palo...\n",
       "99      Building a Large Annotated Corpus of English:...\n",
       "100     Proceedings of the 40th Annual Meeting of the...\n",
       "Length: 100, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_b.apply(lambda d: cut_at_Introduction(d, 1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bAb = data_b\n",
    "data_bAb['query'] = data_b.apply(lambda d: cut_at_Introduction(d, 0), axis = 1)\n",
    "data_bAb['citation'] = data_b.apply(lambda d: cut_at_Introduction(d, 1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.indexing._LocIndexer at 0x11ae5be50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data_bAb.loc(0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con\\xadstituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit = test[21]['query']\n",
    "cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Automatic Labeling of Semantic Roles',\n",
       " 'Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute',\n",
       " 'We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.',\n",
       " 'The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.',\n",
       " 'Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con\\xadstituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.',\n",
       " 'Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.',\n",
       " '1. ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = re.split(\"  \", cit)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ccdc76679b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b.to_csv('data_bert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
