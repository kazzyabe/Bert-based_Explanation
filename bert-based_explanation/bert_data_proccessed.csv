query,citation,decision,query_p,citation_p
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation Cem Akkaya  Alexander Conrad  University of Pittsburgh University of Pittsburgh University of Pittsburgh University of North Texas cem@cs.pitt.edu  Recently researchers have been investigating  Amazon Mechanical Turk (MTurk) as a source of  Amazon Mechanical Turk (MTurk) is a  marketplace for so-called human intelligence  non-expert natural language annotation, which is a  tasks (HITs), or tasks that are easy for  hucheap and quick alternative to expert annotations  mans but currently difficult for automated  proProviders upload tasks to MTurk  In this paper, we utilize MTurk to obtain training  which workers then complete. Natural  landata for Subjectivity Word Sense Disambiguation  guage annotation is one such human  intelli(SWSD) as described in (Akkaya et al., 2009). The  gence task. In this paper, we investigate  usgoal of SWSD is to automatically determine which  ing MTurk to collect annotations for  Subjecword instances in a corpus are being used with  subtivity Word Sense Disambiguation (SWSD),  a coarse-grained word sense disambiguation  jective senses, and which are being used with  obWe investigate whether we can use  jective senses. SWSD is a new task which suffers  MTurk to acquire good annotations with  refrom the absence of a substantial amount of  annospect to gold-standard data, whether we can  tated data and thus can only be applied on a small  filter out low-quality workers (spammers), and  scale. SWSD has strong connections to WSD. Like  whether there is a learning effect associated  supervised WSD, it requires training data where  tarwith repeatedly completing the same kind of  get word instances words which need to be  distask. While our results with respect to  spammers are inconclusive, we are able to  obambiguated by the system are labeled as having  tain high-quality annotations for the SWSD  an objective sense or a subjective sense. (Akkaya  task. These results suggest a greater role for  et al., 2009) show that SWSD may bring substantial  MTurk with respect to constructing a large  improvement in subjectivity and sentiment analysis,  scale SWSD system in the future, promising  if it could be applied on a larger scale. The good  substantial improvement in subjectivity and  news is that training data for 80 selected keywords is  sentiment analysis.  enough to make a substantial difference (Akkaya et  al., 2009). Thus, large scale SWSD is feasible. We  ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","Recently researchers have been investigating marketplace for so-called human intelligence non-expert natural language annotation, which is a tasks (HITs), or tasks that are easy for hucheap and quick alternative to expert annotations mans but currently difficult for automated which workers then complete. Natural  landata for Subjectivity Word Sense Disambiguation  guage annotation is one such human  intelli(SWSD) as described in (Akkaya et al., 2009).The  gence task.In this paper, we investigate  usgoal of SWSD is to automatically determine which  ing MTurk to collect annotations for  Subjecword instances in a corpus are being used with  subtivity Word Sense Disambiguation (SWSD),  a coarse-grained word sense disambiguation  jective senses, and which are being used with  obWe investigate whether we can use  jective senses.SWSD is a new task which suffers  MTurk to acquire good annotations with  refrom the absence of a substantial amount of  annospect to gold-standard data, whether we can  tated data and thus can only be applied on a small  filter out low-quality workers (spammers), and  scale.SWSD has strong connections to WSD.Like  whether there is a learning effect associated  supervised WSD, it requires training data where  tarwith repeatedly completing the same kind of  get word instances words which need to be  distask.While our results with respect to  spammers are inconclusive, we are able to  obambiguated by the system are labeled as having  tain high-quality annotations for the SWSD  an objective sense or a subjective sense.(Akkaya  task. These results suggest a greater role for  et al., 2009) show that SWSD may bring substantial  MTurk with respect to constructing a large  improvement in subjectivity and sentiment analysis,  scale SWSD system in the future, promising  if it could be applied on a larger scale.The good  substantial improvement in subjectivity and  news is that training data for 80 selected keywords is  sentiment analysis.enough to make a substantial difference (Akkaya et  al., 2009).Thus, large scale SWSD is feasible.We"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Mining WordNet for Fuzzy Sentiment:  Sentiment Tag Extraction from WordNet Glosses  Concordia University  {andreev, bergler}@encs.concordia.ca  representation, which regard all members of a  category as equal: no element is more of a  memMany of the tasks required for semantic  ber than any other (Edmonds, 1999). In this  patagging of phrases and texts rely on a list  per, we challenge the applicability of this  assumpof words annotated with some semantic  tion to the semantic category of sentiment, which features.  We present a method for  exconsists of positive, negative and neutral  subcatetracting sentiment-bearing adjectives from  gories, and present a dictionary-based Sentiment  WordNet using the Sentiment Tag  ExtracTag Extraction Program (STEP) that we use to  tion Program (STEP). We did 58 STEP  generate a fuzzy set of English sentiment-bearing runs on unique non-intersecting seed lists  words for the use in sentiment tagging systems 1.  drawn from manually annotated list of  The proposed approach based on the fuzzy logic  positive and negative adjectives and  evalu(Zadeh, 1987) is used here to assign fuzzy  senated the results against other manually  antiment tags to all words in WordNet (Fellbaum,  notated lists. The 58 runs were then  col1998), that is it assigns sentiment tags and a degree lapsed into a single set of 7, 813 unique  of centrality of the annotated words to the  sentiFor each word we computed a  ment category. This assignment is based on  WordNet Overlap Score by subtracting the total  Net glosses. The implications of this approach for  number of runs assigning this word a  negNLP and linguistic research are discussed.  ative sentiment from the total of the runs  that consider it positive. We demonstrate  The Category of Sentiment as a Fuzzy  that Net Overlap Score can be used as a  measure of the words degree of  memberSome semantic categories have clear membership  ship in the fuzzy category of sentiment:  (e.g., lexical fields (Lehrer, 1974) of color, body the core adjectives, which had the high-parts or professions), while others are much more  difficult to define. This prompted the development  most accurately both by STEP and by  huof approaches that regard the transition from  memman annotators, while the words on the  bership to non-membership in a semantic category  periphery of the category had the lowest  as gradual rather than abrupt (Zadeh, 1987; Rosch,  scores and were associated with low rates  1978). In this paper we approach the category of  of inter-annotator agreement.  sentiment as one of such fuzzy categories where 1  ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","Mining WordNet for Fuzzy Sentiment: representation, which regard all members of a category as equal: no element is more of a memMany of the tasks required for semantic In this  patagging of phrases and texts rely on a list  per, we challenge the applicability of this  assumpof words annotated with some semantic  tion to the semantic category of sentiment, which features.We present a method for  exconsists of positive, negative and neutral  subcatetracting sentiment-bearing adjectives from  gories, and present a dictionary-based Sentiment  WordNet using the Sentiment Tag  ExtracTag Extraction Program (STEP) that we use to  tion Program (STEP).We did 58 STEP  generate a fuzzy set of English sentiment-bearing runs on unique non-intersecting seed lists  words for the use in sentiment tagging systems 1.drawn from manually annotated list of  The proposed approach based on the fuzzy logic  positive and negative adjectives and  evalu(Zadeh, 1987) is used here to assign fuzzy  senated the results against other manually  antiment tags to all words in WordNet (Fellbaum,  notated lists.The 58 runs were then  col1998), that is it assigns sentiment tags and a degree lapsed into a single set of 7, 813 unique  of centrality of the annotated words to the  sentiFor each word we computed a  ment category.This assignment is based on  WordNet Overlap Score by subtracting the total  Net glosses.The implications of this approach for  number of runs assigning this word a  negNLP and linguistic research are discussed.ative sentiment from the total of the runs  that consider it positive.We demonstrate  The Category of Sentiment as a Fuzzy  that Net Overlap Score can be used as a  measure of the words degree of  memberSome semantic categories have clear membership  ship in the fuzzy category of sentiment:  (e.g., lexical fields (Lehrer, 1974) of color, body the core adjectives, which had the high-parts or professions), while others are much more  difficult to define.This prompted the development  most accurately both by STEP and by  huof approaches that regard the transition from  memman annotators, while the words on the  bership to non-membership in a semantic category  periphery of the category had the lowest  as gradual rather than abrupt (Zadeh, 1987; Rosch,  scores and were associated with low rates  1978).In this paper we approach the category of  of inter-annotator agreement.sentiment as one of such fuzzy categories where 1"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging  Concordia University  Concordia University  and Gamon (2005), demonstrated that sentiment  annotation classifiers trained in one domain do not  perThis study presents a novel approach to the  form well on other domains. A number of methods  problem of system portability across  differhas been proposed in order to overcome this system  that integrates a corpus-based classifier trained  portability limitation by using out-of-domain data,  on a small set of annotated in-domain data  unlabelled in-domain corpora or a combination of  and a lexicon-based system trained on  Wordin-domain and out-of-domain examples (Aue and  Net. The paper explores the challenges of  system portability across domains and text  genres (movie reviews, news, blogs, and product  reviews), highlights the factors affecting  sysIn this paper, we present a novel approach to the  tem performance on out-of-domain and  smallproblem of system portability across different  doset in-domain data, and presents a new  sysmains by developing a sentiment annotation  system consisting of the ensemble of two  classitem that integrates a corpus-based classifier with  fiers with precision-based vote weighting, that  provides significant gains in accuracy and  rea lexicon-based system trained on WordNet.  call over the corpus-based classifier and the  adopting this approach, we sought to develop a  lexicon-based system taken individually.  system that relies on both general and  domainspecific knowledge, as humans do when analyzing  ",0,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","annotation classifiers trained in one domain do not perThis study presents a novel approach to the form well on other domains. A number of methods  problem of system portability across  differhas been proposed in order to overcome this system  that integrates a corpus-based classifier trained  portability limitation by using out-of-domain data,  on a small set of annotated in-domain data  unlabelled in-domain corpora or a combination of  and a lexicon-based system trained on  Wordin-domain and out-of-domain examples (Aue and  Net.The paper explores the challenges of  system portability across domains and text  genres (movie reviews, news, blogs, and product  reviews), highlights the factors affecting  sysIn this paper, we present a novel approach to the  tem performance on out-of-domain and  smallproblem of system portability across different  doset in-domain data, and presents a new  sysmains by developing a sentiment annotation  system consisting of the ensemble of two  classitem that integrates a corpus-based classifier with  fiers with precision-based vote weighting, that  provides significant gains in accuracy and  rea lexicon-based system trained on WordNet.call over the corpus-based classifier and the  adopting this approach, we sought to develop a  lexicon-based system taken individually.system that relies on both general and  domainspecific knowledge, as humans do when analyzing"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Distilling Opinion in Discourse: A Preliminary Study  Nicholas Asher and Farah Benamara  Yvette Yannick Mathieu  IRIT-CNRS Toulouse,  LLF-CNRS Paris,  {asher, benamara}@irit.fr  opinion analysis using discourse relations. This  analysis is based on a lexical semantic analysis of  In this paper, we describe a preliminary  a wide class of expressions coupled together with  study for a discourse based opinion  catean analysis of how clauses involving these  expresgorization and propose a new annotation  sions are related to each other within a discourse.  schema for a deep contextual opinion  analThe aim of this paper is to establish the  feasibilysis using discourse relations.  ity and stability of our annotation scheme at the  ",0,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","Distilling Opinion in Discourse: A Preliminary Study {asher, benamara}@irit.fr opinion analysis using discourse relations. This  analysis is based on a lexical semantic analysis of  In this paper, we describe a preliminary  a wide class of expressions coupled together with  study for a discourse based opinion  catean analysis of how clauses involving these  expresgorization and propose a new annotation  sions are related to each other within a discourse.schema for a deep contextual opinion  analThe aim of this paper is to establish the  feasibilysis using discourse relations.ity and stability of our annotation scheme at the"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," SENTIWORDNET 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining  In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET  2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20% with respect to SENTIWORDNET 1.0.  ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","SENTIWORDNET 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide.Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality.SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET  2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores.We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0.We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20% with respect to SENTIWORDNET 1.0."
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification  Department of Computer and Information Science  University of Pennsylvania  deployed industrially in systems that gauge market reaction and summarize opinion from Web pages,  Automatic sentiment classification has been  discussion boards, and blogs.  extensively studied and applied in recent  With such widely-varying domains, researchers  years. However, sentiment is expressed  difand engineers who build sentiment classification  ferently in different domains, and annotating  systems need to collect and curate data for each new corpora for every possible domain of interest  domain they encounter. Even in the case of market  analysis, if automatic sentiment classification were tation for sentiment classifiers, focusing on  to be used across a wide range of domains, the  efonline reviews for different types of  prodfort to annotate corpora for each domain may  beucts. First, we extend to sentiment  classificome prohibitive, especially since product features cation the recently-proposed structural cor-change over time. We envision a scenario in which  respondence learning (SCL) algorithm,  redevelopers annotate corpora for a small number of  ducing the relative error due to adaptation  domains, train classifiers on those corpora, and then between domains by an average of 30% over  apply them to other similar corpora. However, this the original SCL algorithm and 46% over  approach raises two important questions. First, it a supervised baseline. Second, we identify  is well known that trained classifiers lose accuracy a measure of domain similarity that corre-when the test data distribution is significantly differ-lates well with the potential for adaptation  ent from the training data distribution 1. Second, it is of a classifier from one domain to another.  not clear which notion of domain similarity should This measure could for instance be used to  be used to select domains to annotate that would be select a small set of domains to annotate  good proxies for many other domains.  whose trained classifiers would transfer well  We propose solutions to these two questions and  to many other domains.  evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs,  ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, Automatic sentiment classification has been discussion boards, and blogs. extensively studied and applied in recent  With such widely-varying domains, researchers  years.However, sentiment is expressed  difand engineers who build sentiment classification  ferently in different domains, and annotating  systems need to collect and curate data for each new corpora for every possible domain of interest  domain they encounter.Even in the case of market  analysis, if automatic sentiment classification were tation for sentiment classifiers, focusing on  to be used across a wide range of domains, the  efonline reviews for different types of  prodfort to annotate corpora for each domain may  beucts.First, we extend to sentiment  classificome prohibitive, especially since product features cation the recently-proposed structural cor-change over time.We envision a scenario in which  respondence learning (SCL) algorithm,  redevelopers annotate corpora for a small number of  ducing the relative error due to adaptation  domains, train classifiers on those corpora, and then between domains by an average of 30% over  apply them to other similar corpora.However, this the original SCL algorithm and 46% over  approach raises two important questions.First, it a supervised baseline.Second, we identify  is well known that trained classifiers lose accuracy a measure of domain similarity that corre-when the test data distribution is significantly differ-lates well with the potential for adaptation  ent from the training data distribution 1.Second, it is of a classifier from one domain to another.not clear which notion of domain similarity should This measure could for instance be used to  be used to select domains to annotate that would be select a small set of domains to annotate  good proxies for many other domains.whose trained classifiers would transfer well  We propose solutions to these two questions and  to many other domains.evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs,"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Kenneth Bloom and Navendu Garg and Shlomo Argamon Computer Science Department  Illinois Institute of Technology  Chicago, IL 60616  Popescu and Etzioni, 2005). Much of this work has utilized the fundamental concept of semantic orien-Sentiment analysis seeks to characterize  tation', (Turney, 2002); however, sentiment analysis opinionated or evaluative aspects of nat-still lacks a unified field theory'.  ural language text. We suggest here that  We propose in this paper that a fundamental task appraisal expression extraction should be  underlying many of these formulations is the extrac-viewed as a fundamental task in sentiment  tion and analysis of appraisal expressions, defined analysis. An appraisal expression is a tex-as those structured textual units which express an tual unit expressing an evaluative stance  evaluation of some object. An appraisal expression towards some target. The task is to find  has three main components: an attitude (which takes and characterize the evaluative attributes  an evaluative stance about an object), a target (the of such elements. This paper describes a  object of the stance), and a source (the person tak-system for effectively extracting and  dising the stance) which may be implied.  The idea of appraisal extraction is a generaliza-sions in English outputting a generic  reption of problem formulations developed in earlier resentation in terms of their evaluative  works. Mullen and Collier's (2004) notion of classi-function in the text. Data mining on  apfying appraisal terms using a multidimensional set praisal expressions gives meaningful and  of attributes is closely tied to the definition of an non-obvious insights.  appraisal expression, which is classified along several dimensions.  In previous work (Whitelaw et  al., 2005), we presented a related technique of find-1  ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","Much of this work has utilized the fundamental concept of semantic orien-Sentiment analysis seeks to characterize  tation', (Turney, 2002); however, sentiment analysis opinionated or evaluative aspects of nat-still lacks a unified field theory'.ural language text.We suggest here that  We propose in this paper that a fundamental task appraisal expression extraction should be  underlying many of these formulations is the extrac-viewed as a fundamental task in sentiment  tion and analysis of appraisal expressions, defined analysis.An appraisal expression is a tex-as those structured textual units which express an tual unit expressing an evaluative stance  evaluation of some object.An appraisal expression towards some target.The task is to find  has three main components: an attitude (which takes and characterize the evaluative attributes  an evaluative stance about an object), a target (the of such elements.This paper describes a  object of the stance), and a source (the person tak-system for effectively extracting and  dising the stance) which may be implied.The idea of appraisal extraction is a generaliza-sions in English outputting a generic  reption of problem formulations developed in earlier resentation in terms of their evaluative  works.Mullen and Collier's (2004) notion of classi-function in the text.Data mining on  apfying appraisal terms using a multidimensional set praisal expressions gives meaningful and  of attributes is closely tied to the definition of an non-obvious insights.appraisal expression, which is classified along several dimensions.In previous work (Whitelaw et  al., 2005), we presented a related technique of find-1"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," ULE-BASED  Bril  Department of Computer Science  University of Pennsylvania  obtained a high degree of accuracy without performing  CT  Automatic part of speech tagging is an area of natural  lanany syntactic analysis on the input. These stochastic  guage processing where statistical techniques have been more  part of speech taggers make use of a Markov model  successful than rule-based methods. In this paper, we present  which captures lexical and contextual information. The  a simple rule-based part of speech tagger which  automatiparameters of the model can be estimated from tagged  cally acquires its rules and tags with accuracy comparable  [1, 3, 4, 6, 12] or untagged [2, 9, 11] text. Once the  to stochastic taggers. The rule-based tagger has many  adparameters of the model are estimated, a sentence can  vantages over these taggers, including: a vast reduction in  stored information required, the perspicuity of a small set  then be automatically tagged by assigning it the tag  seof meaningful rules, ease of nding and implementing  imquence which is assigned the highest probability by the  provements to the tagger, and better portability from one  model. Performance is often enhanced with the aid of  tag set, corpus genre or language to another. Perhaps the  various higher level and postprocessing procedures  biggest contribution of this work is in demonstrating that  or by manually tuning the model.  the stochastic method is not the only viable method for part  of speech tagging. The fact that a simple rule-based tagger  A number of rule-based taggers have been built [10, 7, 8].  that automatically learns its rules can perform so well should  [10] and [7] both have error rates substantially higher  oer encouragement for researchers to further explore  rulethan state of the art stochastic taggers. [8]  disambased tagging, searching for a better and more expressive  biguates words within a deterministic parser. We wanted  set of rule templates and other variations on the simple but  to determine whether a simple rule-based tagger  witheective theme described below.  out any knowledge of syntax can perform as well as a  stochastic tagger, or if part of speech tagging really is a  domain to which stochastic techniques are better suited.  INTR  There has been a dramatic increase in the application of  probabilistic models to natural language processing over In this paper we describe a rule-based tagger which per-the last few years. The appeal of stochastic techniques forms as well as taggers based upon probabilistic models.  over traditional rule-based techniques comes from the The rule-based tagger overcomes the limitationscommon ease with which the necessary statistics can be in rule-based approaches to language processing: it is ically acquired and the fact that very little handcrafted robust, and the rules are automatically acquired. In ad-knowledge need be built into the system. In contrast, dition, the tagger has many advantages over stochastic the rules in rule-based systems are usually dicult to taggers, including: a vast reduction in stored informa-construct and are typically not very robust.  tion required, the perspicuity of a smallset of meaningful  rules as opposed to the large tables of statistics needed  One area in which the statistical approach has done for stochastic taggers, ease of nding and implementing ticularly well is automatic part of speech tagging, improvements to the tagger, and better portability from signing each word in an input sentence its proper part of one tag set or corpus genre to another.  A version of this paper appears in Proceedings of the Third  Conference on Applied Computational Linguistics (ACL), Trento,  The tagger works by automatically recognizing and  remItaly, 1992. Used by permission of the Association for  Computational Linguistics; copies of the publication from which this  maedying its weaknesses, thereby incrementally improving  terial is derived can can be obtained from Dr. Donald E. Walker  its performance. The tagger initially tags by assigning  (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910,  Morriseach word its most likely tag, estimated by examining a  town, NJ 07960-1910, USA. The author would like to thank Mitch  large tagged corpus, without regard to context. In both  Marcus and Rich Pito for valuableinput. This work was supported  by DARPA and AFOSR jointly under grant No. AFOSR-90-0066,  sentences below, run would be tagged as a verb:  and by ARO grant No. DAAL 03-89-C0031 PRI.  The  lasted thirty minutes.  have been tagged with tag in the patch corpus. Next, for  three miles every day.  each error triple, it is determined which instantiation of  a template from the prespecied set of patch templates  results in the greatest error reduction. Currently, the  The initial tagger has two procedures built in to improve patch templates are:  performance; both make use of no contextual  information. One procedure is provided with information that Change tag to tag when:  words that were not in the training corpus and are  capitalized tend to be proper nouns, and attempts to x  tagging mistakes accordingly. This information could be  acquired automatically (see below), but is prespecied  1. The preceding (following) word is tagged z.  in the current implementation. In addition, there is a  procedure which attempts to tag words not seen in the  2. The word two before (after) is tagged z.  training corpus by assigning such words the tag most  3. One of the two preceding (following) words is tagged  common for words ending in the same three letters. For  example, blahblahous would be tagged as an adjective,  because this is the most common tag for words ending  4. One of the three preceding (following) words is  in ous. This information is derived automatically from  the training corpus.  5. The preceding word is tagged z and the following  This very simple algorithm has an error rate of about  7.9% when trained on 90% of the tagged Brown Corpus1  [5], and tested on a separate 5% of the corpus. Training  6. The preceding (following) word is tagged z and the  consists of compiling a list of the most common tag for  word two before (after) is tagged w.  each word in the training corpus.  7. The current word is (is not) capitalized.  The tagger then acquires patches to improve its  performance. Patch templates are of the form:  8. The previous word is (is not) capitalized.  If a word is tagged and it is in context , then For each error triple < tag ;tag ;number > and patch, a  change that tag to , or  we compute the reduction in error which results from  If a word is tagged and it has lexical property , applying the patch to remedy the mistagging of a word a  then change that tag to , or  as tag when it should have been tagged tag . We then  compute the number of new errors caused by applying  If a word is tagged and a word in region has the patch; that is, the number of times the patch results a  lexical property , then change that tag to .  in a word being tagged as tag when it should be tagged  tag . The net improvement is calculated by subtracting  the latter value from the former.  The initial tagger was trained on 90% of the corpus (the For example, when the initial tagger tags the patch cor-training corpus). 5% was held back to be used for the pus, it mistags 159 words as verbs when they should be patch acquisition procedure (the patch corpus) and 5% nouns. If the patch change the tag from verb to noun if for testing. Once the initial tagger is trained, it is used to one of the two preceding words is tagged as a determiner tag the patch corpus. A list of tagging errors is compiled is applied, it corrects 98 of the 159 errors. However, by comparing the output of the tagger to the correct it results in an additional 18 errors from changing tags tagging of the patch corpus. This list consists of triples which really should have been verb to noun. This patch  < tag ;tag ;number >, indicating the number of times results in a net decrease of 80 errors on the patch corpus.  the tagger mistagged a word with tag when it should  The patch which results in the greatest improvement to  The Brown Corpus contains about 1.1 million words from a  the patch corpus is added to the list of patches. The  variety of genres of written English. There are 192 tags in the tag  set, 96 of which occur more than one hundred times in the corpus.  patch is then applied in order to improve the tagging of  The test set contained text from all genres in the Brown  the patch corpus, and the patch acquisition procedure  Corpus.  The rst ten patches found by the system are listed  below .  Patch Application and Error Reduction  (1) TO IN NEXT-TAG AT  (2) VBN VBD PREV-WORD-IS-CAP YES  (6) TO IN NEXT-WORD-IS-CAP YES  (10) NP NN CURRENT-WORD-IS-CAP NO  The rst patch states that if a word is tagged  and the  following word is tagged  , then switch the tag from  to . This is because a noun phrase is much more  likely to immediately follow a preposition than to  immediately follow innitive  . The second patch states  that a tag should be switched from  to  if the  preceding word is capitalized. This patch arises from two  Number of Patches  facts: the past verb tag is more likely than the past  participle verb tag after a proper noun, and is also the more  likely tag for the second word of the sentence. The third  TS  patch states that  should be changed to  if The tagger was tested on 5% of the Brown Corpus  inany of the preceding three words are tagged  cluding sections from every genre. First, the test corpus  Once the list of patches has been acquired, new text was tagged by the simple lexical tagger. Next, each of can be tagged as follows. First, tag the text using the the patches was in turn applied to the corpus. Below is a basic lexical tagger. Next, apply each patch in turn to graph showing the improvement in accuracy from apply-the corpus to decrease the error rate. A patch which ing patches. It is signicant that with only 71 patches, changes the tagging of a word from to only applies an error rate of 5.1% was obtained . Of the 71 patches, 5  66 resulted in a reduction in the number of errors in the  if the word has been tagged somewhere in the training test corpus, 3 resulted in no net change, and 2 resulted b  in a higher number of errors. Almost all patches which  Note that one need not be too careful when constructing were eective on the training corpus were also eective the list of patch templates. Adding a bad template to the on the test corpus.  list will not worsen performance. If a template is bad,  then no rules which are instantiations of that template Unfortunately, it is dicult to compare our results with will appear in the nal list of patches learned by the other published results. In [12], an error rate of 3-4%  tagger. This makes it easy to experiment with extensions on one domain, Wall Street Journal articles and 5.6%  to the tagger.  on another domain, texts on terrorism in Latin  American countries, is quoted. However, both the domains  and the tag set are dierent from what we use. [1]  reAT = article, HVD = had, IN = preposition, MD = modal,  ports an accuracy of \95-99% correct, depending on the  NN = sing. noun, NP = proper noun, PPS = 3rd sing. nom.  denition of correct"". We implemented a version of the  We ran the experiment three times. Each time we divided the  Both the rst word of a sentence and proper nouns are  corpus into training, patch and test sets in a dierent way. All  three runs gave an error rate of 5%.  algorithm described in [1] which did not make use of a was divided into a training corpus of about one million dictionary to extend its lexical knowledge. When trained words, a patch corpus of about 65,000 words and a test and tested on the same samples used in our experiment, corpus of about 65,000 words. Patches were acquired we found the error rate to be about 4.5%. [3] quotes as described above. When tested on the test corpus, a 4% error rate when testing and training on the same with lexical information derived solely from the training text. [6] reports an accuracy of 96-97%. Their corpus, the error rate was 5%. Next, the same patches bilistic tagger has been augmented with a handcrafted were used, but lexical information was gathered from procedure to pretag problematic \idioms"". This the entire Brown Corpus. This reduced the error rate to dure, which requires that a list of idioms be laboriously 4.1%. Finally, the same experiment was run with lexical created by hand, contributes 3% toward the accuracy of information gathered solely from the test corpus. This their tagger, according to [3]. The idiom list would have resulted in a 3.5% error rate. Note that the patches used to be rewritten if one wished to use this tagger for a in the two experiments with no unknown words were dierent tag set or a dierent corpus. It is interesting not the optimal patches for these tests, since they were to note that the information contained in the idiom list derived from a corpus that contained unknown words.  can be automatically acquired by the rule-based tagger.  For example, their tagger had diculty tagging as old  as. An explicit rule was written to pretag as old as with  CONCLUSIONS  the proper tags. According to the tagging scheme of the We have presented a simple rule-based part of speech Brown Corpus, the rst as should be tagged as a tagger which performs as well as existing stochastic tag-er, and the second as a subordinating conjunction. In gers, but has signicant advantages over these taggers.  the rule-based tagger, the most common tag for as is  subordinating conjunction. So initially, the second as is The tagger is extremely portable. Many of the higher tagged correctly and the rst as is tagged incorrectly. To level procedures used to improve the performance of remedy this, the system acquires the patch: if the stochastic taggers would not readily transfer over to a rent word is tagged as a subordinating conjunction, and  dierent tag set or genre, and certainly would not  transso is the word two positions ahead, then change the tag of  fer over to a dierent language. Everything except for  the current word to qualier. The rule-based tagger has  the proper noun discovery procedure is  automaticallyacautomatically learned how to properly tag this \idiom."" quired by the rule-based tagger , making it much more 7  portable than a stochastic tagger. If the tagger were  Regardless of the precise rankings of the various taggers, trained on a dierent corpus, a dierent set of patches we have demonstrated that a simple rule-based tagger suitable for that corpus would be found automatically.  with very few rules performs on par with stochastic  taggers. It should be mentioned that our results were Large tables of statistics are not needed for the rule-tained without the use of a dictionary. Incorporating a based tagger. In a stochastic tagger, tens of thousands large dictionary into the system would improve of lines of statistical information are needed to capture mance in two ways. First, it would increase the accuracy contextual information. This information is usually a ta-in tagging words not seen in the training corpus, since ble of trigram statistics, indicating for all tags tag , tag a  part of speech information for some words not appearing and tag the probability that tag follows tag and tag .  in the training corpus can be obtained from the In the rule-based tagger, contextual information is cap-nary. Second, it would increase the error reduction tured in fewer than eighty rules. This makes for a much sulting from applying patches. When a patch indicates more perspicuous tagger, aiding in better understanding that a word should be tagged with tag instead of tag , and simplifying further development of the tagger. Contextual informationis expressed in a much more compact  the tag is only switched if the word was tagged with tag  and understandable form. As can be seen from  comparsomewhere in the training corpus. Using a dictionary  would provide more accurate knowledge about the set ing error rates, this compact representation of contextual of permissible part of speech tags for a particular word. information is just as eective as the information hidden We plan to incorporate a dictionary into the tagger in in the large tables of contextual probabilities.  the future.  Perhaps the biggest contribution of this work is in  As an estimate of the improvement possible by using demonstrating that the stochastic method is not the only a dictionary, we ran two experiments where all words viable approach for part of speech tagging. The fact that were known by the system. First, the Brown Corpus the simple rule-based tagger can perform so well should oer encouragement for researchers to further explore  rule-based tagging, searching for a better and more  exThis was one of the 71 patches acquired by the rule-based  And even this could be learned by the tagger.  pressive set of patch templates and other variations on  this simple but eective theme.  1. Church, K. A Stochastic Parts Program and Noun  Phrase Parser for Unrestricted Text. In Proceedings of  the Second Conference on Applied Natural Language  Processing, ACL, 136-143, 1988.  2. Cutting, D., Kupiec, J., Pederson, J. and Sibun, P. A  Practical Part-of-Speech Tagger. In Proceedings of the  Third Conference on Applied Natural Language  Process3. DeRose, S.J. Grammatical Category Disambiguation by  Statistical Optimization. Computational Linguistics 14:  4. Deroualt, A. and Merialdo, B. Natural language  modeling for phoneme-to-text transcription. IEEE  Transactions on Pattern Analysis and Machine Intelligence,Vol.  5. Francis, W. Nelson and Kucera, Henry, Frequency  analysis of English usage. Lexicon and grammar. Houghton  6. Garside, R., Leech, G. & Sampson, G. The  Computational Analysis of English: A Corpus-Based Approach.  7. Green, B. and Rubin, G. Automated Grammatical  Tagging of English. Department of Linguistics, Brown  Uni8. Hindle, D. Acquiring disambiguation rules from text.  Proceedings of the 27th Annual Meeting of the  Association for Computational Linguistics, 1989.  9. Jelinek, F. Markov source modeling of text generation.  In J. K. Skwirzinski, ed., Impact of Processing  Tech10. Klein, S. and Simmons, R.F. A Computational  Approach to Grammatical Coding of English Words. JACM  phrase-dependent word tagging. In Proceedings of the  DARPA Speech and Natural Language Workshop,  MorStudies in Part of Speech Labelling, Proceedings of the  DARPA Speech and Natural Language Workshop, ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","ULE-BASED Bril obtained a high degree of accuracy without performing Automatic part of speech tagging is an area of natural These stochastic  guage processing where statistical techniques have been more  part of speech taggers make use of a Markov model  successful than rule-based methods.In this paper, we present  which captures lexical and contextual information.The  a simple rule-based part of speech tagger which  automatiparameters of the model can be estimated from tagged  cally acquires its rules and tags with accuracy comparable  [1, 3, 4, 6, 12] or untagged [2, 9, 11] text.Once the  to stochastic taggers.The rule-based tagger has many  adparameters of the model are estimated, a sentence can  vantages over these taggers, including: a vast reduction in  stored information required, the perspicuity of a small set  then be automatically tagged by assigning it the tag  seof meaningful rules, ease of nding and implementing  imquence which is assigned the highest probability by the  provements to the tagger, and better portability from one  model.Performance is often enhanced with the aid of  tag set, corpus genre or language to another.Perhaps the  various higher level and postprocessing procedures  biggest contribution of this work is in demonstrating that  or by manually tuning the model.the stochastic method is not the only viable method for part  of speech tagging.The fact that a simple rule-based tagger  A number of rule-based taggers have been built [10, 7, 8].  that automatically learns its rules can perform so well should  [10] and [7] both have error rates substantially higher  oer encouragement for researchers to further explore  rulethan state of the art stochastic taggers.[8]  disambased tagging, searching for a better and more expressive  biguates words within a deterministic parser.We wanted  set of rule templates and other variations on the simple but  to determine whether a simple rule-based tagger  witheective theme described below.out any knowledge of syntax can perform as well as a  stochastic tagger, or if part of speech tagging really is a  domain to which stochastic techniques are better suited.INTR  There has been a dramatic increase in the application of  probabilistic models to natural language processing over In this paper we describe a rule-based tagger which per-the last few years.The appeal of stochastic techniques forms as well as taggers based upon probabilistic models.over traditional rule-based techniques comes from the The rule-based tagger overcomes the limitationscommon ease with which the necessary statistics can be in rule-based approaches to language processing: it is ically acquired and the fact that very little handcrafted robust, and the rules are automatically acquired.In ad-knowledge need be built into the system.In contrast, dition, the tagger has many advantages over stochastic the rules in rule-based systems are usually dicult to taggers, including: a vast reduction in stored informa-construct and are typically not very robust.tion required, the perspicuity of a smallset of meaningful  rules as opposed to the large tables of statistics needed  One area in which the statistical approach has done for stochastic taggers, ease of nding and implementing ticularly well is automatic part of speech tagging, improvements to the tagger, and better portability from signing each word in an input sentence its proper part of one tag set or corpus genre to another.A version of this paper appears in Proceedings of the Third  Conference on Applied Computational Linguistics (ACL), Trento,  The tagger works by automatically recognizing and  remItaly, 1992.Used by permission of the Association for  Computational Linguistics; copies of the publication from which this  maedying its weaknesses, thereby incrementally improving  terial is derived can can be obtained from Dr. Donald E. Walker  its performance.The tagger initially tags by assigning  (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910,  Morriseach word its most likely tag, estimated by examining a  town, NJ 07960-1910, USA.The author would like to thank Mitch  large tagged corpus, without regard to context.In both  Marcus and Rich Pito for valuableinput.This work was supported  by DARPA and AFOSR jointly under grant No. AFOSR-90-0066,  sentences below, run would be tagged as a verb:  and by ARO grant No. DAAL 03-89-C0031 PRI.The  lasted thirty minutes.have been tagged with tag in the patch corpus.Next, for  three miles every day.each error triple, it is determined which instantiation of  a template from the prespecied set of patch templates  results in the greatest error reduction.Currently, the  The initial tagger has two procedures built in to improve patch templates are:  performance; both make use of no contextual  information.One procedure is provided with information that Change tag to tag when:  words that were not in the training corpus and are  capitalized tend to be proper nouns, and attempts to x  tagging mistakes accordingly.This information could be  acquired automatically (see below), but is prespecied  1.The preceding (following) word is tagged z.  in the current implementation.In addition, there is a  procedure which attempts to tag words not seen in the  2.The word two before (after) is tagged z.  training corpus by assigning such words the tag most  3.One of the two preceding (following) words is tagged  common for words ending in the same three letters.For  example, blahblahous would be tagged as an adjective,  because this is the most common tag for words ending  4.One of the three preceding (following) words is  in ous.This information is derived automatically from  the training corpus.5. The preceding word is tagged z and the following  This very simple algorithm has an error rate of about  7.9% when trained on 90% of the tagged Brown Corpus1  [5], and tested on a separate 5% of the corpus.Training  6.The preceding (following) word is tagged z and the  consists of compiling a list of the most common tag for  word two before (after) is tagged w.  each word in the training corpus.7. The current word is (is not) capitalized.The tagger then acquires patches to improve its  performance.Patch templates are of the form:  8.The previous word is (is not) capitalized.If a word is tagged and it is in context , then For each error triple < tag ;tag ;number > and patch, a  change that tag to , or  we compute the reduction in error which results from  If a word is tagged and it has lexical property , applying the patch to remedy the mistagging of a word a  then change that tag to , or  as tag when it should have been tagged tag . We then  compute the number of new errors caused by applying  If a word is tagged and a word in region has the patch; that is, the number of times the patch results a  lexical property , then change that tag to .  in a word being tagged as tag when it should be tagged  tag . The net improvement is calculated by subtracting  the latter value from the former.The initial tagger was trained on 90% of the corpus (the For example, when the initial tagger tags the patch cor-training corpus).5% was held back to be used for the pus, it mistags 159 words as verbs when they should be patch acquisition procedure (the patch corpus) and 5% nouns.If the patch change the tag from verb to noun if for testing.Once the initial tagger is trained, it is used to one of the two preceding words is tagged as a determiner tag the patch corpus.A list of tagging errors is compiled is applied, it corrects 98 of the 159 errors.However, by comparing the output of the tagger to the correct it results in an additional 18 errors from changing tags tagging of the patch corpus.This list consists of triples which really should have been verb to noun.This patch  < tag ;tag ;number >, indicating the number of times results in a net decrease of 80 errors on the patch corpus.the tagger mistagged a word with tag when it should  The patch which results in the greatest improvement to  The Brown Corpus contains about 1.1 million words from a  the patch corpus is added to the list of patches.The  variety of genres of written English.There are 192 tags in the tag  set, 96 of which occur more than one hundred times in the corpus.patch is then applied in order to improve the tagging of  The test set contained text from all genres in the Brown  the patch corpus, and the patch acquisition procedure  Corpus.The rst ten patches found by the system are listed  below .  Patch Application and Error Reduction  (1) TO IN NEXT-TAG AT  (2) VBN VBD PREV-WORD-IS-CAP YES  (6) TO IN NEXT-WORD-IS-CAP YES  (10) NP NN CURRENT-WORD-IS-CAP NO  The rst patch states that if a word is tagged  and the  following word is tagged  , then switch the tag from  to . This is because a noun phrase is much more  likely to immediately follow a preposition than to  immediately follow innitive  . The second patch states  that a tag should be switched from  to  if the  preceding word is capitalized.This patch arises from two  Number of Patches  facts: the past verb tag is more likely than the past  participle verb tag after a proper noun, and is also the more  likely tag for the second word of the sentence.The third  TS  patch states that  should be changed to  if The tagger was tested on 5% of the Brown Corpus  inany of the preceding three words are tagged  cluding sections from every genre.First, the test corpus  Once the list of patches has been acquired, new text was tagged by the simple lexical tagger.Next, each of can be tagged as follows.First, tag the text using the the patches was in turn applied to the corpus.Below is a basic lexical tagger.Next, apply each patch in turn to graph showing the improvement in accuracy from apply-the corpus to decrease the error rate.A patch which ing patches.It is signicant that with only 71 patches, changes the tagging of a word from to only applies an error rate of 5.1% was obtained . Of the 71 patches, 5  66 resulted in a reduction in the number of errors in the  if the word has been tagged somewhere in the training test corpus, 3 resulted in no net change, and 2 resulted b  in a higher number of errors.Almost all patches which  Note that one need not be too careful when constructing were eective on the training corpus were also eective the list of patch templates.Adding a bad template to the on the test corpus.list will not worsen performance.If a template is bad,  then no rules which are instantiations of that template Unfortunately, it is dicult to compare our results with will appear in the nal list of patches learned by the other published results.In [12], an error rate of 3-4%  tagger.This makes it easy to experiment with extensions on one domain, Wall Street Journal articles and 5.6%  to the tagger.on another domain, texts on terrorism in Latin  American countries, is quoted.However, both the domains  and the tag set are dierent from what we use.[1]  reAT = article, HVD = had, IN = preposition, MD = modal,  ports an accuracy of \95-99% correct, depending on the  NN = sing.noun, NP = proper noun, PPS = 3rd sing. nom.denition of correct"".We implemented a version of the  We ran the experiment three times.Each time we divided the  Both the rst word of a sentence and proper nouns are  corpus into training, patch and test sets in a dierent way.All  three runs gave an error rate of 5%.algorithm described in [1] which did not make use of a was divided into a training corpus of about one million dictionary to extend its lexical knowledge.When trained words, a patch corpus of about 65,000 words and a test and tested on the same samples used in our experiment, corpus of about 65,000 words.Patches were acquired we found the error rate to be about 4.5%.[3] quotes as described above.When tested on the test corpus, a 4% error rate when testing and training on the same with lexical information derived solely from the training text.[6] reports an accuracy of 96-97%.Their corpus, the error rate was 5%.Next, the same patches bilistic tagger has been augmented with a handcrafted were used, but lexical information was gathered from procedure to pretag problematic \idioms"".This the entire Brown Corpus.This reduced the error rate to dure, which requires that a list of idioms be laboriously 4.1%.Finally, the same experiment was run with lexical created by hand, contributes 3% toward the accuracy of information gathered solely from the test corpus.This their tagger, according to [3].The idiom list would have resulted in a 3.5% error rate.Note that the patches used to be rewritten if one wished to use this tagger for a in the two experiments with no unknown words were dierent tag set or a dierent corpus.It is interesting not the optimal patches for these tests, since they were to note that the information contained in the idiom list derived from a corpus that contained unknown words.can be automatically acquired by the rule-based tagger.For example, their tagger had diculty tagging as old  as.An explicit rule was written to pretag as old as with  CONCLUSIONS  the proper tags.According to the tagging scheme of the We have presented a simple rule-based part of speech Brown Corpus, the rst as should be tagged as a tagger which performs as well as existing stochastic tag-er, and the second as a subordinating conjunction.In gers, but has signicant advantages over these taggers.the rule-based tagger, the most common tag for as is  subordinating conjunction.So initially, the second as is The tagger is extremely portable.Many of the higher tagged correctly and the rst as is tagged incorrectly.To level procedures used to improve the performance of remedy this, the system acquires the patch: if the stochastic taggers would not readily transfer over to a rent word is tagged as a subordinating conjunction, and  dierent tag set or genre, and certainly would not  transso is the word two positions ahead, then change the tag of  fer over to a dierent language.Everything except for  the current word to qualier.The rule-based tagger has  the proper noun discovery procedure is  automaticallyacautomatically learned how to properly tag this \idiom."" quired by the rule-based tagger , making it much more 7  portable than a stochastic tagger.If the tagger were  Regardless of the precise rankings of the various taggers, trained on a dierent corpus, a dierent set of patches we have demonstrated that a simple rule-based tagger suitable for that corpus would be found automatically.  with very few rules performs on par with stochastic  taggers.It should be mentioned that our results were Large tables of statistics are not needed for the rule-tained without the use of a dictionary.Incorporating a based tagger.In a stochastic tagger, tens of thousands large dictionary into the system would improve of lines of statistical information are needed to capture mance in two ways.First, it would increase the accuracy contextual information.This information is usually a ta-in tagging words not seen in the training corpus, since ble of trigram statistics, indicating for all tags tag , tag a  part of speech information for some words not appearing and tag the probability that tag follows tag and tag .  in the training corpus can be obtained from the In the rule-based tagger, contextual information is cap-nary.Second, it would increase the error reduction tured in fewer than eighty rules.This makes for a much sulting from applying patches.When a patch indicates more perspicuous tagger, aiding in better understanding that a word should be tagged with tag instead of tag , and simplifying further development of the tagger.Contextual informationis expressed in a much more compact  the tag is only switched if the word was tagged with tag  and understandable form.As can be seen from  comparsomewhere in the training corpus.Using a dictionary  would provide more accurate knowledge about the set ing error rates, this compact representation of contextual of permissible part of speech tags for a particular word.information is just as eective as the information hidden We plan to incorporate a dictionary into the tagger in in the large tables of contextual probabilities.the future.Perhaps the biggest contribution of this work is in  As an estimate of the improvement possible by using demonstrating that the stochastic method is not the only a dictionary, we ran two experiments where all words viable approach for part of speech tagging.The fact that were known by the system.First, the Brown Corpus the simple rule-based tagger can perform so well should oer encouragement for researchers to further explore  rule-based tagging, searching for a better and more  exThis was one of the 71 patches acquired by the rule-based  And even this could be learned by the tagger.pressive set of patch templates and other variations on  this simple but eective theme.1. Church, K. A Stochastic Parts Program and Noun  Phrase Parser for Unrestricted Text.In Proceedings of  the Second Conference on Applied Natural Language  Processing, ACL, 136-143, 1988.2. Cutting, D., Kupiec, J., Pederson, J. and Sibun, P. A  Practical Part-of-Speech Tagger.In Proceedings of the  Third Conference on Applied Natural Language  Process3.DeRose, S.J. Grammatical Category Disambiguation by  Statistical Optimization.Computational Linguistics 14:  4.Deroualt, A. and Merialdo, B. Natural language  modeling for phoneme-to-text transcription.IEEE  Transactions on Pattern Analysis and Machine Intelligence,Vol.  5. Francis, W. Nelson and Kucera, Henry, Frequency  analysis of English usage.Lexicon and grammar.Houghton  6.Garside, R., Leech, G. & Sampson, G. The  Computational Analysis of English: A Corpus-Based Approach.7. Green, B. and Rubin, G. Automated Grammatical  Tagging of English.Department of Linguistics, Brown  Uni8.Hindle, D. Acquiring disambiguation rules from text.Proceedings of the 27th Annual Meeting of the  Association for Computational Linguistics, 1989.9. Jelinek, F. Markov source modeling of text generation.In J.K. Skwirzinski, ed., Impact of Processing  Tech10.Klein, S. and Simmons, R.F. A Computational  Approach to Grammatical Coding of English Words.JACM  phrase-dependent word tagging.In Proceedings of the  DARPA Speech and Natural Language Workshop,  MorStudies in Part of Speech Labelling, Proceedings of the  DARPA Speech and Natural Language Workshop,"
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Cross-Linguistic Sentiment Analysis: From English to Spanish Julian Brooke  Department of Linguistics  School of Computing Science  Department of Linguistics  Simon Fraser University  Simon Fraser University  Simon Fraser University  resources, which we build both manually and  automatically. The second approach, used in Bautin et al.  We explore the adaptation of English resources  [4] and Wan [18], consists of translating the texts into and techniques for text sentiment analysis to a  new language, Spanish. Our main focus is the  English, and using an existing English calculator. Finally, modification of an existing English semantic  the third approach builds unigram Support Vector  orientation calculator and the building of  Machine classifiers from our Spanish corpora.  dictionaries; however we also compare alternate  approaches, including machine translation and  that, although translation and machine learning  Support Vector Machine classification. The  classification both perform reasonably well, there is a results indicate that, although language-significant cost to automated translation. A language-independent methods provide a decent baseline  specific SO Calculator with dictionaries built using  performance, there is also a significant cost to  words that actually appear in relevant texts gives the best automation, and thus the best path to long-term  performance, with significant potential for improvement.  improvement is through the inclusion of  language-specific knowledge and resources.  2. The English SO Calculator  1. ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","resources, which we build both manually and automatically. The second approach, used in Bautin et al.We explore the adaptation of English resources  [4] and Wan [18], consists of translating the texts into and techniques for text sentiment analysis to a  new language, Spanish.Our main focus is the  English, and using an existing English calculator.Finally, modification of an existing English semantic  the third approach builds unigram Support Vector  orientation calculator and the building of  Machine classifiers from our Spanish corpora.dictionaries; however we also compare alternate  approaches, including machine translation and  that, although translation and machine learning  Support Vector Machine classification.The  classification both perform reasonably well, there is a results indicate that, although language-significant cost to automated translation.A language-independent methods provide a decent baseline  specific SO Calculator with dictionaries built using  performance, there is also a significant cost to  words that actually appear in relevant texts gives the best automation, and thus the best path to long-term  performance, with significant potential for improvement.improvement is through the inclusion of  language-specific knowledge and resources.2. The English SO Calculator  1."
" Lexicon-Based Methods for Sentiment Analysis  Simon Fraser University  University of Toronto  Simon Fraser University  Kimberly Voll  University of British Columbia  University of Potsdam  We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation. SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter. We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.  1. "," Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon's Mechanical Turk  Chris Callison-Burch  Center for Language and Speech Processing  Johns Hopkins University  Baltimore, Maryland  of money to complete human intelligence tests  tasks that are difficult for computers but easy for  Manual evaluation of translation quality is  people. We show that:  generally thought to be excessively time  consuming and expensive. We explore a  fast and inexpensive way of doing it using  that are very similar to experts and that have  Amazon's Mechanical Turk to pay small  a stronger correlation than Bleu.  sums to a large number of non-expert  annotators. For $10 we redundantly  recre Mechanical Turk can be used for complex  ate judgments from a WMT08  translation task. We find that when combined  rate (HTER) and creating multiple reference  non-expert judgments have a high-level of  agreement with the existing gold-standard  Evaluating translation quality through  readjudgments of machine translation quality,  ing comprehension, which is rarely done, can  and correlate more strongly with expert  be easily accomplished through creative use  judgments than Bleu does. We go on to  of Mechanical Turk.  show that Mechanical Turk can be used to  calculate human-mediated translation edit  2 Related work  rate (HTER), to conduct reading  compreSnow et al. (2008) examined the accuracy of  lahension experiments with machine  transbels created using Mechanical Turk for a variety  lation, and to create high quality reference  of natural language processing tasks. These tasks  included word sense disambiguation, word  simi1 ",1,"Lexicon-Based Methods for Sentiment Analysis We present a lexicon-based approach to extracting sentiment from text. The Semantic Orienta tion CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensi.cation and negation.SO-CAL is applied to the polarity classi.cation task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subject matter.We show that SO-CAL's performance is consistent across domains and on completely unseen data.Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability.1.","Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon's Mechanical Turk Center for Language and Speech Processing of money to complete human intelligence tests tasks that are difficult for computers but easy for Manual evaluation of translation quality is people. We show that:  generally thought to be excessively time  consuming and expensive.We explore a  fast and inexpensive way of doing it using  that are very similar to experts and that have  Amazon's Mechanical Turk to pay small  a stronger correlation than Bleu.sums to a large number of non-expert  annotators.For $10 we redundantly  recre Mechanical Turk can be used for complex  ate judgments from a WMT08  translation task.We find that when combined  rate (HTER) and creating multiple reference  non-expert judgments have a high-level of  agreement with the existing gold-standard  Evaluating translation quality through  readjudgments of machine translation quality,  ing comprehension, which is rarely done, can  and correlate more strongly with expert  be easily accomplished through creative use  judgments than Bleu does.We go on to  of Mechanical Turk.show that Mechanical Turk can be used to  calculate human-mediated translation edit  2 Related work  rate (HTER), to conduct reading  compreSnow et al.(2008) examined the accuracy of  lahension experiments with machine  transbels created using Mechanical Turk for a variety  lation, and to create high quality reference  of natural language processing tasks.These tasks  included word sense disambiguation, word  simi1"
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," The Berkeley FrameNet Project  Collin F. Baker and Charles J. Fillmore and John B. Lowe  {collinb, fillmore, jblowe}~icsi.berkeley.edu  International Computer Science Institute  Abstract These descriptions are based on hand-tagged semantic annotations of example sentences  exFrameNet is a three-year NSF-supported tracted from large text corpora and systematic project in corpus-based computational analysis of the semantic patterns they exem-raphy, now in its second year (NSF IRI-9618838, plify by lexicographers and linguists. The ""Tools for Lexicon Building""). The project's mary emphasis of the project therefore is the key features are (a) a commitment to corpus encoding, by humans, of semantic knowledge evidence for semantic and syntactic in machine-readable form. The intuition of the tions, and (b) the representation of the valences lexicographers is guided by and constrained by of its target words (mostly nouns, adjectives, the results of corpus-based research using and verbs) in which the semantic portion makes performance software tools.  use of frame semantics. The resulting database The semantic domains to be covered are:will contain (a) descriptions of the semantic  frames underlying the meanings of the words  described, and (b) the valence representation (parts and functions of the body), MOTION, LIFEmantic and syntactic) of several thousand words  STAGES, SOCIAL CONTEXT~ EMOTION and  COGand phrases, each accompanied by (c) a  representative collection of annotated corpus tations, which jointly exemplify the observed  1.1 Scope of the Project linkings between ""frame elements"" and their  The results of the project are (a) a lexical syntactic realizations (e.g. grammatical source, called the FrameNet database 3, and (b) tion, phrase type, and other syntactic traits).  associated software tools. The database hasThis report will present the project's goals and three major components (described in more workflow, and information about the tail below: tional tools that have been adapted or created Lexicon containing entries which are com-in-house for this work.  posed of: (a) some conventional dictionary-type data, mainly for the sake of human readers; (b)  FOR",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","Abstract These descriptions are based on hand-tagged semantic annotations of example sentences The project's mary emphasis of the project therefore is the key features are (a) a commitment to corpus encoding, by humans, of semantic knowledge evidence for semantic and syntactic in machine-readable form.The intuition of the tions, and (b) the representation of the valences lexicographers is guided by and constrained by of its target words (mostly nouns, adjectives, the results of corpus-based research using and verbs) in which the semantic portion makes performance software tools.use of frame semantics.The resulting database The semantic domains to be covered are:will contain (a) descriptions of the semantic  frames underlying the meanings of the words  described, and (b) the valence representation (parts and functions of the body), MOTION, LIFEmantic and syntactic) of several thousand words  STAGES, SOCIAL CONTEXT~ EMOTION and  COGand phrases, each accompanied by (c) a  representative collection of annotated corpus tations, which jointly exemplify the observed  1.1 Scope of the Project linkings between ""frame elements"" and their  The results of the project are (a) a lexical syntactic realizations (e.g. grammatical source, called the FrameNet database 3, and (b) tion, phrase type, and other syntactic traits).associated software tools.The database hasThis report will present the project's goals and three major components (described in more workflow, and information about the tail below: tional tools that have been adapted or created Lexicon containing entries which are com-in-house for this work.posed of: (a) some conventional dictionary-type data, mainly for the sake of human readers; (b)  FOR"
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," a High-Performance Learning Name-finder  BBN Corporation  BBN Corporation BBN Corporation BBN Corporation  This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.  1. ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","a High-Performance Learning Name-finder We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.1."
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," Playing the Telephone Game: Determining the Hierarchical Structure of Perspective and Speech Expressions  Eric Breck and Claire Cardie  Department of Computer Science  Cornell University  Ithaca, NY 14853  to refer to these mental and emotional states that  News articles report on facts, events, and  opincannot be directly observed or verified (Quirk et al.,  ions with the intent of conveying the truth.  1985). Further, we define the source of a perspec-However, the facts, events, and opinions  appeartive expression to be the experiencer of that private  ing in the text are often known only  secondstate, that is, the person or entity whose opinion  or third-hand, and as any child who has played  or emotion is being conveyed in the text. Second,  telephone knows, this relaying of facts often  speech expressions simply convey the words of an-garbles the original message. Properly  underother individual and by the choice of words, the  standing the information filtering structures that  reporter filters the original source's intent. Consider  govern the interpretation of these facts, then, is  for example, the following sentences (in which  percritical to appropriately analyzing them. In this  work, we present a learning approach that  corspective expressions are denoted in bold, speech ex-rectly determines the hierarchical structure of  pressions are underlined, and sources are denoted in  information filtering expressions 78.30% of the  1. Charlie was angry at Alice's claim that Bob was unhappy.  ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","Playing the Telephone Game: Determining the Hierarchical Structure of Perspective and Speech Expressions to refer to these mental and emotional states that ions with the intent of conveying the truth. 1985). Further, we define the source of a perspec-However, the facts, events, and opinions  appeartive expression to be the experiencer of that private  ing in the text are often known only  secondstate, that is, the person or entity whose opinion  or third-hand, and as any child who has played  or emotion is being conveyed in the text.Second,  telephone knows, this relaying of facts often  speech expressions simply convey the words of an-garbles the original message.Properly  underother individual and by the choice of words, the  standing the information filtering structures that  reporter filters the original source's intent.Consider  govern the interpretation of these facts, then, is  for example, the following sentences (in which  percritical to appropriately analyzing them.In this  work, we present a learning approach that  corspective expressions are denoted in bold, speech ex-rectly determines the hierarchical structure of  pressions are underlined, and sources are denoted in  information filtering expressions 78.30% of the  1.Charlie was angry at Alice's claim that Bob was unhappy."
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," Early Results for  Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons  Department of Computer Science  University of Massachusetts Amherst  {mccallum,weili}@cs.umass.edu  ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons {mccallum,weili}@cs.umass.edu "
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," The Proposition Bank: An Annotated  Corpus of Semantic Roles  Martha Palmer  University of Pennsylvania  University of Rochester  University of Pennsylvania  The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.  We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace'' categories of the treebank.  1. ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace'' categories of the treebank.1."
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye s We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al. (1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  ",0,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","79-86.Association for Computational Linguistics.Thumbs up?Sentiment Classification using Machine Learning Techniques  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use.Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye s We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al.(1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization.Moreover, there are also  perform human-produced baselines.Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem.A challenging aspect of this categorization.We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner.For example, the"
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts  Bo Pang and Lillian Lee  Department of Computer Science  Cornell University  Ithaca, NY 14853-7501  classifier to the resulting extract. This can prevent the polarity classifier from considering irrelevant or Sentiment analysis seeks to identify the view-even potentially misleading text: for example, al-point(s) underlying a text span; an example appli-though the sentence The protagonist tries to procation is classifying a movie review as thumbs up  tect her good name contains the word good , it  or thumbs down . To determine this sentiment po-tells us nothing about the author's opinion and in larity, we propose a novel machine-learning method fact could well be embedded in a negative movie  that applies text-categorization techniques to just review. Also, as mentioned above, subjectivity ex-the subjective portions of the document. Extracting tracts can be provided to users as a summary of the these portions can be implemented using efficient sentiment-oriented content of the document.  techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence Our results show that the subjectivity extracts  we create accurately represent the sentiment  information of the originating documents in a much  ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts classifier to the resulting extract. This can prevent the polarity classifier from considering irrelevant or Sentiment analysis seeks to identify the view-even potentially misleading text: for example, al-point(s) underlying a text span; an example appli-though the sentence The protagonist tries to procation is classifying a movie review as thumbs up  tect her good name contains the word good , it  or thumbs down . To determine this sentiment po-tells us nothing about the author's opinion and in larity, we propose a novel machine-learning method fact could well be embedded in a negative movie  that applies text-categorization techniques to just review.Also, as mentioned above, subjectivity ex-the subjective portions of the document.Extracting tracts can be provided to users as a summary of the these portions can be implemented using efficient sentiment-oriented content of the document.techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence Our results show that the subjectivity extracts  we create accurately represent the sentiment  information of the originating documents in a much"
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," Learning Extraction Patterns for Subjective Expressions  School of Computing  Department of Computer Science  University of Utah  University of Pittsburgh  Salt Lake City, UT 84112  must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to iden-This paper presents a bootstrapping process  tify information could benefit from being able to separate that learns linguistically rich extraction pat-factual and subjective information.  terns for subjective (opinionated) expressions.  Some existing resources contain lists of subjective  High-precision classifiers label unannotated  words (e.g., Levin's desire verbs (1993)), and some em-data to automatically create a large training set,  pirical methods in NLP have automatically identified ad-which is then given to an extraction pattern  jectives, verbs, and N-grams that are statistically associ-learning algorithm. The learned patterns are  ated with subjective language (e.g., (Turney, 2002; Hatzi-then used to identify more subjective sentences.  vassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe  The bootstrapping process learns many  subjecet al., 2001)). However, subjective language can be extive patterns and increases recall while  mainhibited by a staggering variety of words and phrases. In taining high precision.  addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g., preposterous, un-1  ",0,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","Learning Extraction Patterns for Subjective Expressions must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to iden-This paper presents a bootstrapping process  tify information could benefit from being able to separate that learns linguistically rich extraction pat-factual and subjective information.terns for subjective (opinionated) expressions.Some existing resources contain lists of subjective  High-precision classifiers label unannotated  words (e.g., Levin's desire verbs (1993)), and some em-data to automatically create a large training set,  pirical methods in NLP have automatically identified ad-which is then given to an extraction pattern  jectives, verbs, and N-grams that are statistically associ-learning algorithm.The learned patterns are  ated with subjective language (e.g., (Turney, 2002; Hatzi-then used to identify more subjective sentences.vassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe  The bootstrapping process learns many  subjecet al., 2001)).However, subjective language can be extive patterns and increases recall while  mainhibited by a staggering variety of words and phrases.In taining high precision.addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g., preposterous, un-1"
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 417-424.  Thumbs Up or Thumbs Down? Semantic Orientation Applied to  Unsupervised Classification of Reviews  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches. It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion. With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down). The  classifisuch summary statistics. This is the motivation for  cation of a review is predicted by the  the research described here. Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs. A phrase has a positive  ing new kinds of search tools (Hearst, 1992).  semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended. The algorithm  bad associations (e.g., very cavalier ). In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output. The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994). The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997). A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive. The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ). The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations). The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view. If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.  Otherwise, the prediction is that the item is not  recommended.  ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","417-424.Thumbs Up or Thumbs Down?Semantic Orientation Applied to  Unsupervised Classification of Reviews  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches.It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion.With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down).The  classifisuch summary statistics.This is the motivation for  cation of a review is predicted by the  the research described here.Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs.A phrase has a positive  ing new kinds of search tools (Hearst, 1992).semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended.The algorithm  bad associations (e.g., very cavalier ).In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output.The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994).The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997).A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive.The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ).The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations).The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view.If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.Otherwise, the prediction is that the item is not  recommended."
" In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  "," Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences Hong Yu  Department of Computer Science  Department of Computer Science  Columbia University  Columbia University  New York, NY 10027, USA  New York, NY 10027, USA  trieving only editorials in favor of a particular policy Opinion question answering is a challenging task  for natural language processing. In this paper, we Our motivation for building the opinion detec-discuss a necessary component for an opinion question answering system: separating opinions from  tion and classification system described in this pa-fact, at both the document and sentence level. We per is the need for organizing information in the present a Bayesian classifier for discriminating be-context of question answering for complex  questween documents with a preponderance of opinions  such as editorials from regular news stories, and tions.  Unlike questions like Who was the first  describe three unsupervised, statistical techniques man on the moon? which can be answered with  for the significantly harder task of detecting opin-a simple phrase, more intricate questions such as ions at the sentence level. We also present a first model for classifying opinion sentences as positive  What are the reasons for the US-Iraq war? require or negative in terms of the main perspective be-long answers that must be constructed from  multiing expressed in the opinion. Results from a large ple sources. In such a context, it is imperative that collection of news stories and a human evaluation of 400 sentences are reported, indicating that we the question answering system can discriminate be-achieve very high performance in document  classitween opinions and facts, and either use the appro-fication (upwards of 97% precision and recall), and respectable performance in detecting opinions and priate type depending on the question or combine  classifying them at the sentence level as positive, them in a meaningful presentation. Perspective in-negative, or neutral (up to 91% accuracy).  formation can also help highlight contrasts and con-tradictions between different sources there will be significant disparity in the material collected for the 1  ",1,"In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1","trieving only editorials in favor of a particular policy Opinion question answering is a challenging task for natural language processing. In this paper, we Our motivation for building the opinion detec-discuss a necessary component for an opinion question answering system: separating opinions from  tion and classification system described in this pa-fact, at both the document and sentence level.We per is the need for organizing information in the present a Bayesian classifier for discriminating be-context of question answering for complex  questween documents with a preponderance of opinions  such as editorials from regular news stories, and tions.Unlike questions like Who was the first  describe three unsupervised, statistical techniques man on the moon?which can be answered with  for the significantly harder task of detecting opin-a simple phrase, more intricate questions such as ions at the sentence level.We also present a first model for classifying opinion sentences as positive  What are the reasons for the US-Iraq war?require or negative in terms of the main perspective be-long answers that must be constructed from  multiing expressed in the opinion.Results from a large ple sources.In such a context, it is imperative that collection of news stories and a human evaluation of 400 sentences are reported, indicating that we the question answering system can discriminate be-achieve very high performance in document  classitween opinions and facts, and either use the appro-fication (upwards of 97% precision and recall), and respectable performance in detecting opinions and priate type depending on the question or combine  classifying them at the sentence level as positive, them in a meaningful presentation.Perspective in-negative, or neutral (up to 91% accuracy).formation can also help highlight contrasts and con-tradictions between different sources there will be significant disparity in the material collected for the 1"
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," The Berkeley FrameNet Project  Collin F. Baker and Charles J. Fillmore and John B. Lowe  {collinb, fillmore, jblowe}~icsi.berkeley.edu  International Computer Science Institute  Abstract These descriptions are based on hand-tagged semantic annotations of example sentences  exFrameNet is a three-year NSF-supported tracted from large text corpora and systematic project in corpus-based computational analysis of the semantic patterns they exem-raphy, now in its second year (NSF IRI-9618838, plify by lexicographers and linguists. The ""Tools for Lexicon Building""). The project's mary emphasis of the project therefore is the key features are (a) a commitment to corpus encoding, by humans, of semantic knowledge evidence for semantic and syntactic in machine-readable form. The intuition of the tions, and (b) the representation of the valences lexicographers is guided by and constrained by of its target words (mostly nouns, adjectives, the results of corpus-based research using and verbs) in which the semantic portion makes performance software tools.  use of frame semantics. The resulting database The semantic domains to be covered are:will contain (a) descriptions of the semantic  frames underlying the meanings of the words  described, and (b) the valence representation (parts and functions of the body), MOTION, LIFEmantic and syntactic) of several thousand words  STAGES, SOCIAL CONTEXT~ EMOTION and  COGand phrases, each accompanied by (c) a  representative collection of annotated corpus tations, which jointly exemplify the observed  1.1 Scope of the Project linkings between ""frame elements"" and their  The results of the project are (a) a lexical syntactic realizations (e.g. grammatical source, called the FrameNet database 3, and (b) tion, phrase type, and other syntactic traits).  associated software tools. The database hasThis report will present the project's goals and three major components (described in more workflow, and information about the tail below: tional tools that have been adapted or created Lexicon containing entries which are com-in-house for this work.  posed of: (a) some conventional dictionary-type data, mainly for the sake of human readers; (b)  FOR",0,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","Abstract These descriptions are based on hand-tagged semantic annotations of example sentences The project's mary emphasis of the project therefore is the key features are (a) a commitment to corpus encoding, by humans, of semantic knowledge evidence for semantic and syntactic in machine-readable form.The intuition of the tions, and (b) the representation of the valences lexicographers is guided by and constrained by of its target words (mostly nouns, adjectives, the results of corpus-based research using and verbs) in which the semantic portion makes performance software tools.use of frame semantics.The resulting database The semantic domains to be covered are:will contain (a) descriptions of the semantic  frames underlying the meanings of the words  described, and (b) the valence representation (parts and functions of the body), MOTION, LIFEmantic and syntactic) of several thousand words  STAGES, SOCIAL CONTEXT~ EMOTION and  COGand phrases, each accompanied by (c) a  representative collection of annotated corpus tations, which jointly exemplify the observed  1.1 Scope of the Project linkings between ""frame elements"" and their  The results of the project are (a) a lexical syntactic realizations (e.g. grammatical source, called the FrameNet database 3, and (b) tion, phrase type, and other syntactic traits).associated software tools.The database hasThis report will present the project's goals and three major components (described in more workflow, and information about the tail below: tional tools that have been adapted or created Lexicon containing entries which are com-in-house for this work.posed of: (a) some conventional dictionary-type data, mainly for the sake of human readers; (b)  FOR"
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Assigning Function Tags to Parsed Text*  Don Blaheta and Eugene Charniak  Department of Computer Science  Box 1910 / 115 Waterman St.--4th floor  Brown University  It is generally recognized that the common terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and mantic information one would like about parts of a syntactic tree. For example, the Penn bank gives each constituent zero or more tion tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a tistical algorithm for assigning these function tags that, on text already parsed to a label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice.  ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","Assigning Function Tags to Parsed Text* For example, the Penn bank gives each constituent zero or more tion tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels.We present a tistical algorithm for assigning these function tags that, on text already parsed to a label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice."
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Valence Induction with a Head-Lexicalized PCFG  Glenn Carroll and Mats Rooth IMS, Universitiit Stuttgart {glenn,mats}@ims.uni-stuttgart.de  ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.",Valence Induction with a Head-Lexicalized PCFG 
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Three Generative, Lexicalised Models for Statistical Parsing  Dept. of Computer and In format ion Science  Univers i ty of Pennsy lvan ia  Abst ract  In this paper we first propose a new  statistical parsing model, which is a  generative model of lexicalised context-free  grammar. We then extend the model to  include a probabilistic treatment of both  subcategorisation and wh-movement. Results  on Wall Street Journal text show that the  of 2.3% over (Collins 96).  1 ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","Three Generative, Lexicalised Models for Statistical Parsing Dept. of Computer and In format ion Science Abst ract In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to  include a probabilistic treatment of both  subcategorisation and wh-movement.Results  on Wall Street Journal text show that the  of 2.3% over (Collins 96).1"
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Marti A. Hearst  School of Information Management & Systems  University of California, Berkeley  102 South Hall  h ttp ://www. sims. berkeley, edu/-hearst  The possibilities for data mining from large text collections are virtually untapped. Text ex-presses a vast, rich range of information, but codes this information in a form that is difficult to decipher automatically. Perhaps for this son, there has been little work in text data ing to date, and most people who have talked about it have either conflated it with tion access or have not made use of text directly to discover heretofore unknown information.  In this paper I will first define data mining, information access, and corpus-based tional linguistics, and then discuss the ship of these to text data mining. The intent behind these contrasts is to draw attention to exciting new kinds of problems for tional linguists. I describe examples of what I consider to be reM text data mining efforts and  briefly outline recent ideas about how to pursue exploratory data analysis over text.  ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","h ttp ://www. sims. berkeley, edu/-hearst  The possibilities for data mining from large text collections are virtually untapped.Text ex-presses a vast, rich range of information, but codes this information in a form that is difficult to decipher automatically.Perhaps for this son, there has been little work in text data ing to date, and most people who have talked about it have either conflated it with tion access or have not made use of text directly to discover heretofore unknown information.In this paper I will first define data mining, information access, and corpus-based tional linguistics, and then discuss the ship of these to text data mining.The intent behind these contrasts is to draw attention to exciting new kinds of problems for tional linguists.I describe examples of what I consider to be reM text data mining efforts and  briefly outline recent ideas about how to pursue exploratory data analysis over text."
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Building a class-based verb Iexicon using TAGs  Department of Computer and Information Sciences  University of Pennsy!vania  200 South 33rd Street  {ki pper,htd,schuler,mpalmer}@linc.cis.upenn.edu  We present a class-based approach to building a verb lexicon that makes explicit the close relation between syntax and semanrics for Levin classes. We have used a Lexicalized Tree Adjoining Grammar to capture the syntax associated witfi P.ach verb class and have added semantic predicates to each tree, which allow for a compositional inte1pre1u::.::::.  1. ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","{ki pper,htd,schuler,mpalmer}@linc.cis.upenn.edu We have used a Lexicalized Tree Adjoining Grammar to capture the syntax associated witfi P.ach verb class and have added semantic predicates to each tree, which allow for a compositional inte1pre1u::.::::.1."
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Using Subcategorization to Resolve Verb Class Ambiguity  School of Cognitive Science  Division of Informatics  University of Edinburgh  2 Buccleuch Place  Levin's (1993) taxonomy of verbs and their classes is a widely used resource for lexical semantics. In her framework, some verbs, such as give exhibit no class ambiguity. But other verbs, such as write, can inhabit more than one class. In some of these biguous cases the appropriate class for a particular token of a verb is immediately obvious from tion of the surrounding context. In others it is not, and an application which wants to recover this mation will be forced to rely on some more or less elaborate process of inference. We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference.  ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","Using Subcategorization to Resolve Verb Class Ambiguity In her framework, some verbs, such as give exhibit no class ambiguity.But other verbs, such as write, can inhabit more than one class.In some of these biguous cases the appropriate class for a particular token of a verb is immediately obvious from tion of the surrounding context.In others it is not, and an application which wants to recover this mation will be forced to rely on some more or less elaborate process of inference.We present a simple statistical model of verb class ambiguity and show how it can be used to carry out such inference."
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. ", Building a Large Annotated Corpus of English: The Penn Treebank  Mitchell P. Marcus* Beatrice Santorini t  University of Pennsylvania Northwestern University  University of Pennsylvania  1. ,1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.",1. 
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," ARGUMENT STRUCTURE  Department of Computer and Information Science  University of Pennsylvania  The Penn Treebank has recently implemented a new syn-tactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of dexed null elements in what can be thought of as ing"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the ture of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.  During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this terial also annotated with a skeletal syntactic bracketing. All of this material has been hand corrected after processing by automatic tools. The largest component of the corpus sists of materials from the Dow-Jones News Service; over 1.6 million words of this material has been hand parsed, with an additional 1 million words tagged for part of speech. Also included is a skeletally parsed version of the Brown corpus, the classic million word balanced corpus of American English [5, 6]. hand-retagged using the Penn Treebank tagset.  The level of syntactic analysis annotated during this phase of this project was an extended and somewhat modified form of the skeletal analysis which has been produced by the banking effort in Lancaster, England [7]. The released rials in the current Penn Treebank, although still in very liminary form, have been widely distributed, both directly by us, on the ACL/DCI CD-ROM, and now on CD-ROM by the Linguistic Data Consortium; it has been used for purposes ranging from serving as a gold-standard for parser testing to serving as a basis for the induction of stochastic grammars to serving as a basis for quick lexicon induction.  Many users of the Penn Treebank now want forms of notation richer than provided by the project's first phase, as well as an increase in the consistency of the preliminary corpus. Some would also like a less skeletal form of tation, expanding the essentially context-free analysis of the current treebank to indicate non-contiguous structures and dependencies. Most crucially, there is a strong sense that the Treebank could be of much more use if it explicitly provided some form of predicate-argument structure. The desired level of representation would make explicit at least the logical ject and logical object of the verb, and indicate, at least in clear cases, how subconstituents are semantically related to their predicates. Such a representation could serve as both a starting point for the kinds of SEMEVAL representations now being discussed as a basis for evaluation of human guage technology within the ARPA HLT program, and as a basis for ""glass box"" evaluation of parsing technology.  The ongoing effort [1] to develop a standard objective methodology to compare parser outputs across widely gent grammatical frameworks has now resulted in a widely supported standard for parser comparison. On the other hand, many existing parsers cannot be evaluated by this metric because they directly produce a level of tion closer to predicate-argument structure than to classical surface grammatical analysis. Hand-in-hand with this tation of the existing Penn Treebank for parser testing is a parallel limitation for automatic methods for parser training for parsers based on deeper representations. There is also a problem of maintaining consistency with the fairly small (less than 100 page) style book used in the the first phase of the project.  We have recently completed a detailed style-book for this new level of analysis, with consensus across annotators about the particulars of the analysis. This project has taken about eight months of ten-hour a week effort across a significant subset of all the personnel of the Penn Treebank. Such a stylebook, much larger, and much more fully specified than our initial stylebook, is a prerequisite for high levels of annotator agreement. It is our hope that such a stylebook will also alleviate much of the need for extensive cross-talk between annotators during the annotation task, thereby creasing throughput as well. To ensure that the rules of this new stylebook remain in force, we are now giving annotators about 10% overlapped material to evaluate inter-annotator consistency throughout this new project.  We have now begun to annotate this level of structure editing the present Penn Treebank; we intend to automatically tract a bank of predicate-argument structures intended at the very least for parser evaluation from the resulting annotated corpus.  The remainder of this paper will discuss the implementation of each of four crucial aspects of the new annotation scheme, as well as notational devices to allow predicate-argument structure to be recovered in the face of conjoined ture involving gapping, where redundant syntactic structure within a conjoined structure is deleted. In particular, the new scheme:  Incorporates a consistent treatment of related ical phenomena. The issue here is not that the sentation be ""correct"" given some theoretical analysis or other, but merely that instances of what are tively the same phenomenon be represented similarly. In particular, the notation should make it easy to matically recover predicate-argument structure.  Provides a set of null elements in what can be thought of as ""underlying"" position for phenomena such as movement, passive, and the subjects of infinitival structions. These null elements must be co-indexed with the appropriate lexical material.  Provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered.  Allows for a clear, concise distinction between verb arguments and adjuncts where such distinctions are clear, with some easy-to-use notational device to cate where such a distinction is somewhat murky.  Our first step, just now complete, has been to produce a tailed style-book for this new level of analysis, with consensus across annotators about the particulars of the analysis. This project has taken about eight months of ten-hour a week effort across a significant subset of all the personnel of the Penn Treebank. It has become clear during the first stage of the project that a much larger, much more fully fied stylebook than our initial stylebook is a prerequisite for high levels of inter-annotator agreement. It is our hope that such a stylebook will also alleviate much of the need for tensive cross-talk between annotators during the annotation task, thereby increasing throughput as well. To ensure that the rules of this new stylebook remain in force, we intend to give annotators about 10% overlapped material to evaluate inter-annotator consistency throughout this new project.  The remainder of this paper discusses the implementation of each of the four points above, as well as notational vices to allow predicate-argument structure to be recovered in the face of conjoined structure involving gapping, where redundant syntactic structure within a conjoined structure is deleted.  3. CONSISTENT GRAMMATICAL ANALYSES  The current treebank materials suffer from the fact that fering annotation regimes are used across differing syntactic categories. To allow easy automatic extraction of argument structure in particular, these differing analyses must be unified. In the original annotation scheme, adjective phrases that serve as sentential predicates have a different structure than VPs, causing sentential adverbs which occur after auxiliaries introducing the ADJP to attach under VP, while sentential adverbs occurring after auxiliaries ing VPs occur under S. In the current treebank, copular be is treated as a main verb, with predicate adjective or sitional phrases treated as complements to that verb.  In the new stylebook, the predicate is either the lowest most branching) VP or the phrasal structure immediately under copular BE. In cases when the predicate cannot be identified by those criteria (e.g. in ""small clauses"" and some inversion structures), the predicate phrase is tagged -PRD (PReDicate).  (s (NP-SBJ I)  (NP-PRD a fool))))  (SQ Was  (NP-SBJ he)  Note that the surface subject is always tagged -SBJ (SuB-Ject), even though this is usually redundant because the subject can be recognized purely structurally. The -TMP tag here marks time (TeMPoral) phrases. Our use of ""small clauses"" follows one simple rule:, every S maps into a single predication, so here the predicate-argument structure would be something like  consider(I, fool(Kris)).  4. ARGUMENT-ADJUNCT STRUCTURE  In a well developed predicate-argument scheme, it would seem desirable to label each argument of a predicate with an appropriate semantic label to identify its role with respect to that predicate. It would also seem desirable to distinguish between the arguments of a predicate, and adjuncts of the predication. Unfortunately, while it is easy to distinguish arguments and adjuncts in simple cases, it turns out to be very difficult to consistently distinguish these two categories for many verbs in actual contexts. It also turns out to be very difficult to determine a set of underlying semantic roles that holds up in the face of a few paragraphs of text. In our new annotation scheme, we have tried to come up with a middle ground which allows annotation of those tions that seem to hold up across a wide body of material. After many attempts to find a reliable test to distinguish between arguments and adjuncts, we have abandoned turally marking this difference. Instead, we now label a small set of clearly distinguishable roles, building upon syntactic distinctions only when the semantic intuitions are clear cut. Getting annotators to consistently apply even the small set of distinctions we will discuss here is fairly difficult.  In the earlier corpus annotation scheme, We originally used only standard syntactic labels (e.g. NP, ADVP, PP, etc.)  for our constituents in other words, every bracket had just one label. The limitations of this became apparent when a word belonging to one syntactic category is used for another hnction or when it plays a role which we want to be able to identify easily. In the present scheme, each constituent has at least one label but as many as four tags, including numerical indices. We have adopted the set of functional tags shown in Figure 2 for use within the current annotation scheme. NPs and Ss which are clearly arguments of the verb are unmarked by any tag. We allow an open class of other cases that dividual annotators feel strongly should be part of the VP. These cases are tagged as -CLR (for CLosely Relatcd); they axe to be semantically analyzed as adjuncts. This class is an experiment in the current tagging; constituents marked -CLR typically correspond to Quirk et al's [11] class of ication adjuncts. At the moment, we distinguish a handful of semantic roles: direction, location, manner, purpose, and time, as well as the syntactic roles of surface subject, cal subject, and (implicit in the syntactic structure) first and second verbal objects.  One important way in which the level of annotation of the  current Penn Treebank exceeds that of the Lancaster project  is that we have annotated nun elements in a wide range of  cases. In the new annotation scheme, we co-index these null  elements with the lexical material for which the null element  stands. The current scheme happens to use two symbols for  null elements: *T*,which marks WH-movement and  topicalization, and * which is used for all other null elements, but  this distinction is not very important. Co-indexing of null  elements is done by suffixing an integer to non-terminal  categories (e.g~ NP-10, VP-25). This integer serves as an id  number for the constituent. A null element itself is followed by the  id number of the constituent with which it is co-indexed. We  use SBARQ to mark WH-questions, and SQ to mark  auxiliaxy inverted structures. We use the WH-prefixed labels,  WHNP, WHADVP, WHPP, etc., only when there is  WHmovement; they always leave a co-indexed trace. Crucially,  the predicate argument structure can be recovered by simply  replacing the null element with the lexical material that it is  co-indexed with:  (SBARQ (NHNP-1What)  (VP eating  eat(Tim, what)  In passives, the surface subject is tagged -SBJ,a passive trace  is inserted after the verb, indicated by (NP *), and co-indexed  to the surface subject (i.e. logical object). The logical  subject by-phrase, if present, is a child of VP, and is tagged  -LGS (LoGical Subject). For passives, the predicate  argument structure can be recovered by replacing the passive null  element with the material it is co-indexed with, and treating  the NP marked -LGS as the subject.  (s (NP-SBJ-I The ball)  (VP was  (VP thrown  (PP by  (NP-LGS Chris)))))  throw(Chris, ball)  The interpretation rules for passives and WH-phrases act correctly to yield the predicate argument structures for complex nestings of WH-questions and passives.  (SBARQ (WHNP-1 Who)  (NP-SBJ-2 *T*-I)  -TPC  -CLR  Text Categories headlines and datelines list markers titles  Grammatical Functions true clefts non NPs that function as NPs clausal and NP adverbials logical subjects in passives non VP predicates surface subject topicalized and fronted constituents closely related -see text  Semantic Roles vocatives direction & trajectory location manner purpose and reason temporal phrases  Figure I: Functional Tags  (VP to  believe (*someone*, shoot (*someone*, Who))  A null element is also used to indicate which lexical NP is to  be interpreted as the null null subject of an infinitive  complement clause; it is co-indexed with the controlling NP, based  upon the lexical properties of the verb.  (S (NP-SBJ-I Chris)  (VP to  (VP throw  (NP the ball) ) ) ) ) )  Predicate Argument Structure: =ants (Chris, throw (Chris, ball))  We also use null elements to allow the interpretation of other grammatical structures where constituents do not appear in their default positions. Null elements are used in most cases to mark the fronting (or ""topicalization"" of any element of an S before the subject (except in inversion). If an adjunct is topicalized, the fronted element does not leave a trace since the level of attachment is the same, only the word order is different. Topicalized arguments, on the other hand, always are marked by a null element:  (S (NP-TPC-5 This)  (NP-SBJ every man)  (PP-LOC within  Again, this makes predicate argument interpretation straightforward, if the null element is simply replaced by the constituent to which it is co-indexed.  Similarly, if the predicate has moved out of VP, it leaves a null element *T* in the VP node.  (SINV (VP-TPC-I Marching  (NP the reviewing stand)))  (VP *T*-I) )  *PPA* Permanent Predictable Ambiguity  *RNR* Right Node Raising  *EXP* EXPletive  Figure 2: The four forms of pseudo-attachment  Here, the SINVnode marks an inverted S structure, and the -TPC tag (ToPiC) marks a fronted (topicalized) constituent; the -GLR tag is discussed below.  Many otherwise clear argument/adjunct relations in the current corpus cannot be recovered due to the essentially context-free representation of the current Treebank. For ample, currently there is no good representation for sentences in which constituents which serve as complements to the verb occur after a sententiM level adverb. Either the adverb is trapped within the VP, so that the complement can occur within the VP, where it belongs, or else the adverb is tached to the S, closing off the VP and forcing the ment to attach to the S. This ""trapping"" problem serves as a limitation for groups that currently use Treebank material to semiautomatically derive lexicons for particular applications.  To solve ""trapping"" problems and annotation of non-contiguous structure, a wide range of phenomena of the kind discussed above can be handled by simple notational devices that use co-indexing to indicate discontinuous structures. Again, an index number added to the label of the original constituent is incorporated into the null element which shows where that constituent should be interpreted within the icate argument structure.  We use a variety of null elements to show show how non-adjacent constituents are related; we refer to such con-stituents as ""pseudoattached'. There axe four different types of pseudo-attach, as shown in Figure 1; the use of each will be explained below:  The *IGH* pseudo-attach is used for simple extraposition, solving the most common case of ""trapping"":  (S (NP-SBJ Chris)  1 that  (S (NP-SBJ Terry)  (VP would  (VP catch  (NP the ball)))))))  Here, the clause that Terry would catch the ball is to be terpreted as an argument of knew.  The *PPA* tag is reserved for so-called ""permanent dictable ambiguity"", those cases in which one cannot tell where a constituent should be attached, even given context. Here, annotators attach the constituent at the more likely site (or if that is impossible to determine, at the higher site) and pseudo-attach it at all other plausible sites using using the *PPA * null element. Within the annotator workstation, this is done with a single mouse click, using pseudo-move and pseudo-promote operations.  (NP (NP the man)  (PP-CLR-1 vith  (NPthe telescope))))  The *RNR*tag is used for so-called ""right-node raising"" junctions, where the same constituent appears to have been shifted out of both conjuncts.  (IfP-SBJ-2 our outlook)  and  (VP to  (vP be (ADJP *PJ~R*-I) ))))  So that certain kinds of constructions can be found reliably within the corpus, we have adopted special marking of some special constructions. For example, extraposed sentences which leave behind a semantically null ""it"" are parsed as follows, using the *EXP* tag:  (S (NP-SBJ (NP It) (S *EXP*-i)) (VP is (liP a pleasure)) (S-I (NP-SBJ *) (VP to (VP teach (NP her)))))  Predicate Argument Structure: pleasure(teach(*someone*, her))  Note that ""It"" is recognized as the surface subject, and that the extraposed clause is attached at S level and adjoined to ""it"" with what we call *EXPa-attach. The *EXP* is matically co-indexed by our annotator workstation software to the postposed clause. The extraposed clause is interpreted as the subject of a pleasure here; the word it is to be ignored during predicate argument interpretation; this is flagged by the use of a special tag.  In general, we use a Chomsky adjunetion structure to show coordination, and we coordinate structures as low as possible. We leave word level conjunction implicit; two single word NP's or VP's will have only the higher level of structure. If at least one of the conjoined elements consists of more than one word, the coordination is made explicit. The example that follows shows two conjoined relative clauses; note that relative clauses are normally adjoined to the antecedent NP.  (S (NP-SBJ Terry)  (VP knew  (NP (NP the person)  (SBAR (SBAR (hg4NP-I who)  (VP threw  (NP the ball))))  and  (SBAR (NHSP-2 who) (S (NP-SBJ T-2)  (VP caught  (NP it)))))))  (hew Terry (person (and (threw *who* ball)  (caught *who* it))))  Conditional, temporal, and other such subordinate clauses, like other adjuncts, are normally attached at S-level.  The phenomenon of gapping provides a major challenge to our attempt to provide annotation which is sufficient to allow the recovery of predicate argument for whatever structure is complete within a sentence. We have developed a simple notational mechanism, based on structural templates, which allows the predicate argument structure of gapped clauses to be recovered in most cases when the full parallel ture is within the same clause. In essence, we use the plete clause as a template and provide a notation to allow arguments to be mapped from the gapped clause onto that template. In the template notation, we use an equal sign to indicate that constituent NP=I should be mapped over NP-1 in the largest conjoined structure that NP-1 and NP=I both occur in. A variety of simple notational devices, which we will not discuss here, extend this notation to handle constituents that occur in one branch of the conjunct, but not the other.  (S (S (NP-SBJ-1 Mary)  and  like (Mary, Bach) and like (Susan,Beethoven)  (NP-I Mary)  and  (s (s (NP-SBJ Z)  (VP eat  (NP-I breakfast)  (PP-TMP-2 in  (NP the morning))))  and  (PP-TMPffi2 in  (NP the afternoon))))  We do not attempt to recover structure which is outside a single sentence. We use the tag FRAG for those pieces of text which appear to be clauses, but lack too many essen-tial elements for the exact structure to be easily determined. Obviously, predicate argument structure cannot be extracted from FRAG's.  Who threw the ball? Chris, yesterday.  (FRAG (NP Chris)  s  What is Tim eating? Mary Ann thinks chocolate.  (S (MP-SB3 Mary Ann)  (VP thinks  (FRAG (NP chocolate)))))  We are now beginning annotation using this new scheme. We believe that this revised form of annotation will provide a pus of annotated material that is useful for training stochastic parsers on surface syntax, for training stochastic parsers that work at one level of analysis beyond surface syntax, and at the same time provide a consistent database for use in guistic research.  1. Black, E., Abney, S., Flickenger, F., Grishman, R., rison, P., Hindle, D., Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B., and Strzalkowski, T., 1991. A procedure for titatively comparing the syntactic coverage of English grammars. In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, February 1991.  Black, E., Jelinek, F., Lafferty, J., Magerman, D.M., Mercer, R., and Roukos, S. 1992. Towards history-based grammars: Using Richer Models for Probabilistic pars-ing. In Proceedings of the $1th Annual Meeting of the Association for Computational Linguistics.  Brill, E., Marcus, M., 1992. Automatically acquiring phrase structure using distributional analysis. In Pro-ceedings of the DARPA Speech and Natural Language Workshop, February 199~.  Brill, E., 1993. Automatic grammar induction and ing free text: a transformation-based approach. In Pro-ceedings of the 31th Annual Meeting of the Association for Computational Linguistics.  5. Francis, W., 1964. A standard sample of present-day English for use with digital computers. Report to the U.S Office of Education on Cooperative Research Project No. E-OOZ Brown University, Providence.  Francis, W. and Ku~era, H., 1982. Frequency analysis of English usage. Lexicon and grammar. Houghton Mifflin, Boston.  Garside, R., Leech, G., and Sampson, G., 1987. The computational analysis of English. A corpus-based proach. Longman, London.  Hindle, D., and Rooth, M., 1993. Structural Ambiguity and Lexical Relations. Computational Linguistics, Vol  9. D. Magerman and M. Marcus, 1991. PEARL --A abilistic Chart Parser, In Proceedings, Fifth Conference of the European Chapter off the Association for tational Linguistics (EACL), Berlin, April 1991.  Marcus, M., Santorini, B., Marcinkiewicz, M.A., 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, Vol 19.  Quirk, R., Greenbanm, S., Leech, G., and Svaxtvik, J., 1985. A comprehensive grammar of the English language, Longman, London. ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","ARGUMENT STRUCTURE This paper discusses the implementation of crucial aspects of this new annotation scheme.It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of dexed null elements in what can be thought of as ing"" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the ture of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this terial also annotated with a skeletal syntactic bracketing.All of this material has been hand corrected after processing by automatic tools.The largest component of the corpus sists of materials from the Dow-Jones News Service; over 1.6 million words of this material has been hand parsed, with an additional 1 million words tagged for part of speech.Also included is a skeletally parsed version of the Brown corpus, the classic million word balanced corpus of American English [5, 6].hand-retagged using the Penn Treebank tagset.The level of syntactic analysis annotated during this phase of this project was an extended and somewhat modified form of the skeletal analysis which has been produced by the banking effort in Lancaster, England [7].The released rials in the current Penn Treebank, although still in very liminary form, have been widely distributed, both directly by us, on the ACL/DCI CD-ROM, and now on CD-ROM by the Linguistic Data Consortium; it has been used for purposes ranging from serving as a gold-standard for parser testing to serving as a basis for the induction of stochastic grammars to serving as a basis for quick lexicon induction.Many users of the Penn Treebank now want forms of notation richer than provided by the project's first phase, as well as an increase in the consistency of the preliminary corpus.Some would also like a less skeletal form of tation, expanding the essentially context-free analysis of the current treebank to indicate non-contiguous structures and dependencies.Most crucially, there is a strong sense that the Treebank could be of much more use if it explicitly provided some form of predicate-argument structure.The desired level of representation would make explicit at least the logical ject and logical object of the verb, and indicate, at least in clear cases, how subconstituents are semantically related to their predicates.Such a representation could serve as both a starting point for the kinds of SEMEVAL representations now being discussed as a basis for evaluation of human guage technology within the ARPA HLT program, and as a basis for ""glass box"" evaluation of parsing technology.The ongoing effort [1] to develop a standard objective methodology to compare parser outputs across widely gent grammatical frameworks has now resulted in a widely supported standard for parser comparison.On the other hand, many existing parsers cannot be evaluated by this metric because they directly produce a level of tion closer to predicate-argument structure than to classical surface grammatical analysis.Hand-in-hand with this tation of the existing Penn Treebank for parser testing is a parallel limitation for automatic methods for parser training for parsers based on deeper representations.There is also a problem of maintaining consistency with the fairly small (less than 100 page) style book used in the the first phase of the project.We have recently completed a detailed style-book for this new level of analysis, with consensus across annotators about the particulars of the analysis.This project has taken about eight months of ten-hour a week effort across a significant subset of all the personnel of the Penn Treebank.Such a stylebook, much larger, and much more fully specified than our initial stylebook, is a prerequisite for high levels of annotator agreement.It is our hope that such a stylebook will also alleviate much of the need for extensive cross-talk between annotators during the annotation task, thereby creasing throughput as well.To ensure that the rules of this new stylebook remain in force, we are now giving annotators about 10% overlapped material to evaluate inter-annotator consistency throughout this new project.We have now begun to annotate this level of structure editing the present Penn Treebank; we intend to automatically tract a bank of predicate-argument structures intended at the very least for parser evaluation from the resulting annotated corpus.The remainder of this paper will discuss the implementation of each of four crucial aspects of the new annotation scheme, as well as notational devices to allow predicate-argument structure to be recovered in the face of conjoined ture involving gapping, where redundant syntactic structure within a conjoined structure is deleted.In particular, the new scheme:  Incorporates a consistent treatment of related ical phenomena.The issue here is not that the sentation be ""correct"" given some theoretical analysis or other, but merely that instances of what are tively the same phenomenon be represented similarly.In particular, the notation should make it easy to matically recover predicate-argument structure.Provides a set of null elements in what can be thought of as ""underlying"" position for phenomena such as movement, passive, and the subjects of infinitival structions.These null elements must be co-indexed with the appropriate lexical material.Provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered.Allows for a clear, concise distinction between verb arguments and adjuncts where such distinctions are clear, with some easy-to-use notational device to cate where such a distinction is somewhat murky.Our first step, just now complete, has been to produce a tailed style-book for this new level of analysis, with consensus across annotators about the particulars of the analysis.This project has taken about eight months of ten-hour a week effort across a significant subset of all the personnel of the Penn Treebank.It has become clear during the first stage of the project that a much larger, much more fully fied stylebook than our initial stylebook is a prerequisite for high levels of inter-annotator agreement.It is our hope that such a stylebook will also alleviate much of the need for tensive cross-talk between annotators during the annotation task, thereby increasing throughput as well.To ensure that the rules of this new stylebook remain in force, we intend to give annotators about 10% overlapped material to evaluate inter-annotator consistency throughout this new project.The remainder of this paper discusses the implementation of each of the four points above, as well as notational vices to allow predicate-argument structure to be recovered in the face of conjoined structure involving gapping, where redundant syntactic structure within a conjoined structure is deleted.3. CONSISTENT GRAMMATICAL ANALYSES  The current treebank materials suffer from the fact that fering annotation regimes are used across differing syntactic categories.To allow easy automatic extraction of argument structure in particular, these differing analyses must be unified.In the original annotation scheme, adjective phrases that serve as sentential predicates have a different structure than VPs, causing sentential adverbs which occur after auxiliaries introducing the ADJP to attach under VP, while sentential adverbs occurring after auxiliaries ing VPs occur under S.In the current treebank, copular be is treated as a main verb, with predicate adjective or sitional phrases treated as complements to that verb.In the new stylebook, the predicate is either the lowest most branching) VP or the phrasal structure immediately under copular BE.In cases when the predicate cannot be identified by those criteria (e.g. in ""small clauses"" and some inversion structures), the predicate phrase is tagged -PRD (PReDicate).(s (NP-SBJ I)  (NP-PRD a fool))))  (SQ Was  (NP-SBJ he)  Note that the surface subject is always tagged -SBJ (SuB-Ject), even though this is usually redundant because the subject can be recognized purely structurally.The -TMP tag here marks time (TeMPoral) phrases.Our use of ""small clauses"" follows one simple rule:, every S maps into a single predication, so here the predicate-argument structure would be something like  consider(I, fool(Kris)).4. ARGUMENT-ADJUNCT STRUCTURE  In a well developed predicate-argument scheme, it would seem desirable to label each argument of a predicate with an appropriate semantic label to identify its role with respect to that predicate.It would also seem desirable to distinguish between the arguments of a predicate, and adjuncts of the predication.Unfortunately, while it is easy to distinguish arguments and adjuncts in simple cases, it turns out to be very difficult to consistently distinguish these two categories for many verbs in actual contexts.It also turns out to be very difficult to determine a set of underlying semantic roles that holds up in the face of a few paragraphs of text.In our new annotation scheme, we have tried to come up with a middle ground which allows annotation of those tions that seem to hold up across a wide body of material.After many attempts to find a reliable test to distinguish between arguments and adjuncts, we have abandoned turally marking this difference.Instead, we now label a small set of clearly distinguishable roles, building upon syntactic distinctions only when the semantic intuitions are clear cut.Getting annotators to consistently apply even the small set of distinctions we will discuss here is fairly difficult.In the earlier corpus annotation scheme, We originally used only standard syntactic labels (e.g. NP, ADVP, PP, etc.)  for our constituents in other words, every bracket had just one label.The limitations of this became apparent when a word belonging to one syntactic category is used for another hnction or when it plays a role which we want to be able to identify easily.In the present scheme, each constituent has at least one label but as many as four tags, including numerical indices.We have adopted the set of functional tags shown in Figure 2 for use within the current annotation scheme.NPs and Ss which are clearly arguments of the verb are unmarked by any tag.We allow an open class of other cases that dividual annotators feel strongly should be part of the VP.These cases are tagged as -CLR (for CLosely Relatcd); they axe to be semantically analyzed as adjuncts.This class is an experiment in the current tagging; constituents marked -CLR typically correspond to Quirk et al's [11] class of ication adjuncts.At the moment, we distinguish a handful of semantic roles: direction, location, manner, purpose, and time, as well as the syntactic roles of surface subject, cal subject, and (implicit in the syntactic structure) first and second verbal objects.One important way in which the level of annotation of the  current Penn Treebank exceeds that of the Lancaster project  is that we have annotated nun elements in a wide range of  cases.In the new annotation scheme, we co-index these null  elements with the lexical material for which the null element  stands.The current scheme happens to use two symbols for  null elements: *T*,which marks WH-movement and  topicalization, and * which is used for all other null elements, but  this distinction is not very important.Co-indexing of null  elements is done by suffixing an integer to non-terminal  categories (e.g~ NP-10, VP-25).This integer serves as an id  number for the constituent.A null element itself is followed by the  id number of the constituent with which it is co-indexed.We  use SBARQ to mark WH-questions, and SQ to mark  auxiliaxy inverted structures.We use the WH-prefixed labels,  WHNP, WHADVP, WHPP, etc., only when there is  WHmovement; they always leave a co-indexed trace.Crucially,  the predicate argument structure can be recovered by simply  replacing the null element with the lexical material that it is  co-indexed with:  (SBARQ (NHNP-1What)  (VP eating  eat(Tim, what)  In passives, the surface subject is tagged -SBJ,a passive trace  is inserted after the verb, indicated by (NP *), and co-indexed  to the surface subject (i.e. logical object).The logical  subject by-phrase, if present, is a child of VP, and is tagged  -LGS (LoGical Subject).For passives, the predicate  argument structure can be recovered by replacing the passive null  element with the material it is co-indexed with, and treating  the NP marked -LGS as the subject.(s (NP-SBJ-I The ball)  (VP was  (VP thrown  (PP by  (NP-LGS Chris)))))  throw(Chris, ball)  The interpretation rules for passives and WH-phrases act correctly to yield the predicate argument structures for complex nestings of WH-questions and passives.(SBARQ (WHNP-1 Who)  (NP-SBJ-2 *T*-I)  -TPC  -CLR  Text Categories headlines and datelines list markers titles  Grammatical Functions true clefts non NPs that function as NPs clausal and NP adverbials logical subjects in passives non VP predicates surface subject topicalized and fronted constituents closely related -see text  Semantic Roles vocatives direction & trajectory location manner purpose and reason temporal phrases  Figure I: Functional Tags  (VP to  believe (*someone*, shoot (*someone*, Who))  A null element is also used to indicate which lexical NP is to  be interpreted as the null null subject of an infinitive  complement clause; it is co-indexed with the controlling NP, based  upon the lexical properties of the verb.(S (NP-SBJ-I Chris)  (VP to  (VP throw  (NP the ball) ) ) ) ) )  Predicate Argument Structure: =ants (Chris, throw (Chris, ball))  We also use null elements to allow the interpretation of other grammatical structures where constituents do not appear in their default positions.Null elements are used in most cases to mark the fronting (or ""topicalization"" of any element of an S before the subject (except in inversion).If an adjunct is topicalized, the fronted element does not leave a trace since the level of attachment is the same, only the word order is different.Topicalized arguments, on the other hand, always are marked by a null element:  (S (NP-TPC-5 This)  (NP-SBJ every man)  (PP-LOC within  Again, this makes predicate argument interpretation straightforward, if the null element is simply replaced by the constituent to which it is co-indexed.Similarly, if the predicate has moved out of VP, it leaves a null element *T* in the VP node.(SINV (VP-TPC-I Marching  (NP the reviewing stand)))  (VP *T*-I) )  *PPA* Permanent Predictable Ambiguity  *RNR* Right Node Raising  *EXP* EXPletive  Figure 2: The four forms of pseudo-attachment  Here, the SINVnode marks an inverted S structure, and the -TPC tag (ToPiC) marks a fronted (topicalized) constituent; the -GLR tag is discussed below.Many otherwise clear argument/adjunct relations in the current corpus cannot be recovered due to the essentially context-free representation of the current Treebank.For ample, currently there is no good representation for sentences in which constituents which serve as complements to the verb occur after a sententiM level adverb.Either the adverb is trapped within the VP, so that the complement can occur within the VP, where it belongs, or else the adverb is tached to the S, closing off the VP and forcing the ment to attach to the S.This ""trapping"" problem serves as a limitation for groups that currently use Treebank material to semiautomatically derive lexicons for particular applications.To solve ""trapping"" problems and annotation of non-contiguous structure, a wide range of phenomena of the kind discussed above can be handled by simple notational devices that use co-indexing to indicate discontinuous structures.Again, an index number added to the label of the original constituent is incorporated into the null element which shows where that constituent should be interpreted within the icate argument structure.We use a variety of null elements to show show how non-adjacent constituents are related; we refer to such con-stituents as ""pseudoattached'.There axe four different types of pseudo-attach, as shown in Figure 1; the use of each will be explained below:  The *IGH* pseudo-attach is used for simple extraposition, solving the most common case of ""trapping"":  (S (NP-SBJ Chris)  1 that  (S (NP-SBJ Terry)  (VP would  (VP catch  (NP the ball)))))))  Here, the clause that Terry would catch the ball is to be terpreted as an argument of knew.The *PPA* tag is reserved for so-called ""permanent dictable ambiguity"", those cases in which one cannot tell where a constituent should be attached, even given context.Here, annotators attach the constituent at the more likely site (or if that is impossible to determine, at the higher site) and pseudo-attach it at all other plausible sites using using the *PPA * null element.Within the annotator workstation, this is done with a single mouse click, using pseudo-move and pseudo-promote operations.(NP (NP the man)  (PP-CLR-1 vith  (NPthe telescope))))  The *RNR*tag is used for so-called ""right-node raising"" junctions, where the same constituent appears to have been shifted out of both conjuncts.(IfP-SBJ-2 our outlook)  and  (VP to  (vP be (ADJP *PJ~R*-I) ))))  So that certain kinds of constructions can be found reliably within the corpus, we have adopted special marking of some special constructions.For example, extraposed sentences which leave behind a semantically null ""it"" are parsed as follows, using the *EXP* tag:  (S (NP-SBJ (NP It) (S *EXP*-i)) (VP is (liP a pleasure)) (S-I (NP-SBJ *) (VP to (VP teach (NP her)))))  Predicate Argument Structure: pleasure(teach(*someone*, her))  Note that ""It"" is recognized as the surface subject, and that the extraposed clause is attached at S level and adjoined to ""it"" with what we call *EXPa-attach.The *EXP* is matically co-indexed by our annotator workstation software to the postposed clause.The extraposed clause is interpreted as the subject of a pleasure here; the word it is to be ignored during predicate argument interpretation; this is flagged by the use of a special tag.In general, we use a Chomsky adjunetion structure to show coordination, and we coordinate structures as low as possible.We leave word level conjunction implicit; two single word NP's or VP's will have only the higher level of structure.If at least one of the conjoined elements consists of more than one word, the coordination is made explicit.The example that follows shows two conjoined relative clauses; note that relative clauses are normally adjoined to the antecedent NP.(S (NP-SBJ Terry)  (VP knew  (NP (NP the person)  (SBAR (SBAR (hg4NP-I who)  (VP threw  (NP the ball))))  and  (SBAR (NHSP-2 who) (S (NP-SBJ T-2)  (VP caught  (NP it)))))))  (hew Terry (person (and (threw *who* ball)  (caught *who* it))))  Conditional, temporal, and other such subordinate clauses, like other adjuncts, are normally attached at S-level.The phenomenon of gapping provides a major challenge to our attempt to provide annotation which is sufficient to allow the recovery of predicate argument for whatever structure is complete within a sentence.We have developed a simple notational mechanism, based on structural templates, which allows the predicate argument structure of gapped clauses to be recovered in most cases when the full parallel ture is within the same clause.In essence, we use the plete clause as a template and provide a notation to allow arguments to be mapped from the gapped clause onto that template.In the template notation, we use an equal sign to indicate that constituent NP=I should be mapped over NP-1 in the largest conjoined structure that NP-1 and NP=I both occur in.A variety of simple notational devices, which we will not discuss here, extend this notation to handle constituents that occur in one branch of the conjunct, but not the other.(S (S (NP-SBJ-1 Mary)  and  like (Mary, Bach) and like (Susan,Beethoven)  (NP-I Mary)  and  (s (s (NP-SBJ Z)  (VP eat  (NP-I breakfast)  (PP-TMP-2 in  (NP the morning))))  and  (PP-TMPffi2 in  (NP the afternoon))))  We do not attempt to recover structure which is outside a single sentence.We use the tag FRAG for those pieces of text which appear to be clauses, but lack too many essen-tial elements for the exact structure to be easily determined.Obviously, predicate argument structure cannot be extracted from FRAG's.Who threw the ball?Chris, yesterday.(FRAG (NP Chris)  s  What is Tim eating?Mary Ann thinks chocolate.(S (MP-SB3 Mary Ann)  (VP thinks  (FRAG (NP chocolate)))))  We are now beginning annotation using this new scheme.We believe that this revised form of annotation will provide a pus of annotated material that is useful for training stochastic parsers on surface syntax, for training stochastic parsers that work at one level of analysis beyond surface syntax, and at the same time provide a consistent database for use in guistic research.1. Black, E., Abney, S., Flickenger, F., Grishman, R., rison, P., Hindle, D., Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B., and Strzalkowski, T., 1991.A procedure for titatively comparing the syntactic coverage of English grammars.In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, February 1991.Black, E., Jelinek, F., Lafferty, J., Magerman, D.M., Mercer, R., and Roukos, S. 1992. Towards history-based grammars: Using Richer Models for Probabilistic pars-ing.In Proceedings of the $1th Annual Meeting of the Association for Computational Linguistics.Brill, E., Marcus, M., 1992.Automatically acquiring phrase structure using distributional analysis.In Pro-ceedings of the DARPA Speech and Natural Language Workshop, February 199~.Brill, E., 1993.Automatic grammar induction and ing free text: a transformation-based approach.In Pro-ceedings of the 31th Annual Meeting of the Association for Computational Linguistics.5. Francis, W., 1964.A standard sample of present-day English for use with digital computers.Report to the U.S Office of Education on Cooperative Research Project No. E-OOZ Brown University, Providence.Francis, W. and Ku~era, H., 1982.Frequency analysis of English usage.Lexicon and grammar.Houghton Mifflin, Boston.Garside, R., Leech, G., and Sampson, G., 1987.The computational analysis of English.A corpus-based proach.Longman, London.Hindle, D., and Rooth, M., 1993.Structural Ambiguity and Lexical Relations.Computational Linguistics, Vol  9.D. Magerman and M. Marcus, 1991.PEARL --A abilistic Chart Parser, In Proceedings, Fifth Conference of the European Chapter off the Association for tational Linguistics (EACL), Berlin, April 1991.Marcus, M., Santorini, B., Marcinkiewicz, M.A., 1993.Building a large annotated corpus of English: the Penn Treebank.Computational Linguistics, Vol 19.Quirk, R., Greenbanm, S., Leech, G., and Svaxtvik, J., 1985.A comprehensive grammar of the English language, Longman, London."
" Automatic Labeling of Semantic Roles  Daniel Gildea* Daniel Jurafsky† University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.  The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible .llers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents. At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. "," Using Semantic Preferences to Identify Verbal Participation in  Role Switching Alternations.  Cognitive & Computing Sciences,  University of Sussex  Brighton BN1 9QH, UK  We propose a method for identifying diathesis nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional erences acquired as probability distributions over WordNet. Preferences for the target slots are pared using a measure of distributional similarity. The method is evaluated on the causative and tive alternations, but is generally applicable and does not require a priori knowledge specific to the alternation.  ",1,"Automatic Labeling of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, .lled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-speci.c semantic roles such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classi.ers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible .llers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classi.ers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con­stituents.At the more dif.cult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1.","Using Semantic Preferences to Identify Verbal Participation in Role Switching Alternations. Cognitive & Computing Sciences,  University of Sussex  Brighton BN1 9QH, UK  We propose a method for identifying diathesis nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms.The method uses selectional erences acquired as probability distributions over WordNet.Preferences for the target slots are pared using a measure of distributional similarity.The method is evaluated on the causative and tive alternations, but is generally applicable and does not require a priori knowledge specific to the alternation."
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440?447,  Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics  Biographies, Bollywood, Boom-boxes and Blenders:  Domain Adaptation for Sentiment Classification  Department of Computer and Information Science  University of Pennsylvania  Automatic sentiment classification has been  extensively studied and applied in recent  years. However, sentiment is expressed  differently in different domains, and annotating  corpora for every possible domain of interest  is impractical. We investigate domain  adaptation for sentiment classifiers, focusing on  online reviews for different types of  products. First, we extend to sentiment  classification the recently-proposed structural  correspondence learning (SCL) algorithm,  reducing the relative error due to adaptation  between domains by an average of 30% over  the original SCL algorithm and 46% over  a supervised baseline. Second, we identify  a measure of domain similarity that  correlates well with the potential for adaptation  of a classifier from one domain to another.  This measure could for instance be used to  select a small set of domains to annotate  whose trained classifiers would transfer well  to many other domains.  1 ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","c?2007 Association for Computational Linguistics  Biographies, Bollywood, Boom-boxes and Blenders:  Domain Adaptation for Sentiment Classification  Department of Computer and Information Science  University of Pennsylvania  Automatic sentiment classification has been  extensively studied and applied in recent  years.However, sentiment is expressed  differently in different domains, and annotating  corpora for every possible domain of interest  is impractical.We investigate domain  adaptation for sentiment classifiers, focusing on  online reviews for different types of  products.First, we extend to sentiment  classification the recently-proposed structural  correspondence learning (SCL) algorithm,  reducing the relative error due to adaptation  between domains by an average of 30% over  the original SCL algorithm and 46% over  a supervised baseline.Second, we identify  a measure of domain similarity that  correlates well with the potential for adaptation  of a classifier from one domain to another.This measure could for instance be used to  select a small set of domains to annotate  whose trained classifiers would transfer well  to many other domains.1"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis  Yejin Choi and Claire Cardie  Department of Computer Science  Cornell University  Ithaca, NY 14853  1: [I did [not] have any [doubt] about it.]+  2: [The report [eliminated] my [doubt] .]+  Determining the polarity of a  sentiment3: [They could [not] [eliminate] my [doubt] .]  bearing expression requires more than a  simple bag-of-words approach.  words or constituents within the expression  In the first example, doubt in isolation carries  can interact with each other to yield a  particua negative sentiment, but the overall polarity of the lar overall polarity. In this paper, we view such  sentence is positive because there is a negator not , subsentential interactions in light of composi-which flips the polarity. In the second example, both tional semantics, and present a novel  learning eliminated and doubt carry negative sentiment  based approach that incorporates structural  inin isolation, but the overall polarity of the sentence ference motivated by compositional seman-is positive because eliminated acts as a negator for tics into the learning procedure. Our exper-its argument doubt . In the last example, there are iments show that (1) simple heuristics based  on compositional semantics can perform  beteffectively two negators not and eliminated  ter than learning-based methods that do not  inwhich reverse the polarity of doubt twice,  resultcorporate compositional semantics (accuracy  ing in the negative polarity for the overall sentence.  of 89.7% vs. 89.1%), but (2) a method that  These examples demonstrate that words or  conintegrates compositional semantics into  learnstituents interact with each other to yield the  ing performs better than all other  alternaexpression-level polarity. And a system that  simWe also find that  contentply takes the majority vote of the polarity of  indiword negators , not widely employed in  previous work, play an important role in  devidual words will not work well on the above  examtermining expression-level polarity. Finally,  ples. Indeed, much of the previous learning-based  in contrast to conventional wisdom, we find  research on this topic tries to incorporate salient in-that expression-level classification accuracy  teractions by encoding them as features. One  apuniformly decreases as additional, potentially  proach includes features based on contextual  vadisambiguating, context is considered.  lence shifters1 (Polanyi and Zaenen, 2004), which  are words that affect the polarity or intensity of sen-1  ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis 1: [I did [not] have any [doubt] about it.]+ 2: [The report [eliminated] my [doubt] .]+ Determining the polarity of a sentiment3: [They could [not] [eliminate] my [doubt] .] bearing expression requires more than a simple bag-of-words approach. words or constituents within the expression  In the first example, doubt in isolation carries  can interact with each other to yield a  particua negative sentiment, but the overall polarity of the lar overall polarity.In this paper, we view such  sentence is positive because there is a negator not , subsentential interactions in light of composi-which flips the polarity.In the second example, both tional semantics, and present a novel  learning eliminated and doubt carry negative sentiment  based approach that incorporates structural  inin isolation, but the overall polarity of the sentence ference motivated by compositional seman-is positive because eliminated acts as a negator for tics into the learning procedure.Our exper-its argument doubt . In the last example, there are iments show that (1) simple heuristics based  on compositional semantics can perform  beteffectively two negators not and eliminated  ter than learning-based methods that do not  inwhich reverse the polarity of doubt twice,  resultcorporate compositional semantics (accuracy  ing in the negative polarity for the overall sentence.  of 89.7% vs. 89.1%), but (2) a method that  These examples demonstrate that words or  conintegrates compositional semantics into  learnstituents interact with each other to yield the  ing performs better than all other  alternaexpression-level polarity.And a system that  simWe also find that  contentply takes the majority vote of the polarity of  indiword negators , not widely employed in  previous work, play an important role in  devidual words will not work well on the above  examtermining expression-level polarity.Finally,  ples.Indeed, much of the previous learning-based  in contrast to conventional wisdom, we find  research on this topic tries to incorporate salient in-that expression-level classification accuracy  teractions by encoding them as features.One  apuniformly decreases as additional, potentially  proach includes features based on contextual  vadisambiguating, context is considered.lence shifters1 (Polanyi and Zaenen, 2004), which  are words that affect the polarity or intensity of sen-1"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Hierarchical Sequential Learning for Extracting Opinions and their Attributes  Yejin Choi and Claire Cardie  Department of Computer Science  Cornell University  systems with cascaded component architectures,  causing performance degradation in the  end-toAutomatic opinion recognition involves a  number of related tasks, such as  identicase, in the end-to-end opinion recognition  sysfying the boundaries of opinion  expression, determining their polarity, and  deIn this paper, we apply a hierarchical  paramtermining their intensity. Although much  eter sharing technique (e.g., Cai and Hofmann  progress has been made in this area,  ex(2004), Zhao et al. (2008)) using Conditional  Ranisting research typically treats each of the  dom Fields (CRFs) (Lafferty et al., 2001) to  fineabove tasks in isolation.  In this paper,  grained opinion analysis. In particular, we aim to  we apply a hierarchical parameter  sharjointly identify the boundaries of opinion  expresing technique using Conditional Random  sions as well as to determine two of their key  atFields for fine-grained opinion analysis,  tributes polarity and intensity.  jointly detecting the boundaries of opinion  Experimental results show that our proposed  apexpressions as well as determining two of  proach improves the performance over the  basetheir key attributes polarity and  intenline that does not exploit the hierarchical structure sity. Our experimental results show that  among the classes. In addition, we find that the  our proposed approach improves the  perjoint approach outperforms a baseline that is based  formance over a baseline that does not  on cascading two separate systems.  exploit hierarchical structure among the  classes. In addition, we find that the joint  approach outperforms a baseline that is  Hierarchical Sequential Learning  based on cascading two separate  compoWe define the problem of joint extraction of  opinion expressions and their attributes as a sequence  ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","Hierarchical Sequential Learning for Extracting Opinions and their Attributes systems with cascaded component architectures, causing performance degradation in the end-toAutomatic opinion recognition involves a number of related tasks, such as identicase, in the end-to-end opinion recognition sysfying the boundaries of opinion expression, determining their polarity, and paramtermining their intensity. Although much  eter sharing technique (e.g., Cai and Hofmann  progress has been made in this area,  ex(2004), Zhao et al.(2008)) using Conditional  Ranisting research typically treats each of the  dom Fields (CRFs) (Lafferty et al., 2001) to  fineabove tasks in isolation.In this paper,  grained opinion analysis.In particular, we aim to  we apply a hierarchical parameter  sharjointly identify the boundaries of opinion  expresing technique using Conditional Random  sions as well as to determine two of their key  atFields for fine-grained opinion analysis,  tributes polarity and intensity.jointly detecting the boundaries of opinion  Experimental results show that our proposed  apexpressions as well as determining two of  proach improves the performance over the  basetheir key attributes polarity and  intenline that does not exploit the hierarchical structure sity.Our experimental results show that  among the classes.In addition, we find that the  our proposed approach improves the  perjoint approach outperforms a baseline that is based  formance over a baseline that does not  on cascading two separate systems.exploit hierarchical structure among the  classes.In addition, we find that the joint  approach outperforms a baseline that is  Hierarchical Sequential Learning  based on cascading two separate  compoWe define the problem of joint extraction of  opinion expressions and their attributes as a sequence"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Proceedings of the Conference on Empirical Methods in Natural  Language Processing (EMNLP), Philadelphia, July 2002, pp. 1-8.  Association for Computational Linguistics.  riminativeTrainingMethodsforHiddenMarkovModels: TheoryandExperimentswithPer  eptronAlgorithms  haelCollins  AT&TLabs-Resear  h,FlorhamPark,NewJersey.  ollinsresear  ribenewalgorithmsfortrain­ingtaggingmodels,asanalternative tomaximum­entropymodelsor  ondi­tionalrandomfelds(CRFs).Theal­gorithmsrelyonViterbide  odingof trainingexamples,  ombinedwithsim­pleadditiveupdates.Wedes  ribethe­oryjustifyingthealgorithmsthrough amodif  ationoftheproofof  eoftheper  eptronalgorithmfor  ationproblems.Wegiveexper­imentalresultsonpart­of­spee  hunking,in both  asesshowingimprovementsover resultsforamaximum­entropytagger.  tion  efortaggingproblemsin NaturalLanguagePro  essing:forexamplesee (Ratnaparkhi96)fortheiruseonpart­of­spee  Callumetal.2000)fortheir useonaFAQsegmentationtask.MEmodels havetheadvantageofbeingquitefexibleinthe featuresthat  orporatedinthemodel. However,re  alandexperimentalre­sultsin(Lafertyetal.2001)havehighlighted problemswiththeparameterestimationmethod forMEmodels.Inresponsetotheseproblems, theydes  ribealternativeparameterestimation methodsbasedonConditionalMarkovRandom Fields(CRFs).(Lafertyetal.2001)giveexper­imentalresultssuggestingthatCRFs  antlybetterthanMEmodels.  Inthispaperwedes  ribeparameterestima­tionalgorithmswhi  harenaturalalternativesto CRFs.Thealgorithmsarebasedontheper  hapire99).Thesealgorithms havebeenshownby(Freund&S  hapire99)to be  ompetitivewithmodernlearningalgorithms su  hassupportve  hines;however,they havepreviouslybeenappliedmainlyto  ationtasks,anditisnotentirely  learhowthe algorithms  anbe  Thispaperdes  ribesvariantsoftheper  ep­tronalgorithmfortaggingproblems.Theal­gorithmsrelyonViterbide  odingoftraining examples,  ombinedwithsimpleadditiveup­dates.Wedes  ribetheoryjustifyingthealgo­rithmthroughamodif  ationoftheproofof  eoftheper  eptronalgorithmfor  ationproblems.Wegiveexperimentalresults onpart­of­spee  hunking,inboth  asesshowingimprovements overresultsforamaximum­entropytagger(a 11.9%relativeredu  tioninerrorforPOStag­ging,a5.1%relativeredu  hunking).Althoughwe  entrateontagging problemsinthispaper,thetheoreti  tion3of thispapershouldbeappli  abletoawideva­rietyofmodelswhereViterbi­stylealgorithms  Context­FreeGrammars,orMEmodels forparsing.See(CollinsandDufy2001;Collins andDufy2002;Collins2002)forotherappli  a­tionsofthevotedper  Inthisse  tion,asamotivatingexample,wede­s  aseofthealgorithminthis paper:thealgorithmappliedtoatrigramtag­ger.InatrigramHMMtagger,ea  tronalgorithm(Rosenblatt58),andthevoted  1Thetheoremsinse  tion3,andtheproofsinse  ­oraveragedversionsoftheper  ribed tion5,applydire  tlytotheworkintheseotherpapers.  htag/wordpairhaveasso  iated parameters.Wewritetheparameterasso  iated withatrigram(x,y,Z)asaxNyNz,andtheparam­eterasso  iatedwithatag/wordpair(t,w)as atNw.A  histotaketheparam­eterstobeestimatesof  onditionalprobabilities: axNyNz=logP(ZIx,y),atNw=logP(wIt).  ewewillusew[1:n]asshort­handforasequen  eofwords[w1,w2wn], andt[1:n]asshorthandforataqsequen  e [t1,t2tn].Inatrigramtaggerthes  ew[1:n]is2i=1ati 2Nti 1Nti+i=1atiNwi. Whentheparametersare  onditionalprobabil­itiesasabovethis""s  ore""isanestimateofthe logofthejointprobabilityP(w[1:n],t[1:n]).The Viterbialgorithm  anbeusedtofndthehighest s  ore.  Asanalternativetomaximum likelihoodpa­rameterestimates,thispaperwillproposethe followingestimationalgorithm.Saythetrain­ingset  es,thei'th senten  ebeingoflengthni.Wewillwritethese examplesas(wi,tfori=1n.Then  thetrainingalgorithmisasfollows:  •ChooseaparameterTdefningthenumber  ofiterationsoverthetrainingset.  InitiallysetallparametersaxNyNzandatNw tobezero.  Fort=1T,i=1n:UsetheViterbi algorithmtofndthebesttaggedsequen  allthistaggedsequen  2timesinZ[1:ni]where  2.Foreverytag/word pair(t,w)seen  1timesin(wi,tand  2timesin(wi,Zwhere  2. Asanexample,saythei'thtaggedsenten  the/Dman/Nsaw/Vthe/Ddog/N  andunderthe  urrentparametersettingsthe highests  2WetakeL 1andL 2tobespe  ialNULLtagsymbols. 3Tisusually  the/Dman/Nsaw/Nthe/Ddog/N  Thentheparameterupdatewilladd1tothe parametersaNNNv,aNNvN ,avN NN,avNsawand subtra  t1fromtheparametersaNNNN,aNNNN , aNN NN,aNNsaw.Intuitivelythishastheef­fe  tofin  reasingtheparametervaluesforfea­tureswhi  hwere""missing""fromtheproposed sequen  eZ[1:ni],anddownweightingparameter valuesfor""in  orre  t""featuresinthesequen  hanges aremadetotheparametervalues.  tors  ribehowtogeneralizethealgorithm tomoregeneralrepresentationsoftaggedse­quen  es.Inthisse  tionwedes  ribethefeature­ve  torrepresentationswhi  ommonlyused inmaximum­entropymodelsfortagging,and whi  harealsousedinthispaper.  isionsin taggingtheprobleminleft­to­rightfashion.At ea  hpointthereisa""history"" the  ontextin whi  isionismade andthetask istopredi  tthetaggiventhehistory.Formally, ahistoryisa4­tuple(tL1,tL2,w[1:n],i)where tL1,tL2aretheprevioustwotags,w[1:n]isanar­rayspe  e, andiistheindexofthewordbeingtagged.We use1todenotethesetofallpossiblehistories.  Maximum­entropymodelsrepresentthetag­gingtaskthroughafeature-ve  torrepresentation ofhistory­tagpairs.Afeatureve  tion¢thatmapsa history tagpairtoad­dimensionalfeatureve  ­tor.Ea  omponent¢s(h,t)fors=1d  tionof(h,t).Itis  ommon(e.g.,see(Ratnaparkhi96))forea  h feature¢stobeanindi  tion.Forex­ample,onesu  hfeaturemightbe  urrentwordwiisthe  0otherwise  Similarfeaturesmightbedefnedforevery word/tagpairseenintrainingdata.Another  featuretypemighttra  ktrigramsoftags,forex­ample¢1oo1(h,t)=1if(tL2,tL1,t)=(D,N,V) and0otherwise.Similarfeatureswouldbede­fnedforalltrigramsoftagsseenintraining.A realadvantageofthesemodels  omesfromthe freedomindefningthesefeatures:forexample, (Ratnaparkhi96;M  Callumetal.2000)both des  ribefeaturesetswhi  Inadditiontofeatureve  torrepresentations ofhistory/tagpairs,wewillfndit  onvenient todefnefeatureve  torsof(w[1:n],t[1:n])pairs wherew[1:n]isasequen  tionfrom(w[1:n],t[1:n])pairstod­dimensionalfeatureve  tors.Wewilloftenrefer to<asa""global""representation,in  al""representation.Theparti  onsideredinthispaper aresimplefun  tionsoflo  wherehi =(tiL1,tiL2,w[1:n],i).Ea  hglobal feature<s(w[1:n],t[1:n])issimplythevaluefor thelo  alrepresentation¢ssummedoverallhis­tory/tagpairsin(w[1:n],t[1:n]).Ifthelo  tions,thentheglobalfea­tureswilltypi  ounts"".Forexample, with¢1ooodefnedasabove,<1ooo(w[1:n],t[1:n]) isthenumberoftimestheisseentaggedasDT inthepairofsequen  Inmaximum­entropytaggersthefeatureve  tors ¢togetherwithaparameterve  onditionalprobabilitydistri­butionovertagsgivenahistoryas  as4s(hNt)  as4s(hNl)  whereZ(h,a  )=s.Thelogof  lETethisprobabilityhastheformlogp(tIh,a  s=1as¢s(h,t)-logZ(h,a  wherehi =(tiL1,tiL2,w,i).Givenparame­  ethe  w[1:n],highestprobabilitytaggedsequen  eunderthe formulainEq.2  anbefoundeÆ  ientlyusing theViterbialgorithm. Theparameterve  isestimatedfroma trainingsetofsenten  anbe estimatedusingGeneralizedIterativeS  asesitmaybepreferabletoapplya bayesianapproa  hin  2..ANewEstimationMethod  ribeanalternativemethodfores­timatingparametersofthemodel.Givenase­quen  eofwordsw[1:n]andasequen  eofpartof spee  htags,t[1:n],wewilltakethe""s  as¢s(hi,ti)=as<s(w[1:n],t[1:n]) i=1s=1 s=1  wherehiisagain(tiL1,tiL2,w[1:n],i).Notethat thisisalmostidenti  altoEq.2,butwithoutthe lo  ).Under thismethodforassignings  es,thehighests  anbefoundusingtheViterbi algorithm.(We  odingalgorithmtothatformaximum­entropy taggers,thediferen  ulated.)  Wethenproposethetrainingalgorithminfg­ure1.ThealgorithmtakesTpassesoverthe trainingsample.Allparametersareinitiallyset tobezero.Ea  odedus­ingthe  urrentparametersettings.Ifthehigh­ests  orre  t,theparametersasareupdatedina simpleadditivefashion.  Notethatifthelo  tions,thentheglobalfeatures<swillbe  ounts.Inthis  s -ds toea  hparameteras,where  sisthenumber oftimesthes'thfeatureo  urredinthe  e,anddsisthenumberoftimes  es, (wiTLifori=ln.AparameterT  ifyingnumberofiterationsoverthetrainingset.A ""lo  alrepresentation""<whi  tionthatmaps history/tagpairstod­dimensionalfeatureve  tors.The globalrepresentation<isdefnedthrough<asinEq.l. Initialization:Setparameterve  tor a=. Algorithm:  •UsetheViterbialgorithmtofndtheoutputofthe modelonthei'thtrainingsenten  ewiththe  rn.  whereisthesetofalltagsequen  •IfZ[1:n.] [1:n.]thenupdatetheparameters  s= s<s(wiTLi(wiTZ[1:n.])  Output:Parameterve  tor a.  Figure1:Thetrainingalgorithmfortagging.  ursinhighests  oringsequen  urrentmodel.Forexample,ifthefeatures¢s areindi  kingalltrigramsand word/tagpairs,thenthetrainingalgorithmis identi  altothatgiveninse  tion2.1.  Thereisasimplerefnementtothealgorithm infgure1,  alledthe""averagedparameters""  method.Defneastobethevalueforthes'th parameterafterthei'thtrainingexamplehas beenpro  essedinpasstoverthetrainingdata. Thenthe""averagedparameters""aredefnedas Is =atNi/nTforalls=1d.  Itissimpletomodifythealgorithmtostore thisadditionalsetofparameters.Experiments inse  tion4showthattheaveragedparameters performsignif  antlybetterthanthefnalpa­  rametersas.Thetheoryinthenextse  ationfortheaveragingmethod.  3TheoryJustifyingtheAlgorithm  Inthisse  tionwegiveageneralalgorithmfor problemssu  hastaggingandparsing,andgive theoremsjustifyingthealgorithm.Wealsoshow howthetaggingalgorithminfgure1isaspe­  aseofthisalgorithm.Convergen  etheo­remsfortheper  ation problemsappearin(Freund&S  hapire99) theresultsinthisse  ­tion5,showhowthe  Inputs:Trainingexamples(xiTYi) Initialization:Set a= Algorithm:  If(Zi then =a<(TYi)-<(xiTZ  Figure2:Avariantoftheper  eptronalgorithm.  Thetaskistolearnamappingfrominputs xEXtooutputsyEY.Forexample,Xmight beasetofsenten  es,withYbeingasetofpos­sibletagsequen  Trainingexamples(xi,yi)fori=1n.  tionGENwhi  henumeratesasetof  andidatesGEN(x)foraninputx.  Arepresentation<mappingea  h(x,y)E XYtoafeatureve  •Aparameterve  The  defneamap­pingfromaninputxtoanoutputF(x)through  where<(x,y)·a  t (x,y).Thelearningtaskistosetthe  usingthetrainingexamples aseviden  Thetaggingprobleminse  tion2  anbe mappedtothissettingasfollows:  •Thetrainingexamplesaresenten  Givenasetofpossibletags,wedefne GEN(w[1:n])=n ,i.e.,thefun  etothesetof  Therepresentation<(x,y)= <(w[1:n],t[1:n])isdefnedthroughlo  tors¢(h,t)where(h,t)isa history/tagpair.(SeeEq.1.)  Figure2showsanalgorithmforsettingthe weightsa  .It  anbeverifedthatthetraining  algorithmfortaggersinfgure1isaspe  ase ofthisalgorithm,ifwedefne(xi,yi),GENand <asjustdes  Wewillnowgiveafrsttheoremregarding the  eofthisalgorithm.Thistheorem thereforealsodes  ribes  onditionsunderwhi  h thealgorithminfgure1  onverges.First,we needthefollowingdefnition:  Defnition1LetGEN(xi)=GEN(xi)-{Yi}.In otherwordsGEN(xi)isthesetofin  orre  andidates foranexamplexi.Wewillsaythatatrainingsequen  e (xiTYi)fori=lnisseparablewithmarginÆ ifthereexistssomeve  torUwithllUll=lsu  hthat  .llUllisthe2-normofU,i.e.,llUll=sUs2.J  anthenstatethefollowingtheorem(see se  tion5foraproof): Theorem1Foranytrainingsequen  his separablewithmarginÆ,thenfortheper  eptronalgorithm infgure2  Æ2 whereRisa  Thistheoremimpliesthatifthereisaparam­eterve  torUwhi  hmakeszeroerrorsonthe trainingset,thenafterafnitenumberofitera­tionsthetrainingalgorithmwillhave  onverged toparametervalueswithzerotrainingerror.A  ialpointisthatthenumberofmistakesisin­dependentofthenumberof  h example(i.e.thesizeofGEN(xi)forea  hi), dependingonlyontheseparationofthetraining data,whereseparationisdefnedabove.This isimportantbe  anbeexponentialinthesizeofthe inputs.Allofthe  eandgeneraliza­tionresultsinthispaperdependonnotionsof separabilityratherthanthesizeofGEN.  ometomind.First,arethere guaranteesforthealgorithmifthetrainingdata isnotseparable?Se  ond,performan  eona trainingsampleisallverywell,butwhatdoes thisguaranteeabouthowwellthealgorithm generalizestonewlydrawntestexamples?(Fre­und&S  usshowthetheory  an beextendedtodealwithbothofthesequestions. Thenextse  ribehowtheseresults  an beappliedtothealgorithmsinthispaper.  3.1Theoryforinseparabledata  Inthisse  tionwegiveboundswhi  happlywhen thedataisnotseparable.First,weneedthe followingdefnition:  ThevalueDuNÆisameasureofhow  loseU istoseparatingthetrainingdatawithmarginÆ. DuNÆis0iftheve  torUseparatesthedatawith atleastmarginÆ.IfUseparatesalmostallof theexampleswithmarginÆ,butafewexamples arein  orre  tlytaggedorhavemarginlessthan Æ,thenDuNÆwilltakearelativelysmallvalue.  Thefollowingtheoremthenapplies(seese  ­tion5foraproof):  Theorem2Foranytrainingsequen  e(xiTYi),forthe frstpassoverthetrainingsetoftheper  eptronalgorithm infgure2,  whereRisa  hthatViTVZE GEN(xi)ll<(xiTYi)-<(xiTZ)ll:R,andthe ministakenoverÆ,llUll=l.  Thistheoremimpliesthatifthetrainingdata is""  lose""tobeingseparablewithmarginÆ i.e.,thereexistssomeUsu  hthatDuNÆisrela­tivelysmallthenthealgorithmwillagainmake asmallnumberofmistakes.Thustheorem2 showsthattheper  eptronalgorithm  anbero­busttosometrainingdataexamplesbeingdif­f  orre  tly.  3.2Generalizationresults  Theorems1and2giveresultsboundingthe numberoferrorsontrainingsamples,butthe questionwearereallyinterestedin  erns guaranteesofhowwellthemethodgeneralizes tonewtestexamples.Fortunately,thereare severaltheoreti  alresultssuggestingthatifthe per  eptronalgorithmmakesarelativelysmall numberofmistakesonatrainingsamplethenit islikelytogeneralizewelltonewexamples.This se  ribessomeoftheseresults,whi  h originallyappearedin(Freund&S  tlyfromresultsin(Helm­boldandWarmuth95).  ationoftheper  ep­tronalgorithm,thevotedper  onsiderthefrstpassoftheper  hofthesewilldefneanoutput Vi =argmaxzEGEN(x)a  1Ni·<(x,Z).Thevoted per  eptrontakesthemostfrequentlyo  urring outputintheset{V1Vn}.Thusthevoted per  eptronisamethodwhereea  1Nifori=1ngetasin­glevotefortheoutput,andthemajoritywins. Theaveragedalgorithminse  tion2.5  anbe  onsideredtobeanapproximationofthevoted method,withtheadvantagethatasinglede  od­ingwiththeaveragedparameters  anbeper­formed,ratherthannde  odingswithea  hof thenparametersettings.  Inanalyzingthevotedper  eptrontheoneas­sumptionwewillmakeisthatthereissome unknowndistributionP(x,y)overthesetX Y,andthatbothtrainingandtestexamples aredrawnindependently,identi  allydistributed (i.i.d.)fromthisdistribution.Corollary1of (Freund&S  hapire99)thenstates:  eoftrainingexamples andlet(xn+1TYn+1)beatestexample.Thentheprob­ability.overthe  eofallnlexamplesJthatthe voted-per  whereEn+1[]isanexpe  tedvaluetakenovernlex­amples,RandDu,Æareasdefnedabove,andtheminis takenoverÆ,llUll=l.  Weranexperimentsontwodatasets:part­of­spee  htaggingonthePennWallStreetJournal treebank(Mar  usetal.93),andbasenoun­phrasere  ognitiononthedatasetsoriginallyin­trodu  edby(RamshawandMar  us95).Inea  asewehadatraining,developmentandtestset. Forpart­of­spee  htaggingthetrainingsetwas se  tions0 18ofthetreebank,thedevelopment setwasse  tions19 21andthefnaltestsetwas se  tions22­24.InNP  CurrentwordW. &t.  PreviouswordW..l &t.  NextwordW.+l &t.  WordtwoaheadW.+2 &t.  CurrenttagP. &t.  PrevioustagP..l &t.  TagtwoaheadP.+2 &t.  Figure3:FeaturetemplatesusedintheNP  hunking experiments.wiisthe  e.PiisPOStagforthe  urrentword,and P1PnisthePOSsequen  eforthesenten  e.Liisthe  hunkingtagassignedtothei'thword.  tion15 18,thedevelopment setwasse  tion21,andthetestsetwasse  tion 20.ForPOStaggingwereporttheper  entage of  orre  or­respondingtobaseNP  ..2Features  alfeaturesto thoseof(Ratnaparkhi96),theonlydiferen  e beingthatwedidnotmaketherareworddis­tin  tionintable1of(Ratnaparkhi96)(i.e., spellingfeatureswerein  ludedforallwordsin trainingdata,andtheworditselfwasusedasa featureregardlessofwhetherthewordwasrare). Thefeaturesettakesintoa  ounttheprevious tagandpreviouspairsoftagsinthehistory,as wellasthewordbeingtagged,spellingfeatures ofthewordsbeingtagged,andvariousfeatures ofthewordssurroundingthewordbeingtagged.  Inthe  h forthosewordsfromthetaggerin(Brill95).Ta­ble3showsthefeaturesusedintheexperiments. The  hunkingproblemisrepresentedasathree­tagtask,wherethetagsareB,I,0forwords beginninga  tively.All  hunksbe­ginwithaBsymbol,regardlessofwhetherthe previouswordistagged0orI.  Method F­Measure Numits  Method Errorrate/% Numits  Figure4:Resultsforvariousmethodsonthepart­of­spee  entages.Numitsisthenumber oftrainingiterationsatwhi  hthebests  hieved. Per  istheper  eptronalgorithm,MEisthemaximum entropymethod.Avg/noavgistheper  eptronwithor withoutaveragedparameterve  tors.  =5meansonly featureso  urring5timesormoreintrainingarein­  =meansallfeaturesintrainingarein  Weappliedbothmaximum­entropymodelsand theper  eptronalgorithmtothetwotagging problems.Wetestedseveralvariantsforea  h algorithmonthedevelopmentset,togainsome understandingofhowthealgorithms'perfor­man  evariedwithvariousparametersettings, andtoallowoptimizationoffreeparametersso thatthe  omparisononthefnaltestsetisafair one.Forbothmethods,wetriedthealgorithms withfeature  ount  ut­ofssetat0and5(i.e., weranexperimentswithallfeaturesintraining datain  luded,orwithallfeatureso  urring5 timesormorein  ount  ut­ofof5).Intheper  eptronalgo­rithm,thenumberofiterationsToverthetrain­ingsetwasvaried,andthemethodwastested withbothaveragedandunaveragedparameter  tors(i.e.,withasandIs,asdefnedin se  tion2.5,foravarietyofvaluesforT).In themaximumentropymodelthenumberofit­erationsoftrainingusingGeneralizedIterative S  Figure4showsresultsondevelopmentdata onthetwotasks.Thetrendsarefairly  antlyforthe per  eptronmethod,asdoesin  ludingallfea­turesratherthanimposinga  ount  ut­ofof5. In  ontrast,theMEmodels'performan  luded.Thebestper  onfgurationgivesimprovementsoverthe maximum­entropymodelsinboth  ases:anim­provementinF­measurefrom9265%to9353% in  tionfrom328%to 293%errorrateinPOStagging.Inlooking attheresultsfordiferentnumbersofiterations ondevelopmentdatawefoundthataveraging notonlyimprovesthebestresult,butalsogives mu  hgreaterstabilityofthetagger(thenon­averagedvarianthasmu  hgreatervarian  ores).  Asafnaltest,theper  eptronandMEtag­gerswereappliedtothetestsets,withtheop­timalparametersettingsondevelopmentdata. OnPOStaggingtheper  omparedto3.28%errorforthe maximum­entropymodel(a11.9%relativere­du  tioninerror).InNP  hunkingtheper  hievesanF­measureof93.63%, in  ontrasttoanF­measureof93.29%forthe MEmodel(a5.1%relativeredu  tioninerror).  5ProofsoftheTheorems  Thisse  tiongivesproofsoftheorems1and2. Theproofsareadaptedfromproofsforthe  ation  ProofofTheorem1:Leta  betheweights beforethek'thmistakeismade.Itfollowsthat a  1=0.Supposethek'thmistakeismadeat thei'thexample.TakeZtotheoutputproposed atthisexample,Z=argmaxyEGEN(xi)<(xi,y)·  .Itfollowsfromthealgorithmupdatesthat  tsofbothsideswiththeve  torU:  wheretheinequalityfollowsbe  auseoftheprop­ertyofUassumedinEq.3.Be  1=0, andthereforeU·a  1=0,itfollowsbyindu  tiononkthatforallk,U·a  itfollowsthat IIa  WealsoderiveanupperboundforIIa  wheretheinequalityfollowsbe  ause II<(xi,yi)-<(xi,Z)II2:R2byassump­tion,anda  ause Zisthehighests  andidateforxiunder theparametersa  k .Itfollowsbyindu  tionthat IIa  CombiningtheboundsIIa  :kRgivestheresultforallkthat  ProofofTheorem2:Wetransformtherep­resentation<(x,y)E.dtoanewrepresentation a.d+n  <d+j(x,y)=6if(x,y)=(xj,yj),0otherwise, where6isaparameterwhi  hisgreaterthan0. Similary,saywearegivenaU,Æpair,and  or­respondingvaluesforEiasdefnedabove.We D.d+n  torUE  withUi =Uifori=1dandUd+j=Ej/6 forj=1n.Underthesedefnitionsit  It  anbeseenthattheve  torU/IIDseparates  thedatawithmarginÆ/1+D2/62 .Bythe­  orem1,thismeansthatthefrstpassoftheper­a  eptronalgorithmwithrepresentation<makes  atmostkmax(6)= Æ1 2(R2+62)(1+}u 2 ,Æ)mis­takes.Butthefrstpassoftheoriginalalgo­rithmwithrepresentation<isidenti  altothe frstpassofthealgorithmwithrepresentation  ausetheparameterweightsfortheaddi­a  tionalfeatures<d+jforj=1nea  ta singleexampleoftrainingdata,anddonotafe  t the  ationoftestdataexamples.Thus theoriginalper  eptronalgorithmalsomakesat mostkmax(6)mistakesonitsfrstpassoverthe trainingdata.Finally,we  withrespe  tto6,giving6=RDuNÆ,and  kmax(RDuNÆ)=(R2+D2)/Æ2 ,implyingthe  boundinthetheorem.  6Con  ribednewalgorithmsfortagging, whoseperforman  eguaranteesdependonano­tionof""separability""oftrainingdataexam­ples.Thegeneri  algorithminfgure2,and  thetheoremsdes  ouldbeappliedtoseveralothermodels intheNLPliterature.Forexample,aweighted  analsobe  ,sothe weightsforgenerativemodelssu  hasPCFGs  ouldbetrainedusingthismethod.  hapireandYoram Singerformanyusefuldis  ussionsregarding thealgorithmsinthispaper,andtoFernando PereiraforpointerstotheNP  hunkingdata set,andforsuggestionsregardingthefeatures usedintheexperiments.  Brill,E.(l995).Transformation­BasedError­Driven LearningandNaturalLanguagePro  essing:ACase StudyinPartofSpee  s.  Collins,M.,andDufy,N.(2l).ConvolutionKernels forNaturalLanguage.InPro  eedingsofNeuralInfor­mationPro  Collins,M.,andDufy,N.(22).NewRankingAlgo­rithmsforParsingandTagging:KernelsoverDis  tures,andtheVotedPer  Collins,M.(22).RankingAlgorithmsforNamed. EntityExtra  tion:BoostingandtheVotedPer  ationusingthePer  eptronAlgorithm.InMa  hine Learning,37(3):277.296.  Helmbold,D.,andWarmuth,M.Onweaklearning.Jour­nalofComputerandSystemS  Laferty,J.,M  Callum,A.,Freitag,D.,andPereira,F.(2)Max­imumentropymarkovmodelsforinformationextra  ­tionandsegmentation.InPro  z,M.(l993). Buildingalargeannotated  orpusofenglish:The Penntreebank.ComputationalLinguisti  Ramshaw,L.,andMar  us,M.P.(l995).TextChunking UsingTransformation­BasedLearning.InPro  eedings oftheThirdACLWorkshoponVeryLargeCorpora, Asso  ModelforInformationStorageandOrganizationinthe Brain.Psy  alReview,65,386.4 8.(Reprinted inNeuro  omputing(MITPress,l998).) ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","Proceedings of the Conference on Empirical Methods in Natural 1-8.Association for Computational Linguistics.riminativeTrainingMethodsforHiddenMarkovModels: TheoryandExperimentswithPer  eptronAlgorithms  haelCollins  AT&TLabs-Resear  h,FlorhamPark,NewJersey.ollinsresear  ribenewalgorithmsfortrain­ingtaggingmodels,asanalternative tomaximum­entropymodelsor  ondi­tionalrandomfelds(CRFs).Theal­gorithmsrelyonViterbide  odingof trainingexamples,  ombinedwithsim­pleadditiveupdates.Wedes  ribethe­oryjustifyingthealgorithmsthrough amodif  ationoftheproofof  eoftheper  eptronalgorithmfor  ationproblems.Wegiveexper­imentalresultsonpart­of­spee  hunking,in both  asesshowingimprovementsover resultsforamaximum­entropytagger.tion  efortaggingproblemsin NaturalLanguagePro  essing:forexamplesee (Ratnaparkhi96)fortheiruseonpart­of­spee  Callumetal.2000)fortheir useonaFAQsegmentationtask.MEmodels havetheadvantageofbeingquitefexibleinthe featuresthat  orporatedinthemodel.However,re  alandexperimentalre­sultsin(Lafertyetal.2001)havehighlighted problemswiththeparameterestimationmethod forMEmodels.Inresponsetotheseproblems, theydes  ribealternativeparameterestimation methodsbasedonConditionalMarkovRandom Fields(CRFs).(Lafertyetal.2001)giveexper­imentalresultssuggestingthatCRFs  antlybetterthanMEmodels.Inthispaperwedes  ribeparameterestima­tionalgorithmswhi  harenaturalalternativesto CRFs.Thealgorithmsarebasedontheper  hapire99).Thesealgorithms havebeenshownby(Freund&S  hapire99)to be  ompetitivewithmodernlearningalgorithms su  hassupportve  hines;however,they havepreviouslybeenappliedmainlyto  ationtasks,anditisnotentirely  learhowthe algorithms  anbe  Thispaperdes  ribesvariantsoftheper  ep­tronalgorithmfortaggingproblems.Theal­gorithmsrelyonViterbide  odingoftraining examples,  ombinedwithsimpleadditiveup­dates.Wedes  ribetheoryjustifyingthealgo­rithmthroughamodif  ationoftheproofof  eoftheper  eptronalgorithmfor  ationproblems.Wegiveexperimentalresults onpart­of­spee  hunking,inboth  asesshowingimprovements overresultsforamaximum­entropytagger(a 11.9%relativeredu  tioninerrorforPOStag­ging,a5.1%relativeredu  hunking).Althoughwe  entrateontagging problemsinthispaper,thetheoreti  tion3of thispapershouldbeappli  abletoawideva­rietyofmodelswhereViterbi­stylealgorithms  Context­FreeGrammars,orMEmodels forparsing.See(CollinsandDufy2001;Collins andDufy2002;Collins2002)forotherappli  a­tionsofthevotedper  Inthisse  tion,asamotivatingexample,wede­s  aseofthealgorithminthis paper:thealgorithmappliedtoatrigramtag­ger.InatrigramHMMtagger,ea  tronalgorithm(Rosenblatt58),andthevoted  1Thetheoremsinse  tion3,andtheproofsinse  ­oraveragedversionsoftheper  ribed tion5,applydire  tlytotheworkintheseotherpapers.htag/wordpairhaveasso  iated parameters.Wewritetheparameterasso  iated withatrigram(x,y,Z)asaxNyNz,andtheparam­eterasso  iatedwithatag/wordpair(t,w)as atNw.A  histotaketheparam­eterstobeestimatesof  onditionalprobabilities: axNyNz=logP(ZIx,y),atNw=logP(wIt).ewewillusew[1:n]asshort­handforasequen  eofwords[w1,w2wn], andt[1:n]asshorthandforataqsequen  e [t1,t2tn].Inatrigramtaggerthes  ew[1:n]is2i=1ati 2Nti 1Nti+i=1atiNwi.Whentheparametersare  onditionalprobabil­itiesasabovethis""s  ore""isanestimateofthe logofthejointprobabilityP(w[1:n],t[1:n]).The Viterbialgorithm  anbeusedtofndthehighest s  ore.Asanalternativetomaximum likelihoodpa­rameterestimates,thispaperwillproposethe followingestimationalgorithm.Saythetrain­ingset  es,thei'th senten  ebeingoflengthni.Wewillwritethese examplesas(wi,tfori=1n.Then  thetrainingalgorithmisasfollows:  •ChooseaparameterTdefningthenumber  ofiterationsoverthetrainingset.InitiallysetallparametersaxNyNzandatNw tobezero.Fort=1T,i=1n:UsetheViterbi algorithmtofndthebesttaggedsequen  allthistaggedsequen  2timesinZ[1:ni]where  2.Foreverytag/word pair(t,w)seen  1timesin(wi,tand  2timesin(wi,Zwhere  2.Asanexample,saythei'thtaggedsenten  the/Dman/Nsaw/Vthe/Ddog/N  andunderthe  urrentparametersettingsthe highests  2WetakeL 1andL 2tobespe  ialNULLtagsymbols.3Tisusually  the/Dman/Nsaw/Nthe/Ddog/N  Thentheparameterupdatewilladd1tothe parametersaNNNv,aNNvN ,avN NN,avNsawand subtra  t1fromtheparametersaNNNN,aNNNN , aNN NN,aNNsaw.Intuitivelythishastheef­fe  tofin  reasingtheparametervaluesforfea­tureswhi  hwere""missing""fromtheproposed sequen  eZ[1:ni],anddownweightingparameter valuesfor""in  orre  t""featuresinthesequen  hanges aremadetotheparametervalues.tors  ribehowtogeneralizethealgorithm tomoregeneralrepresentationsoftaggedse­quen  es.Inthisse  tionwedes  ribethefeature­ve  torrepresentationswhi  ommonlyused inmaximum­entropymodelsfortagging,and whi  harealsousedinthispaper.isionsin taggingtheprobleminleft­to­rightfashion.At ea  hpointthereisa""history"" the  ontextin whi  isionismade andthetask istopredi  tthetaggiventhehistory.Formally, ahistoryisa4­tuple(tL1,tL2,w[1:n],i)where tL1,tL2aretheprevioustwotags,w[1:n]isanar­rayspe  e, andiistheindexofthewordbeingtagged.We use1todenotethesetofallpossiblehistories.Maximum­entropymodelsrepresentthetag­gingtaskthroughafeature-ve  torrepresentation ofhistory­tagpairs.Afeatureve  tion¢thatmapsa history tagpairtoad­dimensionalfeatureve  ­tor.Ea  omponent¢s(h,t)fors=1d  tionof(h,t).Itis  ommon(e.g.,see(Ratnaparkhi96))forea  h feature¢stobeanindi  tion.Forex­ample,onesu  hfeaturemightbe  urrentwordwiisthe  0otherwise  Similarfeaturesmightbedefnedforevery word/tagpairseenintrainingdata.Another  featuretypemighttra  ktrigramsoftags,forex­ample¢1oo1(h,t)=1if(tL2,tL1,t)=(D,N,V) and0otherwise.Similarfeatureswouldbede­fnedforalltrigramsoftagsseenintraining.A realadvantageofthesemodels  omesfromthe freedomindefningthesefeatures:forexample, (Ratnaparkhi96;M  Callumetal.2000)both des  ribefeaturesetswhi  Inadditiontofeatureve  torrepresentations ofhistory/tagpairs,wewillfndit  onvenient todefnefeatureve  torsof(w[1:n],t[1:n])pairs wherew[1:n]isasequen  tionfrom(w[1:n],t[1:n])pairstod­dimensionalfeatureve  tors.Wewilloftenrefer to<asa""global""representation,in  al""representation.Theparti  onsideredinthispaper aresimplefun  tionsoflo  wherehi =(tiL1,tiL2,w[1:n],i).Ea  hglobal feature<s(w[1:n],t[1:n])issimplythevaluefor thelo  alrepresentation¢ssummedoverallhis­tory/tagpairsin(w[1:n],t[1:n]).Ifthelo  tions,thentheglobalfea­tureswilltypi  ounts"".Forexample, with¢1ooodefnedasabove,<1ooo(w[1:n],t[1:n]) isthenumberoftimestheisseentaggedasDT inthepairofsequen  Inmaximum­entropytaggersthefeatureve  tors ¢togetherwithaparameterve  onditionalprobabilitydistri­butionovertagsgivenahistoryas  as4s(hNt)  as4s(hNl)  whereZ(h,a  )=s.Thelogof  lETethisprobabilityhastheformlogp(tIh,a  s=1as¢s(h,t)-logZ(h,a  wherehi =(tiL1,tiL2,w,i).Givenparame­  ethe  w[1:n],highestprobabilitytaggedsequen  eunderthe formulainEq.2  anbefoundeÆ  ientlyusing theViterbialgorithm.Theparameterve  isestimatedfroma trainingsetofsenten  anbe estimatedusingGeneralizedIterativeS  asesitmaybepreferabletoapplya bayesianapproa  hin  2..ANewEstimationMethod  ribeanalternativemethodfores­timatingparametersofthemodel.Givenase­quen  eofwordsw[1:n]andasequen  eofpartof spee  htags,t[1:n],wewilltakethe""s  as¢s(hi,ti)=as<s(w[1:n],t[1:n]) i=1s=1 s=1  wherehiisagain(tiL1,tiL2,w[1:n],i).Notethat thisisalmostidenti  altoEq.2,butwithoutthe lo  ).Under thismethodforassignings  es,thehighests  anbefoundusingtheViterbi algorithm.(We  odingalgorithmtothatformaximum­entropy taggers,thediferen  ulated.)Wethenproposethetrainingalgorithminfg­ure1.ThealgorithmtakesTpassesoverthe trainingsample.Allparametersareinitiallyset tobezero.Ea  odedus­ingthe  urrentparametersettings.Ifthehigh­ests  orre  t,theparametersasareupdatedina simpleadditivefashion.Notethatifthelo  tions,thentheglobalfeatures<swillbe  ounts.Inthis  s -ds toea  hparameteras,where  sisthenumber oftimesthes'thfeatureo  urredinthe  e,anddsisthenumberoftimes  es, (wiTLifori=ln.AparameterT  ifyingnumberofiterationsoverthetrainingset.A ""lo  alrepresentation""<whi  tionthatmaps history/tagpairstod­dimensionalfeatureve  tors.The globalrepresentation<isdefnedthrough<asinEq.l.Initialization:Setparameterve  tor a=.Algorithm:  •UsetheViterbialgorithmtofndtheoutputofthe modelonthei'thtrainingsenten  ewiththe  rn.  whereisthesetofalltagsequen  •IfZ[1:n.][1:n.]thenupdatetheparameters  s= s<s(wiTLi(wiTZ[1:n.])Output:Parameterve  tor a.Figure1:Thetrainingalgorithmfortagging.ursinhighests  oringsequen  urrentmodel.Forexample,ifthefeatures¢s areindi  kingalltrigramsand word/tagpairs,thenthetrainingalgorithmis identi  altothatgiveninse  tion2.1.Thereisasimplerefnementtothealgorithm infgure1,  alledthe""averagedparameters""  method.Defneastobethevalueforthes'th parameterafterthei'thtrainingexamplehas beenpro  essedinpasstoverthetrainingdata.Thenthe""averagedparameters""aredefnedas Is =atNi/nTforalls=1d.Itissimpletomodifythealgorithmtostore thisadditionalsetofparameters.Experiments inse  tion4showthattheaveragedparameters performsignif  antlybetterthanthefnalpa­  rametersas.Thetheoryinthenextse  ationfortheaveragingmethod.3TheoryJustifyingtheAlgorithm  Inthisse  tionwegiveageneralalgorithmfor problemssu  hastaggingandparsing,andgive theoremsjustifyingthealgorithm.Wealsoshow howthetaggingalgorithminfgure1isaspe­  aseofthisalgorithm.Convergen  etheo­remsfortheper  ation problemsappearin(Freund&S  hapire99) theresultsinthisse  ­tion5,showhowthe  Inputs:Trainingexamples(xiTYi) Initialization:Set a= Algorithm:  If(Zi then =a<(TYi)-<(xiTZ  Figure2:Avariantoftheper  eptronalgorithm.Thetaskistolearnamappingfrominputs xEXtooutputsyEY.Forexample,Xmight beasetofsenten  es,withYbeingasetofpos­sibletagsequen  Trainingexamples(xi,yi)fori=1n.tionGENwhi  henumeratesasetof  andidatesGEN(x)foraninputx.Arepresentation<mappingea  h(x,y)E XYtoafeatureve  •Aparameterve  The  defneamap­pingfromaninputxtoanoutputF(x)through  where<(x,y)·a  t (x,y).Thelearningtaskistosetthe  usingthetrainingexamples aseviden  Thetaggingprobleminse  tion2  anbe mappedtothissettingasfollows:  •Thetrainingexamplesaresenten  Givenasetofpossibletags,wedefne GEN(w[1:n])=n ,i.e.,thefun  etothesetof  Therepresentation<(x,y)= <(w[1:n],t[1:n])isdefnedthroughlo  tors¢(h,t)where(h,t)isa history/tagpair.(SeeEq.1.)Figure2showsanalgorithmforsettingthe weightsa  .It  anbeverifedthatthetraining  algorithmfortaggersinfgure1isaspe  ase ofthisalgorithm,ifwedefne(xi,yi),GENand <asjustdes  Wewillnowgiveafrsttheoremregarding the  eofthisalgorithm.Thistheorem thereforealsodes  ribes  onditionsunderwhi  h thealgorithminfgure1  onverges.First,we needthefollowingdefnition:  Defnition1LetGEN(xi)=GEN(xi)-{Yi}.In otherwordsGEN(xi)isthesetofin  orre  andidates foranexamplexi.Wewillsaythatatrainingsequen  e (xiTYi)fori=lnisseparablewithmarginÆ ifthereexistssomeve  torUwithllUll=lsu  hthat  .llUllisthe2-normofU,i.e.,llUll=sUs2.J  anthenstatethefollowingtheorem(see se  tion5foraproof): Theorem1Foranytrainingsequen  his separablewithmarginÆ,thenfortheper  eptronalgorithm infgure2  Æ2 whereRisa  Thistheoremimpliesthatifthereisaparam­eterve  torUwhi  hmakeszeroerrorsonthe trainingset,thenafterafnitenumberofitera­tionsthetrainingalgorithmwillhave  onverged toparametervalueswithzerotrainingerror.A  ialpointisthatthenumberofmistakesisin­dependentofthenumberof  h example(i.e.thesizeofGEN(xi)forea  hi), dependingonlyontheseparationofthetraining data,whereseparationisdefnedabove.This isimportantbe  anbeexponentialinthesizeofthe inputs.Allofthe  eandgeneraliza­tionresultsinthispaperdependonnotionsof separabilityratherthanthesizeofGEN.ometomind.First,arethere guaranteesforthealgorithmifthetrainingdata isnotseparable?Se  ond,performan  eona trainingsampleisallverywell,butwhatdoes thisguaranteeabouthowwellthealgorithm generalizestonewlydrawntestexamples?(Fre­und&S  usshowthetheory  an beextendedtodealwithbothofthesequestions.Thenextse  ribehowtheseresults  an beappliedtothealgorithmsinthispaper.3.1Theoryforinseparabledata  Inthisse  tionwegiveboundswhi  happlywhen thedataisnotseparable.First,weneedthe followingdefnition:  ThevalueDuNÆisameasureofhow  loseU istoseparatingthetrainingdatawithmarginÆ.DuNÆis0iftheve  torUseparatesthedatawith atleastmarginÆ.IfUseparatesalmostallof theexampleswithmarginÆ,butafewexamples arein  orre  tlytaggedorhavemarginlessthan Æ,thenDuNÆwilltakearelativelysmallvalue.Thefollowingtheoremthenapplies(seese  ­tion5foraproof):  Theorem2Foranytrainingsequen  e(xiTYi),forthe frstpassoverthetrainingsetoftheper  eptronalgorithm infgure2,  whereRisa  hthatViTVZE GEN(xi)ll<(xiTYi)-<(xiTZ)ll:R,andthe ministakenoverÆ,llUll=l.Thistheoremimpliesthatifthetrainingdata is""  lose""tobeingseparablewithmarginÆ i.e.,thereexistssomeUsu  hthatDuNÆisrela­tivelysmallthenthealgorithmwillagainmake asmallnumberofmistakes.Thustheorem2 showsthattheper  eptronalgorithm  anbero­busttosometrainingdataexamplesbeingdif­f  orre  tly.3.2Generalizationresults  Theorems1and2giveresultsboundingthe numberoferrorsontrainingsamples,butthe questionwearereallyinterestedin  erns guaranteesofhowwellthemethodgeneralizes tonewtestexamples.Fortunately,thereare severaltheoreti  alresultssuggestingthatifthe per  eptronalgorithmmakesarelativelysmall numberofmistakesonatrainingsamplethenit islikelytogeneralizewelltonewexamples.This se  ribessomeoftheseresults,whi  h originallyappearedin(Freund&S  tlyfromresultsin(Helm­boldandWarmuth95).ationoftheper  ep­tronalgorithm,thevotedper  onsiderthefrstpassoftheper  hofthesewilldefneanoutput Vi =argmaxzEGEN(x)a  1Ni·<(x,Z).Thevoted per  eptrontakesthemostfrequentlyo  urring outputintheset{V1Vn}.Thusthevoted per  eptronisamethodwhereea  1Nifori=1ngetasin­glevotefortheoutput,andthemajoritywins.Theaveragedalgorithminse  tion2.5  anbe  onsideredtobeanapproximationofthevoted method,withtheadvantagethatasinglede  od­ingwiththeaveragedparameters  anbeper­formed,ratherthannde  odingswithea  hof thenparametersettings.Inanalyzingthevotedper  eptrontheoneas­sumptionwewillmakeisthatthereissome unknowndistributionP(x,y)overthesetX Y,andthatbothtrainingandtestexamples aredrawnindependently,identi  allydistributed (i.i.d.)fromthisdistribution.Corollary1of (Freund&S  hapire99)thenstates:  eoftrainingexamples andlet(xn+1TYn+1)beatestexample.Thentheprob­ability.overthe  eofallnlexamplesJthatthe voted-per  whereEn+1[]isanexpe  tedvaluetakenovernlex­amples,RandDu,Æareasdefnedabove,andtheminis takenoverÆ,llUll=l.Weranexperimentsontwodatasets:part­of­spee  htaggingonthePennWallStreetJournal treebank(Mar  usetal.93),andbasenoun­phrasere  ognitiononthedatasetsoriginallyin­trodu  edby(RamshawandMar  us95).Inea  asewehadatraining,developmentandtestset.Forpart­of­spee  htaggingthetrainingsetwas se  tions0 18ofthetreebank,thedevelopment setwasse  tions19 21andthefnaltestsetwas se  tions22­24.InNP  CurrentwordW.&t.PreviouswordW..l &t.NextwordW.+l &t.WordtwoaheadW.+2 &t.CurrenttagP.&t.PrevioustagP..l &t.TagtwoaheadP.+2 &t.Figure3:FeaturetemplatesusedintheNP  hunking experiments.wiisthe  e.PiisPOStagforthe  urrentword,and P1PnisthePOSsequen  eforthesenten  e.Liisthe  hunkingtagassignedtothei'thword.tion15 18,thedevelopment setwasse  tion21,andthetestsetwasse  tion 20.ForPOStaggingwereporttheper  entage of  orre  or­respondingtobaseNP  ..2Features  alfeaturesto thoseof(Ratnaparkhi96),theonlydiferen  e beingthatwedidnotmaketherareworddis­tin  tionintable1of(Ratnaparkhi96)(i.e., spellingfeatureswerein  ludedforallwordsin trainingdata,andtheworditselfwasusedasa featureregardlessofwhetherthewordwasrare).Thefeaturesettakesintoa  ounttheprevious tagandpreviouspairsoftagsinthehistory,as wellasthewordbeingtagged,spellingfeatures ofthewordsbeingtagged,andvariousfeatures ofthewordssurroundingthewordbeingtagged.Inthe  h forthosewordsfromthetaggerin(Brill95).Ta­ble3showsthefeaturesusedintheexperiments.The  hunkingproblemisrepresentedasathree­tagtask,wherethetagsareB,I,0forwords beginninga  tively.All  hunksbe­ginwithaBsymbol,regardlessofwhetherthe previouswordistagged0orI.Method F­Measure Numits  Method Errorrate/% Numits  Figure4:Resultsforvariousmethodsonthepart­of­spee  entages.Numitsisthenumber oftrainingiterationsatwhi  hthebests  hieved.Per  istheper  eptronalgorithm,MEisthemaximum entropymethod.Avg/noavgistheper  eptronwithor withoutaveragedparameterve  tors.=5meansonly featureso  urring5timesormoreintrainingarein­  =meansallfeaturesintrainingarein  Weappliedbothmaximum­entropymodelsand theper  eptronalgorithmtothetwotagging problems.Wetestedseveralvariantsforea  h algorithmonthedevelopmentset,togainsome understandingofhowthealgorithms'perfor­man  evariedwithvariousparametersettings, andtoallowoptimizationoffreeparametersso thatthe  omparisononthefnaltestsetisafair one.Forbothmethods,wetriedthealgorithms withfeature  ount  ut­ofssetat0and5(i.e., weranexperimentswithallfeaturesintraining datain  luded,orwithallfeatureso  urring5 timesormorein  ount  ut­ofof5).Intheper  eptronalgo­rithm,thenumberofiterationsToverthetrain­ingsetwasvaried,andthemethodwastested withbothaveragedandunaveragedparameter  tors(i.e.,withasandIs,asdefnedin se  tion2.5,foravarietyofvaluesforT).In themaximumentropymodelthenumberofit­erationsoftrainingusingGeneralizedIterative S  Figure4showsresultsondevelopmentdata onthetwotasks.Thetrendsarefairly  antlyforthe per  eptronmethod,asdoesin  ludingallfea­turesratherthanimposinga  ount  ut­ofof5.In  ontrast,theMEmodels'performan  luded.Thebestper  onfgurationgivesimprovementsoverthe maximum­entropymodelsinboth  ases:anim­provementinF­measurefrom9265%to9353% in  tionfrom328%to 293%errorrateinPOStagging.Inlooking attheresultsfordiferentnumbersofiterations ondevelopmentdatawefoundthataveraging notonlyimprovesthebestresult,butalsogives mu  hgreaterstabilityofthetagger(thenon­averagedvarianthasmu  hgreatervarian  ores).Asafnaltest,theper  eptronandMEtag­gerswereappliedtothetestsets,withtheop­timalparametersettingsondevelopmentdata.OnPOStaggingtheper  omparedto3.28%errorforthe maximum­entropymodel(a11.9%relativere­du  tioninerror).InNP  hunkingtheper  hievesanF­measureof93.63%, in  ontrasttoanF­measureof93.29%forthe MEmodel(a5.1%relativeredu  tioninerror).5ProofsoftheTheorems  Thisse  tiongivesproofsoftheorems1and2.Theproofsareadaptedfromproofsforthe  ation  ProofofTheorem1:Leta  betheweights beforethek'thmistakeismade.Itfollowsthat a  1=0.Supposethek'thmistakeismadeat thei'thexample.TakeZtotheoutputproposed atthisexample,Z=argmaxyEGEN(xi)<(xi,y)·  .Itfollowsfromthealgorithmupdatesthat  tsofbothsideswiththeve  torU:  wheretheinequalityfollowsbe  auseoftheprop­ertyofUassumedinEq.3.Be  1=0, andthereforeU·a  1=0,itfollowsbyindu  tiononkthatforallk,U·a  itfollowsthat IIa  WealsoderiveanupperboundforIIa  wheretheinequalityfollowsbe  ause II<(xi,yi)-<(xi,Z)II2:R2byassump­tion,anda  ause Zisthehighests  andidateforxiunder theparametersa  k .Itfollowsbyindu  tionthat IIa  CombiningtheboundsIIa  :kRgivestheresultforallkthat  ProofofTheorem2:Wetransformtherep­resentation<(x,y)E.dtoanewrepresentation a.d+n  <d+j(x,y)=6if(x,y)=(xj,yj),0otherwise, where6isaparameterwhi  hisgreaterthan0.Similary,saywearegivenaU,Æpair,and  or­respondingvaluesforEiasdefnedabove.We D.d+n  torUE  withUi =Uifori=1dandUd+j=Ej/6 forj=1n.Underthesedefnitionsit  It  anbeseenthattheve  torU/IIDseparates  thedatawithmarginÆ/1+D2/62 .Bythe­  orem1,thismeansthatthefrstpassoftheper­a  eptronalgorithmwithrepresentation<makes  atmostkmax(6)= Æ1 2(R2+62)(1+}u 2 ,Æ)mis­takes.Butthefrstpassoftheoriginalalgo­rithmwithrepresentation<isidenti  altothe frstpassofthealgorithmwithrepresentation  ausetheparameterweightsfortheaddi­a  tionalfeatures<d+jforj=1nea  ta singleexampleoftrainingdata,anddonotafe  t the  ationoftestdataexamples.Thus theoriginalper  eptronalgorithmalsomakesat mostkmax(6)mistakesonitsfrstpassoverthe trainingdata.Finally,we  withrespe  tto6,giving6=RDuNÆ,and  kmax(RDuNÆ)=(R2+D2)/Æ2 ,implyingthe  boundinthetheorem.6Con  ribednewalgorithmsfortagging, whoseperforman  eguaranteesdependonano­tionof""separability""oftrainingdataexam­ples.Thegeneri  algorithminfgure2,and  thetheoremsdes  ouldbeappliedtoseveralothermodels intheNLPliterature.Forexample,aweighted  analsobe  ,sothe weightsforgenerativemodelssu  hasPCFGs  ouldbetrainedusingthismethod.hapireandYoram Singerformanyusefuldis  ussionsregarding thealgorithmsinthispaper,andtoFernando PereiraforpointerstotheNP  hunkingdata set,andforsuggestionsregardingthefeatures usedintheexperiments.Brill,E.(l995).Transformation­BasedError­Driven LearningandNaturalLanguagePro  essing:ACase StudyinPartofSpee  s.Collins,M.,andDufy,N.(2l).ConvolutionKernels forNaturalLanguage.InPro  eedingsofNeuralInfor­mationPro  Collins,M.,andDufy,N.(22).NewRankingAlgo­rithmsforParsingandTagging:KernelsoverDis  tures,andtheVotedPer  Collins,M.(22).RankingAlgorithmsforNamed.EntityExtra  tion:BoostingandtheVotedPer  ationusingthePer  eptronAlgorithm.InMa  hine Learning,37(3):277.296.Helmbold,D.,andWarmuth,M.Onweaklearning.Jour­nalofComputerandSystemS  Laferty,J.,M  Callum,A.,Freitag,D.,andPereira,F.(2)Max­imumentropymarkovmodelsforinformationextra  ­tionandsegmentation.InPro  z,M.(l993).Buildingalargeannotated  orpusofenglish:The Penntreebank.ComputationalLinguisti  Ramshaw,L.,andMar  us,M.P.(l995).TextChunking UsingTransformation­BasedLearning.InPro  eedings oftheThirdACLWorkshoponVeryLargeCorpora, Asso  ModelforInformationStorageandOrganizationinthe Brain.Psy  alReview,65,386.4 8.(Reprinted inNeuro  omputing(MITPress,l998).)"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Beyond NomBank:  A Study of Implicit Arguments for Nominal Predicates Matthew Gerber and Joyce Y. Chai  Department of Computer Science  Michigan State University  East Lansing, Michigan, USA  producer and arg1 is the produced entity. The  second sentence contains an instance of the nominal  Despite its substantial coverage,  Nompredicate shipping that is not associated with argu-Bank does not account for all  withinments in NomBank (Meyers, 2007).  From the sentences in Example 1, the reader can  sentential arguments altogether. These  arinfer that The two companies refers to the agents  guments, which we call implicit, are  im(arg0) of the shipping predicate. The reader can  portant to semantic processing, and their  also infer that market pulp, containerboard and  recovery could potentially benefit many  white paper refers to the shipped entities (arg1  NLP applications. We present a study of  of shipping).1 These extra-sentential arguments  implicit arguments for a select group of  have not been annotated for the shipping  predifrequent nominal predicates. We show that  cate and cannot be identified by a system that  reimplicit arguments are pervasive for these  stricts the argument search space to the sentence  predicates, adding 65% to the coverage of  containing the predicate. NomBank also ignores  NomBank. We demonstrate the  feasibilmany within-sentence arguments. This is shown  ity of recovering implicit arguments with  in the second sentence of Example 1, where The  goods can be interpreted as the arg1 of shipping.  sults and analyses provide a baseline for  These examples demonstrate the presence of  argufuture work on this emerging task.  ments that are not included in NomBank and  cannot easily be identified by systems trained on the 1  ",0,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","Beyond NomBank: producer and arg1 is the produced entity. The  second sentence contains an instance of the nominal  Despite its substantial coverage,  Nompredicate shipping that is not associated with argu-Bank does not account for all  withinments in NomBank (Meyers, 2007).From the sentences in Example 1, the reader can  sentential arguments altogether.These  arinfer that The two companies refers to the agents  guments, which we call implicit, are  im(arg0) of the shipping predicate.The reader can  portant to semantic processing, and their  also infer that market pulp, containerboard and  recovery could potentially benefit many  white paper refers to the shipped entities (arg1  NLP applications.We present a study of  of shipping).1 These extra-sentential arguments  implicit arguments for a select group of  have not been annotated for the shipping  predifrequent nominal predicates.We show that  cate and cannot be identified by a system that  reimplicit arguments are pervasive for these  stricts the argument search space to the sentence  predicates, adding 65% to the coverage of  containing the predicate.NomBank also ignores  NomBank.We demonstrate the  feasibilmany within-sentence arguments.This is shown  ity of recovering implicit arguments with  in the second sentence of Example 1, where The  goods can be interpreted as the arg1 of shipping.sults and analyses provide a baseline for  These examples demonstrate the presence of  argufuture work on this emerging task.ments that are not included in NomBank and  cannot easily be identified by systems trained on the 1"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," More than Words:  Syntactic Packaging and Implicit Sentiment  ATG, Inc.  Linguistics / UMIACS CLIP Laboratory  1111 19th St, NW Suite 600  University of Maryland  Washington, DC 20036  College Park, MD 20742  Both descriptions appear on the surface to be  objective statements, and they use nearly the same words.  Work on sentiment analysis often focuses on  Lexically, the sentences' first clauses differ only in the words and phrases that people use in  the difference between 's and his to express the rela-overtly opinionated text. In this paper, we  intionship between the soldier and the jeep, and in the troduce a new approach to the problem that  second clauses both kill and death are terms with focuses not on lexical indicators, but on the  syntactic packaging of ideas, which is well  negative connotations, at least according to the  Gensuited to investigating the identification of  implicit sentiment, or perspective. We establish a tions clearly differ in the feelings they evoke: if the strong predictive connection between linguis-soldier were being tried for his role in what  haptically well motivated features and implicit  pened on November 25, surely the prosecutor would  sentiment, and then show how computational  be more likely to say (1a) to the jury, and the defense approximations of these features can be used  attorney (1b), rather than the reverse.1  to improve on existing state-of-the-art  sentiWhy, then, should a description like (1a) be  perceived as less sympathetic to the soldier than (1b)?  If the difference is not in the words, it must be in  ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","More than Words: Syntactic Packaging and Implicit Sentiment Linguistics / UMIACS CLIP Laboratory  1111 19th St, NW Suite 600  University of Maryland  Washington, DC 20036  College Park, MD 20742  Both descriptions appear on the surface to be  objective statements, and they use nearly the same words.Work on sentiment analysis often focuses on  Lexically, the sentences' first clauses differ only in the words and phrases that people use in  the difference between 's and his to express the rela-overtly opinionated text.In this paper, we  intionship between the soldier and the jeep, and in the troduce a new approach to the problem that  second clauses both kill and death are terms with focuses not on lexical indicators, but on the  syntactic packaging of ideas, which is well  negative connotations, at least according to the  Gensuited to investigating the identification of  implicit sentiment, or perspective.We establish a tions clearly differ in the feelings they evoke: if the strong predictive connection between linguis-soldier were being tried for his role in what  haptically well motivated features and implicit  pened on November 25, surely the prosecutor would  sentiment, and then show how computational  be more likely to say (1a) to the jury, and the defense approximations of these features can be used  attorney (1b), rather than the reverse.1  to improve on existing state-of-the-art  sentiWhy, then, should a description like (1a) be  perceived as less sympathetic to the soldier than (1b)?If the difference is not in the words, it must be in"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Forest Reranking: Discriminative Parsing with Non-Local Features  University of Pennsylvania  conventional reranking  only at the root  Conventional n-best reranking techniques  ofDP-based discrim. parsing  ten suffer from the limited scope of the  nthis work: forest-reranking  on-the-fly  best list, which rules out many potentially  good alternatives. We instead propose forest  Table 1: Comparison of various approaches for  inreranking, a method that reranks a packed for-corporating local and non-local features.  est of exponentially many parses. Since  exact inference is intractable with non-local  features, we present an approximate algorithm  insentence length. As a result, we often see very few  spired by forest rescoring that makes  discrimvariations among the n-best trees, for example,  50inative training practical over the whole  Treebest trees typically just represent a combination of 5  bank. Our final result, an F-score of 91.7,  outto 6 binary ambiguities (since 25 < 50 < 26).  performs both 50-best and 100-best reranking  Alternatively, discriminative parsing is tractable  baselines, and is better than any previously  rewith exact and efficient search based on dynamic  ported systems trained on the Treebank.  programming (DP) if all features are restricted to be local, that is, only looking at a local window within 1  ",0,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","Forest Reranking: Discriminative Parsing with Non-Local Features conventional reranking only at the root Conventional n-best reranking techniques ofDP-based discrim. parsing  ten suffer from the limited scope of the  nthis work: forest-reranking  on-the-fly  best list, which rules out many potentially  good alternatives.We instead propose forest  Table 1: Comparison of various approaches for  inreranking, a method that reranks a packed for-corporating local and non-local features.est of exponentially many parses.Since  exact inference is intractable with non-local  features, we present an approximate algorithm  insentence length.As a result, we often see very few  spired by forest rescoring that makes  discrimvariations among the n-best trees, for example,  50inative training practical over the whole  Treebest trees typically just represent a combination of 5  bank.Our final result, an F-score of 91.7,  outto 6 binary ambiguities (since 25 < 50 < 26).performs both 50-best and 100-best reranking  Alternatively, discriminative parsing is tractable  baselines, and is better than any previously  rewith exact and efficient search based on dynamic  ported systems trained on the Treebank.programming (DP) if all features are restricted to be local, that is, only looking at a local window within 1"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Generating Focused Topic-specific Sentiment Lexicons Valentin Jijkoun  ISLA, University of Amsterdam, The Netherlands  networks etc. (Altheide, 1996). Recent advances  in language technology, especially in sentiment  We present a method for automatically  analysis, promise to (partially) automate this task.  generating focused and accurate  topicSentiment analysis is often considered in the  specific subjectivity lexicons from a  gencontext of the following two tasks:  eral purpose polarity lexicon that allow  users to pin-point subjective on-topic  in sentiment extraction: given a set of textual  formation in a set of relevant documents.  We motivate the need for such lexicons  in the field of media analysis, describe  tudes, and determine the polarity of these  ata bootstrapping method for generating a  titudes (Kim and Hovy, 2004); and  topic-specific lexicon from a general  pur sentiment retrieval: given a topic (and  possipose polarity lexicon, and evaluate the  bly, a list of documents relevant to the topic),  quality of the generated lexicons both  identify documents that express attitudes  tomanually and using a TREC Blog track  ward this topic (Ounis et al., 2007).  Although the generated lexicons can be an  How can technology developed for sentiment  order of magnitude more selective than the  analysis be applied to media analysis? In order  general purpose lexicon, they maintain, or  to use a sentiment extraction system for a media  even improve, the performance of an  opinanalysis problem, a system would have to be able  to determine which of the extracted sentiments are actually relevant, i.e., it would not only have to 1  ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","networks etc. (Altheide, 1996).Recent advances  in language technology, especially in sentiment  We present a method for automatically  analysis, promise to (partially) automate this task.generating focused and accurate  topicSentiment analysis is often considered in the  specific subjectivity lexicons from a  gencontext of the following two tasks:  eral purpose polarity lexicon that allow  users to pin-point subjective on-topic  in sentiment extraction: given a set of textual  formation in a set of relevant documents.We motivate the need for such lexicons  in the field of media analysis, describe  tudes, and determine the polarity of these  ata bootstrapping method for generating a  titudes (Kim and Hovy, 2004); and  topic-specific lexicon from a general  pur sentiment retrieval: given a topic (and  possipose polarity lexicon, and evaluate the  bly, a list of documents relevant to the topic),  quality of the generated lexicons both  identify documents that express attitudes  tomanually and using a TREC Blog track  ward this topic (Ounis et al., 2007).Although the generated lexicons can be an  How can technology developed for sentiment  order of magnitude more selective than the  analysis be applied to media analysis?In order  general purpose lexicon, they maintain, or  to use a sentiment extraction system for a media  even improve, the performance of an  opinanalysis problem, a system would have to be able  to determine which of the extracted sentiments are actually relevant, i.e., it would not only have to 1"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Dependency-based Syntactic Semantic Analysis with PropBank and NomBank  Richard Johansson and Pierre Nugues  Lund University, Sweden  {richard, pierre}@cs.lth.se  such as paths when predicting semantic structures, exact search is clearly intractable. This is true even This paper presents our contribution in the  with simpler feature representations the problem closed track of the 2008 CoNLL Shared  is a special case of multi-headed dependency anal-Task (Surdeanu et al., 2008). To tackle the  ysis, which is NP-hard even if the number of heads problem of joint syntactic semantic anal-is bounded (Chickering et al., 1994).  ysis, the system relies on a syntactic and  This means that we must resort to a simplifica-a semantic subcomponent. The syntactic  tion such as an incremental method or a reranking model is a bottom-up projective parser us-approach. We chose the latter option and thus cre-ing pseudo-projective transformations, and  ated syntactic and semantic submodels. The joint the semantic model uses global inference  syntactic semantic prediction is selected from a mechanisms on top of a pipeline of clas-small list of candidates generated by the respective sifiers. The complete syntactic semantic  output is selected from a candidate pool  generated by the subsystems.  The system achieved the top score in the  We model the process of syntactic parsing of  closed challenge: a labeled syntactic  accua sentence x as finding the parse tree  racy of 89.32%, a labeled semantic F1 of  arg maxy F (x, y) that maximizes a scoring func-81.65, and a labeled macro F1 of 85.49.  tion F . The learning problem consists of fitting this function so that the cost of the predictions is 1  ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","{richard, pierre}@cs.lth.se such as paths when predicting semantic structures, exact search is clearly intractable. This is true even This paper presents our contribution in the  with simpler feature representations the problem closed track of the 2008 CoNLL Shared  is a special case of multi-headed dependency anal-Task (Surdeanu et al., 2008).To tackle the  ysis, which is NP-hard even if the number of heads problem of joint syntactic semantic anal-is bounded (Chickering et al., 1994).ysis, the system relies on a syntactic and  This means that we must resort to a simplifica-a semantic subcomponent.The syntactic  tion such as an incremental method or a reranking model is a bottom-up projective parser us-approach.We chose the latter option and thus cre-ing pseudo-projective transformations, and  ated syntactic and semantic submodels.The joint the semantic model uses global inference  syntactic semantic prediction is selected from a mechanisms on top of a pipeline of clas-small list of candidates generated by the respective sifiers.The complete syntactic semantic  output is selected from a candidate pool  generated by the subsystems.The system achieved the top score in the  We model the process of syntactic parsing of  closed challenge: a labeled syntactic  accua sentence x as finding the parse tree  racy of 89.32%, a labeled semantic F1 of  arg maxy F (x, y) that maximizes a scoring func-81.65, and a labeled macro F1 of 85.49.tion F . The learning problem consists of fitting this function so that the cost of the predictions is 1"
" Relational Features in Fine-Grained  Richard Johansson?  University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account. This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions. These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable. However, a reranker can be used as  a sufficiently accurate and efficient approximation.  A number of feature sets and machine learning approaches for the rerankers are evaluated.  For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly. Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively. In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks. In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.  1. "," Joint Extraction of Entities and Relations for Opinion Recognition Yejin Choi and Eric Breck and Claire Cardie  Department of Computer Science  Cornell University  Ithaca, NY 14853  (2005) and Kim and Hovy (2005b)), to determine  the polarity and strength of opinion expressions  We present an approach for the joint  ex(e.g. Wilson et al. (2005)), and to recognize propo-traction of entities and relations in the  consitional opinions and their sources (e.g. Bethard  text of opinion recognition and analysis.  et al. (2004)) with reasonable accuracy. To date,  We identify two types of opinion-related  however, there has been no effort to  simultaneentities expressions of opinions and  ously identify arbitrary opinion expressions, their sources of opinions along with the link-sources, and the relations between them. Without  ing relation that exists between them.  Inprogress on the joint extraction of opinion enti-spired by Roth and Yih (2004), we employ  ties and their relations, the capabilities of opinion-an integer linear programming approach  based applications will remain limited.  to solve the joint opinion recognition task,  and show that global, constraint-based  inFortunately, research in machine learning has  ference can significantly boost the  perforproduced methods for global inference and joint  mance of both relation extraction and the  classification that can help to address this  defiextraction of opinion-related entities.  Perciency (e.g. Bunescu and Mooney (2004), Roth  formance further improves when a  semanand Yih (2004)). Moreover, it has been shown that  tic role labeling system is incorporated.  The resulting system achieves F-measures  lations via global inference not only solves the  of 79 and 69 for entity and relation  extracjoint extraction task, but often boosts performance tion, respectively, improving substantially  on the individual tasks when compared to  clasover prior results in the area.  sifiers that handle the tasks independently for  ",1,"Relational Features in Fine-Grained University of Gothenburg  University of Trento  Fine-grained opinion analysis methods often make use of linguistic features but typically do  not take the interaction between opinions into account.This article describes a set of experiments  that demonstrate that relational features, mainly derived from dependency-syntactic and  semantic role structures, can significantly improve the performance of automatic systems for a  number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion  holders, and determining the polarities of opinion expressions.These features make it possible  to model the way opinions expressed in natural-language discourse interact in a sentence over  arbitrary distances.The use of relations requires us to consider multiple opinions simultaneously,  which makes the search for the optimal analysis intractable.However, a reranker can be used as  a sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute  improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local  contextual features, while precision decreases only slightly.Significant improvements are also  seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,  respectively.In addition, the systems outperform previously published results for unlabeled (6  F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.Finally,  as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical  opinion mining tasks.In all scenarios considered, the machine learning features derived from the  opinion expressions lead to statistically significant improvements.1.","the polarity and strength of opinion expressions We present an approach for the joint (2005)), and to recognize propo-traction of entities and relations in the  consitional opinions and their sources (e.g. Bethard  text of opinion recognition and analysis.et al.(2004)) with reasonable accuracy.To date,  We identify two types of opinion-related  however, there has been no effort to  simultaneentities expressions of opinions and  ously identify arbitrary opinion expressions, their sources of opinions along with the link-sources, and the relations between them.Without  ing relation that exists between them.Inprogress on the joint extraction of opinion enti-spired by Roth and Yih (2004), we employ  ties and their relations, the capabilities of opinion-an integer linear programming approach  based applications will remain limited.to solve the joint opinion recognition task,  and show that global, constraint-based  inFortunately, research in machine learning has  ference can significantly boost the  perforproduced methods for global inference and joint  mance of both relation extraction and the  classification that can help to address this  defiextraction of opinion-related entities.Perciency (e.g. Bunescu and Mooney (2004), Roth  formance further improves when a  semanand Yih (2004)).Moreover, it has been shown that  tic role labeling system is incorporated.The resulting system achieves F-measures  lations via global inference not only solves the  of 79 and 69 for entity and relation  extracjoint extraction task, but often boosts performance tion, respectively, improving substantially  on the individual tasks when compared to  clasover prior results in the area.sifiers that handle the tasks independently for"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," ï»¿automatic Labeling of Semantic Roles Daniel Gildea Daniel Jurafsky University of California, Berkeley, and University of Colorado, Boulder International Computer Science Institute  We present a system for identifying the semantic relationships, or semantic roles , filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT , or more domain-specific semantic roles such as SPEAKER , MESSAGE , and TOPIC .  The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.  We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.  Our system achieves 82% accuracy in identifying the semantic role of pre-segmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.  Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.  1. ",1,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","We present a system for identifying the semantic relationships, or semantic roles , filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT , or more domain-specific semantic roles such as SPEAKER , MESSAGE , and TOPIC .  The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence.These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.We used various lexical clustering algorithms to generalize across possible fillers of roles.Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.Our system achieves 82% accuracy in identifying the semantic role of pre-segmented constituents.At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature-combination methods in the semantic role labeling task.We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.1."
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," ï»¿Predicting the Semantic Orientation of Adjectives  Department of Computer Science  450 Computer Science Building  Columbia University  New York, N.Y. 10027, USA  {vh, kathy) cs, columbia, edu  We identify and validate from a large pus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achiev-ing 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many jectives, a clustering algorithm separates the adjectives into groups of different tations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of tions in the corpus.  ",0,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","ï»¿Predicting the Semantic Orientation of Adjectives We identify and validate from a large pus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achiev-ing 82% accuracy in this task when each conjunction is considered independently.Combining the constraints across many jectives, a clustering algorithm separates the adjectives into groups of different tations, and finally, adjectives are labeled positive or negative.Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of tions in the corpus."
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall sentiment Terveen et al. (1997), Tatemura (2000)), where user  , e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  ",1,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","79-86.Association for Computational Linguistics.Thumbs up?Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use.Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall sentiment Terveen et al.(1997), Tatemura (2000)), where user  , e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization.Moreover, there are also  perform human-produced baselines.Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem.A challenging aspect of this categorization.We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner.For example, the"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," Extracting Product Features and Opinions from Reviews Ana-Maria Popescu and Oren Etzioni  Department of Computer Science and Engineering  University of Washington  Seattle, WA 98195-2350  {amp, etzioni}@cs.washington.edu  We decompose the problem of review mining into the  Consumers are often forced to wade  following main subtasks:  through many on-line reviews in  I. Identify product features.  order to make an informed  prodII. Identify opinions regarding product features.  uct choice.  This paper introduces  III. Determine the polarity of opinions.  OPINE, an unsupervised  informationIV. Rank opinions based on their strength.  extraction system which mines  reThis paper introduces  views in order to build a model of  imOPINE, an unsupervised  information extraction system that embodies a solution to each portant product features, their evalu-of the above subtasks.  ation by reviewers, and their relative  OPINE is built on top of the  KnowItAll Web information-extraction system (Etzioni et al., quality across products.  2005) as detailed in Section 3.  Compared to previous work, OPINE  achieves 22% higher precision (with  Given a particular product and a corresponding set of  only 3% lower recall) on the feature  reviews, OPINE solves the opinion mining tasks outlined  extraction task.  above and outputs a set of product features, each accom-OPINE's novel use of  relaxation labeling for finding the  sepanied by a list of associated opinions which are ranked mantic orientation of words in con-based on strength ( e.g. , 'abominable' is stronger than text leads to strong performance on  'bad). This output information can then be used to  genthe tasks of finding opinion phrases  erate various types of opinion summaries.  and their polarity.  This paper focuses on the first 3 review mining  subtasks and our contributions are as follows:  ",1,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","{amp, etzioni}@cs.washington.edu We decompose the problem of review mining into the Consumers are often forced to wade following main subtasks: through many on-line reviews in I. Identify product features.order to make an informed  prodII.Identify opinions regarding product features.uct choice.This paper introduces  III.Determine the polarity of opinions.OPINE, an unsupervised  informationIV.Rank opinions based on their strength.extraction system which mines  reThis paper introduces  views in order to build a model of  imOPINE, an unsupervised  information extraction system that embodies a solution to each portant product features, their evalu-of the above subtasks.ation by reviewers, and their relative  OPINE is built on top of the  KnowItAll Web information-extraction system (Etzioni et al., quality across products.  2005) as detailed in Section 3.Compared to previous work, OPINE  achieves 22% higher precision (with  Given a particular product and a corresponding set of  only 3% lower recall) on the feature  reviews, OPINE solves the opinion mining tasks outlined  extraction task.above and outputs a set of product features, each accom-OPINE's novel use of  relaxation labeling for finding the  sepanied by a list of associated opinions which are ranked mantic orientation of words in con-based on strength ( e.g. , 'abominable' is stronger than text leads to strong performance on  'bad).This output information can then be used to  genthe tasks of finding opinion phrases  erate various types of opinion summaries.  and their polarity.This paper focuses on the first 3 review mining  subtasks and our contributions are as follows:"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," ï»¿ A maximum entropy Approach  to Natural Language Processing  Columbia University Renaissance Technologies  The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.  1. ",0,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","ï»¿ A maximum entropy Approach Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition.In this paper, we describe a method for statistical modeling based on maximum entropy.We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.1."
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," In Proceedings of HLT/EMNLP 2005  Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns  Yejin Choi and Claire Cardie  Ellen Riloff and Siddharth Patwardhan  Department of Computer Science  School of Computing  Cornell University  University of Utah  Ithaca, NY 14853  Salt Lake City, UT 84112  (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al. (2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength). We pursue  anand Riloff (2005)). Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments. We view this problem as  Yi et al. (2003)), opinion-oriented summarization  an information extraction task and adopt  (e.g., Cardie et al. (2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al. (2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a). While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.  Identifying opinion sources will  toSlog learns extraction patterns. Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')  one alone. The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.  indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.  To illustrate the nature of this problem, consider the 1  ",0,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","In Proceedings of HLT/EMNLP 2005 Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns (2004)). Other research efforts analyze opinion  expressions at the sentence level or below to  recognize opinions, their polarity, and their strength (e.g., sentiment classification, opinion recogni-Dave et al.(2003), Pang and Lee (2004), Wilson et  tion, and opinion analysis (e.g.,  detecting polarity and strength).We pursue  anand Riloff (2005)).Many applications could  benother aspect of opinion analysis:  identiefit from these opinion analyzers, including  prodfying the sources of opinions, emotions,  uct reputation tracking (e.g., Morinaga et al. (2002), and sentiments.We view this problem as  Yi et al.(2003)), opinion-oriented summarization  an information extraction task and adopt  (e.g., Cardie et al.(2004)), and question answering a hybrid approach that combines  Con(e.g., Bethard et al.(2004), Yu and Hatzivassiloglou ditional Random Fields (Lafferty et al.,  2001) and a variation of AutoSlog (Riloff,  We focus here on another aspect of opinion  1996a).While CRFs model source  idenanalysis: automatically identifying the sources of  tification as a sequence tagging task,  Authe opinions.Identifying opinion sources will  toSlog learns extraction patterns.Our  rebe especially critical for opinion-oriented question-sults show that the combination of these  answering systems (e.g., systems that answer  questwo methods performs better than either  tions of the form 'How does [X] feel about [Y]?')one alone.The resulting system identifies  and opinion-oriented summarization systems, both  opinion sources with 79.3% precision and  of which need to distinguish the opinions of one  59.5% recall using a head noun matching  source from those of another.1  measure, and 81.2% precision and 60.6%  The goal of our research is to identify direct and  recall using an overlap measure.indirect sources of opinions, emotions, sentiments,  and other private states that are expressed in text.To illustrate the nature of this problem, consider the 1"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," Maximum Entropy Models for FrameNet Classification Michael Fleischman, Namhee Kwon and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  NP, PP), and their grammatical function (e.g.,  external argument, object argument). Figure 1 shows  an example of an annotated sentence and its  approThe development of FrameNet, a large  database of semantically annotated  sentences, has primed research into statistical  methods for semantic tagging. We  advance previous work by adopting a  Maximum Entropy approach and by using  Agent Body Part  previous tag information to find the  highest probability tag sequence for a given  She clapped her hands in inspiration.  sentence. Further we examine the use of  sentence level syntactic pattern features to  -Comp  increase performance. We analyze our  Figure 1. Frame for lemma 'clap' shown with three  strategy on both human annotated and  core frame elements and a sentence annotated with  eleautomatically identified frame elements,  ment type, phrase type, and grammatical function.  and compare performance to previous  As of its first release in June 2002, FrameNet  work on identical test data. Experiments  has made available 49,000 annotated sentences.  The release contains 99,000 annotated frame  eleprovement (p<0.01) of over 6%.  verbs, 339 nouns, and 175 adjectives).  While considerable in scale, the FrameNet  da1 ",1,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","external argument, object argument). Figure 1 shows  an example of an annotated sentence and its  approThe development of FrameNet, a large  database of semantically annotated  sentences, has primed research into statistical  methods for semantic tagging.We  advance previous work by adopting a  Maximum Entropy approach and by using  Agent Body Part  previous tag information to find the  highest probability tag sequence for a given  She clapped her hands in inspiration.  sentence.Further we examine the use of  sentence level syntactic pattern features to  -Comp  increase performance.We analyze our  Figure 1.Frame for lemma 'clap' shown with three  strategy on both human annotated and  core frame elements and a sentence annotated with  eleautomatically identified frame elements,  ment type, phrase type, and grammatical function.  and compare performance to previous  As of its first release in June 2002, FrameNet  work on identical test data.Experiments  has made available 49,000 annotated sentences.The release contains 99,000 annotated frame  eleprovement (p<0.01) of over 6%.verbs, 339 nouns, and 175 adjectives).While considerable in scale, the FrameNet  da1"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," Effects of Adjective Orientation and Gradability on Sentence Subjectivity  Department of Computer Science  Department of Computer Science  Columbia University  New Mexico State University  New York, NY 10027  sages ('flames') and mining online sources for product  Subjectivity is a pragmatic, sentence-level feature that reviews. Other tasks for which subjectivity recognition has important implications for text processing applica-is potentially very useful include information extraction tions such as information extraction and information re-and information retrieval. Assigning subjectivity labels trieval. We study the effects of dynamic adjectives, se-to documents or portions of documents is an example of  mantically oriented adjectives, and gradable adjectives non-topical characterization of information. Current in-on a simple subjectivity classifier, and establish that formation extraction and retrieval technology focuses al-they are strong predictors of subjectivity. A novel train-most exclusively on the subject matter of the documents.  able method that statistically combines two indicators of Yet, additional components of a document influence its  gradability is presented and evaluated, complementing  relevance to particular users or tasks, including, for ex-existing automatic techniques for assigning orientation ample, the evidential status of the material presented, and labels.  attitudes adopted in favor or against a particular person, event, or position (e.g., articles on a presidential cam-1  ",0,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","Effects of Adjective Orientation and Gradability on Sentence Subjectivity sages ('flames') and mining online sources for product Subjectivity is a pragmatic, sentence-level feature that reviews. Other tasks for which subjectivity recognition has important implications for text processing applica-is potentially very useful include information extraction tions such as information extraction and information re-and information retrieval.Assigning subjectivity labels trieval.We study the effects of dynamic adjectives, se-to documents or portions of documents is an example of  mantically oriented adjectives, and gradable adjectives non-topical characterization of information.Current in-on a simple subjectivity classifier, and establish that formation extraction and retrieval technology focuses al-they are strong predictors of subjectivity.A novel train-most exclusively on the subject matter of the documents.able method that statistically combines two indicators of Yet, additional components of a document influence its  gradability is presented and evaluated, complementing  relevance to particular users or tasks, including, for ex-existing automatic techniques for assigning orientation ample, the evidential status of the material presented, and labels.attitudes adopted in favor or against a particular person, event, or position (e.g., articles on a presidential cam-1"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," Determining the Sentiment of Opinions  Information Sciences Institute  University of Southern California  4676 Admiralty Way  Eduard Hovy  Information Sciences Institute  University of Southern California  4676 Admiralty Way  Identifying sentiments (the affective parts  of opinions) is a challenging problem. We  present a system that, given a topic,  automatically finds the people who hold  opinions about that topic and the sentiment  of each opinion. The system contains a  module for determining word sentiment  and another for combining sentiments  within a sentence. We experiment with  various models of classifying and  combining sentiment at word and sentence  levels, with promising results.  1 ",1,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","Identifying sentiments (the affective parts of opinions) is a challenging problem. We  present a system that, given a topic,  automatically finds the people who hold  opinions about that topic and the sentiment  of each opinion.The system contains a  module for determining word sentiment  and another for combining sentiments  within a sentence.We experiment with  various models of classifying and  combining sentiment at word and sentence  levels, with promising results.1"
" extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text  Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way  This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline.  "," Learning Subjective Nouns using Extraction Pattern Bootstrapping  Theresa Wilson  School of Computing  Department of Computer Science Intelligent Systems Program University of Utah  University of Pittsburgh  University of Pittsburgh  Salt Lake City, UT 84112  document summarization systems need to summarize different opinions and perspectives. Spam filtering systems We explore the idea of creating a subjectiv-must recognize rants and emotional tirades, among other ity classifier that uses lists of subjective nouns  things. In general, nearly any system that seeks to iden-learned by bootstrapping algorithms. The goal  tify information could benefit from being able to separate of our research is to develop a system that  factual and subjective information.  can distinguish subjective sentences from  obSubjective language has been previously studied in  jective sentences. First, we use two  bootstrapfields such as linguistics, literary theory, psychology, and ping algorithms that exploit extraction patterns  content analysis. Some manually-developed knowledge  to learn sets of subjective nouns.  Then we  resources exist, but there is no comprehensive dictionary train a Naive Bayes classifier using the subjec-of subjective language.  tive nouns, discourse features, and subjectivity  Meta-Bootstrapping (Riloff and Jones, 1999) and  clues identified in prior research. The  bootBasilisk (Thelen and Riloff, 2002) are bootstrapping al-strapping algorithms learned over 1000  subjecgorithms that use automatically generated extraction pattive nouns, and the subjectivity classifier  perterns to identify words belonging to a semantic cate-formed well, achieving 77% recall with 81%  We hypothesized that extraction patterns could  also identify subjective words. For example, the pattern â€œexpressed < direct object> ' often extracts subjective nouns, such as 'concern', 'hope', and 'support'.  ",0,"extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence in online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective.This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data.We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles.For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet.Our experimental results show that our system performs significantly better than the baseline.","Learning Subjective Nouns using Extraction Pattern Bootstrapping document summarization systems need to summarize different opinions and perspectives. Spam filtering systems We explore the idea of creating a subjectiv-must recognize rants and emotional tirades, among other ity classifier that uses lists of subjective nouns  things.In general, nearly any system that seeks to iden-learned by bootstrapping algorithms.The goal  tify information could benefit from being able to separate of our research is to develop a system that  factual and subjective information.can distinguish subjective sentences from  obSubjective language has been previously studied in  jective sentences.First, we use two  bootstrapfields such as linguistics, literary theory, psychology, and ping algorithms that exploit extraction patterns  content analysis.Some manually-developed knowledge  to learn sets of subjective nouns.Then we  resources exist, but there is no comprehensive dictionary train a Naive Bayes classifier using the subjec-of subjective language.tive nouns, discourse features, and subjectivity  Meta-Bootstrapping (Riloff and Jones, 1999) and  clues identified in prior research.The  bootBasilisk (Thelen and Riloff, 2002) are bootstrapping al-strapping algorithms learned over 1000  subjecgorithms that use automatically generated extraction pattive nouns, and the subjectivity classifier  perterns to identify words belonging to a semantic cate-formed well, achieving 77% recall with 81%  We hypothesized that extraction patterns could  also identify subjective words.For example, the pattern â€œexpressed < direct object> ' often extracts subjective nouns, such as 'concern', 'hope', and 'support'."
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al. (1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","79-86.Association for Computational Linguistics.Thumbs up?Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use.Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al.(1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization.Moreover, there are also  perform human-produced baselines.Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem.A challenging aspect of this categorization.We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner.For example, the"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Emotions from text: machine learning for text-based emotion prediction Cecilia Ovesdotter Alm  Dan Roth  Richard Sproat  Dept. of Linguistics  Dept. of Computer Science  Dept. of Linguistics  UIUC  UIUC  Dept. of Electrical Eng.  UIUC  study, including the machine learning model, the  corpus, the feature set, parameter tuning, etc.  SecIn addition to information, text  contion 5 presents experimental results from two  classitains attitudinal, and more specifically,  fication tasks and feature set modifications. Section  emotional content. This paper explores  6 describes the agenda for refining the model, before  the text-based emotion prediction  probpresenting concluding remarks in 7.  lem empirically, using supervised machine  learning with the SNoW learning  archiApplication area: Text-to-speech  tecture. The goal is to classify the  emotional affinity of sentences in the  narraNarrative text is often especially prone to having  tive domain of children's fairy tales, for  emotional contents. In the literary genre of fairy  tales, emotions such as HAPPINESS and ANGER and  sive rendering of text-to-speech  syntherelated cognitive states, e.g. LOVE or HATE, become  sis. Initial experiments on a preliminary  integral parts of the story plot, and thus are of  pardata set of 22 fairy tales show  encouragticular importance. Moreover, the story teller  reading results over a na ve baseline and BOW  ing the story interprets emotions in order to orally  approach for classification of emotional  convey the story in a fashion which makes the story  versus non-emotional contents, with some  come alive and catches the listeners' attention.  In speech, speakers effectively express emotions  by modifying prosody, including pitch, intensity,  which covers emotional valence, as well  and durational cues in the speech signal. Thus, in  as feature set alternations. In addition, we  order to make text-to-speech synthesis sound as  natpresent plans for a more cognitively sound  ural and engaging as possible, it is important to  convey the emotional stance in the text. However, this  tion a larger set of basic emotions.  implies first having identified the appropriate  emotional meaning of the corresponding text passage.  Thus, an application for emotional text-to-speech  ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","Dept. of Linguistics Dept. of Computer Science Dept. of Linguistics Dept. of Electrical Eng. UIUC  study, including the machine learning model, the  corpus, the feature set, parameter tuning, etc.SecIn addition to information, text  contion 5 presents experimental results from two  classitains attitudinal, and more specifically,  fication tasks and feature set modifications.Section  emotional content.This paper explores  6 describes the agenda for refining the model, before  the text-based emotion prediction  probpresenting concluding remarks in 7.lem empirically, using supervised machine  learning with the SNoW learning  archiApplication area: Text-to-speech  tecture.The goal is to classify the  emotional affinity of sentences in the  narraNarrative text is often especially prone to having  tive domain of children's fairy tales, for  emotional contents.In the literary genre of fairy  tales, emotions such as HAPPINESS and ANGER and  sive rendering of text-to-speech  syntherelated cognitive states, e.g. LOVE or HATE, become  sis.Initial experiments on a preliminary  integral parts of the story plot, and thus are of  pardata set of 22 fairy tales show  encouragticular importance.Moreover, the story teller  reading results over a na ve baseline and BOW  ing the story interprets emotions in order to orally  approach for classification of emotional  convey the story in a fashion which makes the story  versus non-emotional contents, with some  come alive and catches the listeners' attention.In speech, speakers effectively express emotions  by modifying prosody, including pitch, intensity,  which covers emotional valence, as well  and durational cues in the speech signal.Thus, in  as feature set alternations.In addition, we  order to make text-to-speech synthesis sound as  natpresent plans for a more cognitively sound  ural and engaging as possible, it is important to  convey the emotional stance in the text.However, this  tion a larger set of basic emotions.implies first having identified the appropriate  emotional meaning of the corresponding text passage.Thus, an application for emotional text-to-speech"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Mining WordNet for Fuzzy Sentiment:  Sentiment Tag Extraction from WordNet Glosses  Concordia University  {andreev, bergler}@encs.concordia.ca  representation, which regard all members of a  category as equal: no element is more of a  memMany of the tasks required for semantic  ber than any other (Edmonds, 1999). In this  patagging of phrases and texts rely on a list  per, we challenge the applicability of this  assumpof words annotated with some semantic  tion to the semantic category of sentiment, which features.  We present a method for  exconsists of positive, negative and neutral  subcatetracting sentiment-bearing adjectives from  gories, and present a dictionary-based Sentiment  WordNet using the Sentiment Tag  ExtracTag Extraction Program (STEP) that we use to  tion Program (STEP). We did 58 STEP  generate a fuzzy set of English sentiment-bearing runs on unique non-intersecting seed lists  words for the use in sentiment tagging systems 1.  drawn from manually annotated list of  The proposed approach based on the fuzzy logic  positive and negative adjectives and  evalu(Zadeh, 1987) is used here to assign fuzzy  senated the results against other manually  antiment tags to all words in WordNet (Fellbaum,  notated lists. The 58 runs were then  col1998), that is it assigns sentiment tags and a degree lapsed into a single set of 7, 813 unique  of centrality of the annotated words to the  sentiFor each word we computed a  ment category. This assignment is based on  WordNet Overlap Score by subtracting the total  Net glosses. The implications of this approach for  number of runs assigning this word a  negNLP and linguistic research are discussed.  ative sentiment from the total of the runs  that consider it positive. We demonstrate  The Category of Sentiment as a Fuzzy  that Net Overlap Score can be used as a  measure of the words degree of  memberSome semantic categories have clear membership  ship in the fuzzy category of sentiment:  (e.g., lexical fields (Lehrer, 1974) of color, body the core adjectives, which had the high-parts or professions), while others are much more  difficult to define. This prompted the development  most accurately both by STEP and by  huof approaches that regard the transition from  memman annotators, while the words on the  bership to non-membership in a semantic category  periphery of the category had the lowest  as gradual rather than abrupt (Zadeh, 1987; Rosch,  scores and were associated with low rates  1978). In this paper we approach the category of  of inter-annotator agreement.  sentiment as one of such fuzzy categories where 1  ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","Mining WordNet for Fuzzy Sentiment: representation, which regard all members of a category as equal: no element is more of a memMany of the tasks required for semantic In this  patagging of phrases and texts rely on a list  per, we challenge the applicability of this  assumpof words annotated with some semantic  tion to the semantic category of sentiment, which features.We present a method for  exconsists of positive, negative and neutral  subcatetracting sentiment-bearing adjectives from  gories, and present a dictionary-based Sentiment  WordNet using the Sentiment Tag  ExtracTag Extraction Program (STEP) that we use to  tion Program (STEP).We did 58 STEP  generate a fuzzy set of English sentiment-bearing runs on unique non-intersecting seed lists  words for the use in sentiment tagging systems 1.drawn from manually annotated list of  The proposed approach based on the fuzzy logic  positive and negative adjectives and  evalu(Zadeh, 1987) is used here to assign fuzzy  senated the results against other manually  antiment tags to all words in WordNet (Fellbaum,  notated lists.The 58 runs were then  col1998), that is it assigns sentiment tags and a degree lapsed into a single set of 7, 813 unique  of centrality of the annotated words to the  sentiFor each word we computed a  ment category.This assignment is based on  WordNet Overlap Score by subtracting the total  Net glosses.The implications of this approach for  number of runs assigning this word a  negNLP and linguistic research are discussed.ative sentiment from the total of the runs  that consider it positive.We demonstrate  The Category of Sentiment as a Fuzzy  that Net Overlap Score can be used as a  measure of the words degree of  memberSome semantic categories have clear membership  ship in the fuzzy category of sentiment:  (e.g., lexical fields (Lehrer, 1974) of color, body the core adjectives, which had the high-parts or professions), while others are much more  difficult to define.This prompted the development  most accurately both by STEP and by  huof approaches that regard the transition from  memman annotators, while the words on the  bership to non-membership in a semantic category  periphery of the category had the lowest  as gradual rather than abrupt (Zadeh, 1987; Rosch,  scores and were associated with low rates  1978).In this paper we approach the category of  of inter-annotator agreement.sentiment as one of such fuzzy categories where 1"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Jordan Boyd-Graber and Philip Resnik. Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation. Empirical Methods in Natural Language Processing, 2010, 11 pages.  Author = {Jordan Boyd-Graber and Philip Resnik},  Booktitle = {Empirical Methods in Natural Language Processing}, Location = {Cambridge, MA},  Title = {Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation},  Downloaded from http://cs.colorado.edu/~jbg/docs/jbg-mlslda-2010.pdf  Holistic Sentiment Analysis Across Languages:  Multilingual Supervised Latent Dirichlet Allocation  UMD iSchool  Department of Linguistics  and UMIACS  and UMIACS  University of Maryland  University of Maryland  College Park, MD  Up to this point, multiple languages have been  addressed in sentiment analysis primarily by  transIn this paper, we develop multilingual  superferring knowledge from a resource-rich language to  vised latent Dirichlet allocation (MLSLDA),  a probabilistic generative model that allows  a less rich language (Banea et al., 2008), or by  iginsights gleaned from one language's data to  noring differences in languages via translation into  inform how the model captures properties of  English (Denecke, 2008). These approaches are  limother languages. MLSLDA accomplishes this  ited to a view of sentiment that takes place through  by jointly modeling two aspects of text: how  an English-centric lens, and they ignore the  potenmultilingual concepts are clustered into  themattial to share information between languages.  Ideically coherent topics and how topics  associally, learning sentiment cues holistically, across  lanated with text connect to an observed  regresguages, would result in a richer and more globally  sion variable (such as ratings on a sentiment  scale). Concepts are represented in a general  hierarchical framework that is flexible enough  In this paper, we introduce Multilingual  Superto express semantic ontologies, dictionaries,  vised Latent Dirichlet Allocation (M  clustering constraints, and, as a special,  degenerate case, conventional topic models. Both  model for sentiment analysis on a multilingual  corthe topics and the regression are discovered  pus. MLSLDA discovers a consistent, unified picture  via posterior inference from corpora. We show  of sentiment across multiple languages by learning  MLSLDA can build topics that are consistent  topics, probabilistic partitions of the vocabulary  across languages, discover sensible bilingual  that are consistent in terms of both meaning and  relevance to observed sentiment. Our approach makes  gual corpora to better predict sentiment.  neither parallel corpora nor machine translation.  Sentiment analysis (Pang and Lee, 2008) offers  the promise of automatically discerning how people  The rest of the paper proceeds as follows. In  Secfeel about a product, person, organization, or issue  tion 1, we describe the probabilistic tools that we use  based on what they write online, which is potentially  to create consistent topics bridging across languages  of great value to businesses and other organizations. and the MLSLDA model. In Section 2, we present However, the vast majority of sentiment resources  the inference process. We discuss our set of  semanand algorithms are limited to a single language, tic bridges between languages in Section 3, and our ally English (Wilson, 2008; Baccianella and experiments in Section 4 demonstrate that this ap-tiani, 2010). Since no single language captures a  proach functions as an effective multilingual topic  majority of the content online, adopting such a model, discovers sentiment-biased topics, and uses ited approach in an increasingly global community  risks missing important details and trends that might  dictions across languages. Sections 5 and 6 discuss  only be available when text in multiple languages is  related research and discusses future work,  respectaken into account.  tively.  1 Predictions from Multilingual Topics  p(h ao|z) all tend to be high at the same time, or low  at the same time. More generally, the structure of our  As its name suggests, MLSLDA is an extension of  model must encourage topics to be consistent across  Latent Dirichlet allocation (LDA) (Blei et al., 2003), languages, and Dirichlet distributions cannot encode a modeling approach that takes a corpus of correlations between elements.  notated documents as input and produces two  outOne possible solution to this problem is to use the  puts, a set of topics and assignments of documents  multivariate normal distribution, which can produce  to topics. Both the topics and the assignments are  correlated multinomials (Blei and Lafferty, 2005),  probabilistic: a topic is represented as a probability  in place of the Dirichlet distribution. This has been  distribution over words in the corpus, and each done successfully in multilingual settings (Cohen ument is assigned a probability distribution over all  and Smith, 2009). However, such models complicate  the topics. Topic models built on the foundations of  inference by not being conjugate.  LDA are appealing for sentiment analysis because  Instead, we appeal to tree-based extensions of the  the learned topics can cluster together Dirichlet distribution, which has been used to induce bearing words, and because topic distributions are a  correlation in semantic ontologies (Boyd-Graber et  parsimonious way to represent a document.1  al., 2007) and to encode clustering constraints  (AnLDA has been used to discover latent structure  drzejewski et al., 2009). The key idea in this  approach is to assume the vocabularies of all languages  al., 2006) and authorship (Rosen-Zvi et al., 2004)). are organized according to some shared semantic MLSLDA extends the approach by ensuring that this  structure that can be represented as a tree. For  conlatent structure the underlying topics is creteness in this section, we will use WordNet (Miller, tent across languages. We discuss multilingual topic  1990) as the representation of this multilingual  semodeling in Section 1.1, and in Section 1.2 we show  mantic bridge, since it is well known, offers  convehow this enables supervised regression regardless of  nient and intuitive terminology, and demonstrates the  full flexibility of our approach. However, the model  1.1 Capturing Semantic Correlations  we describe generalizes to any tree-structured  representation of multilingual knowledge; we discuss  Topic models posit a straightforward generative some alternatives in Section 3.  cess that creates an observed corpus. For each  docuWordNet organizes a vocabulary into a rooted,  diment d, some distribution d over unobserved topics  rected acyclic graph of nodes called synsets, short for  is chosen. Then, for each word position in the synonym sets. A synset is a child of another synset ument, a topic z is selected. Finally, the word for  if it satisfies a hyponomy relationship; each child is  that position is generated by selecting from the topic  a more specific instantiation of its parent concept  indexed by z. (Recall that in LDA, a topic is a  (thus, hyponomy is often called an isa relationship).  distribution over words).  For example, a dog is a canine is an animal is  In monolingual topic models, the topic distribution  a living thing, etc. As an approximation, it is not  is usually drawn from a Dirichlet distribution. unreasonable to assume that WordNet's structure of ing Dirichlet distributions makes it easy to specify  meaning is language independent, i.e. the concept  sparse priors, and it also simplifies posterior encoded by a synset can be realized using terms in ence because Dirichlet distributions are conjugate  different languages that share the same meaning. In  to multinomial distributions. However, drawing practice, this organization has been used to create ics from Dirichlet distributions will not suffice if  many alignments of international WordNets to the  our vocabulary includes multiple languages. If we  original English WordNet (Ordan and Wintner, 2007;  are working with English, German, and Chinese at  the same time, a Dirichlet prior has no way to  faUsing the structure of WordNet, we can now  devor distributions z such that p(good|z), p(gut|z), and scribe a generative process that produces a distribu-1The latter property has also made LDA popular for  infortion over a multilingual vocabulary, which  encourmation retrieval (Wei and Croft, 2006)).  ages correlations between words with similar  meanings regardless of what language each word is in. Topic 1 is about baseball in English and about travel For each synset h, we create a multilingual word  in German). Separating path from emission helps  distribution for that synset as follows:  ensure that topics are consistent across languages.  1. Draw transition probabilities  Having defined topic distributions in a way that can  2. Draw stop probabilities  preserve cross-language correspondences, we now  3. For each language l, draw emission probabilities for  use this distribution within a larger model that can  that synset  discover cross-language patterns of use that predict  For conciseness in the rest of the paper, we will refer  to this generative process as multilingual Dirichlet  1.2 The MLSLDA Model  hierarchy, or MULTDIRHIER( , , ).2 Each  observed token can be viewed as the end result of a  We will view sentiment analysis as a regression  probsequence of visited synsets  lem: given an input document, we want to predict  . At each node in the  tree, the path can end at node  a real-valued observation y that represents the  sentii with probability i,1,  or it can continue to a child synset with probability  ment of a document. Specifically, we build on  supervised latent Dirichlet allocation (SLDA, (Blei and  i,0. If the path continues to another child synset, it  visits child  McAuliffe, 2007)), which makes predictions based  j with probability i,j. If the path ends at  on the topics expressed in a document; this can be  k with probability i,l,k.3  The probability of a word being emitted from a path  thought of projecting the words in a document to low  with visited synsets  dimensional space of dimension equal to the number  r and final synset h in language  of topics. Blei et al. showed that using this latent  l is therefore  topic structure can offer improved predictions over  regressions based on words alone, and the approach fits  well with our current goals, since word-level cues are  unlikely to be identical across languages. In addition  to text, SLDA has been successfully applied to other  domains such as social networks (Chang and Blei,  Note that the stop probability h is independent of  2009) and image classification (Wang et al., 2009).  language, but the emission h,l is dependent on the  The key innovation in this paper is to extend SLDA  language. This is done to prevent the following by creating topics that are globally consistent across nario: while synset A is highly probable in a topic  languages, using the bridging approach above.  and words in language 1 attached to that synset have  We express our model in the form of a  probabilishigh probability, words in language 2 have low tic generative latent-variable model that generates ability. If this could happen for many synsets in  documents in multiple languages and assigns a  reala topic, an entire language would be effectively valued score to each document. The score comes lenced, which would lead to inconsistent topics (e.g. from a normal distribution whose sum is the dot prod-2Variables  uct between a regression parameter that encodes  h, h,l, and h are hyperparameters. Their mean  is fixed, but their magnitude is sampled during inference (i.e.  the influence of each topic on the observation and  is constant, but h,i is not). For the bushier bridges,  a variance 2. With this model in hand, we use  sta(e.g. dictionary and flat), their mean is uniform. For GermaNet, tistical inference to determine the distribution over  we took frequencies from two balanced corpora of German and  English: the British National Corpus (University of Oxford,  latent variables that, given the model, best explains  2006) and the Kern Corpus of the Digitales W rterbuch der  The generative model is as follows:  We took these frequencies and propagated them through the  multilingual hierarchy, following LDAWN's (Boyd-Graber et  1. For each topic i = 1 . . . K, draw a topic distribution  al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. The variance of the priors was initialized to be  1.0, but could be sampled during inference.  2. For each document d = 1 . . . M with language ld:  3Note that the language and word are taken as given, but the  (a) Choose a distribution over topics d  path through the semantic hierarchy is a latent random variable.  (b) For each word in the document n = 1 . . . Nd, choose a topic assignment  and a path d,n ending at word wd,n according  to Equation 1 using { z ,  3. Choose a response variable from y  z, 2 , where  Crucially, note that the topics are not  independent of the sentiment task; the regression encourages  terms with similar effects on the observation y to  be in the same topic. The consistency of topics  described above allows the same regression to be done  for the entire corpus regardless of the language of the  Sentiment Prediction  Figure 1: Graphical model representing MLSLDA.  2 Inference  Shaded nodes represent observations, plates denote  replication, and lines show probabilistic dependencies.  Finding the model parameters most likely to explain  the data is a problem of statistical inference. We  employ stochastic EM (Diebolt and Ip, 1996), using a  probability of taking a path r is then  Gibbs sampler for the E-step to assign words to paths  and topics. After randomly initializing the topics,  we alternate between sampling the topic and path  of a word (z  d,n, d,n) and finding the regression  parameters that maximize the likelihood. We jointly  sample the topic and path conditioning on all of the  other path and document assignments in the corpus,  Transition  selecting a path and topic with probability  Each of these three terms reflects a different influence  Equation 3 reflects the multilingual aspect of this  on the topics from the vocabulary structure, the model.  The conditional topic distribution for  ument's topics, and the response variable. In the next  SLDA (Blei and McAuliffe, 2007) replaces this term  paragraphs, we will expand each of them to derive  with the standard Multinomial-Dirichlet. However,  the full conditional topic distribution.  we believe this is the first published SLDA-style  As discussed in Section 1.1, the structure of the  model using MCMC inference, as prior work has  topic distribution encourages terms with the same  used variational inference (Blei and McAuliffe, 2007;  meaning to be in the same topic, even across Chang and Blei, 2009; Wang et al., 2009).  guages. During inference, we marginalize over  posBecause the observed response variable depends  sible multinomial distributions , , and , using  on the topic assignments of a document, the  condithe observed transitions from i to j in topic k; Tk,i,j, tional topic distribution is shifted toward topics that stop counts in synset i in topic k, Ok,i,0; continue  explain the observed response. Topics that move the  counts in synsets i in topic k, Ok,i,1; and emission  predicted response yd toward the true yd will be  facounts in synset i in language l in topic k, Fk,i,l. The  vored. We drop terms that are constant across all  topics for the effect of the response variable,  3 Bridges Across Languages  In Section 1.1, we described connections across  languages as offered by semantic networks in a general  way, using WordNet as an example. In this section,  we provide more specifics, as well as alternative ways  Other words' influence  of building semantic connections across languages.  Flat First, we can consider a degenerate mapping  that is nearly equivalent to running SLDA  independently across multiple languages, relating topics only  This word's influence  based on the impact on the response variable.  ConThe above equation represents the supervised aspect  sider a degenerate tree with only one node, with all  of the model, which is inherited from SLDA.  words in all languages associated with that node. This  Finally, there is the effect of the topics already  is consistent with our model, but there is really no  assigned to a document; the conditional distribution  shared semantic space, as all emitted words must  favors topics already assigned in a document,  come from this degenerate synset and the model  only represents the output distribution for this single  WordNet We took the alignment of GermaNet to  This term represents the document focus of this  moved all synsets that were had no mapped German  model; it is present in all Gibbs sampling inference  words. Any German synsets that did not have English  schemes for LDA (Griffiths and Steyvers, 2004).  translations had their words mapped to the lowest  Multiplying together Equations 3, 4, and 5 allows  extant English hypernym (e.g. beinbruch, a  brous to sample a topic using the conditional distribution  ken leg, was mapped to fracture ). We stemmed  from Equation 2, based on the topic and path of the  all words to account for inflected forms not being  other words in all languages. After sampling the  present (Porter and Boulton, 1970). An example  path and topic for each word in a document, we then  of the paths for the German word wunsch (wish,  find new regression parameters that maximize the  request) is shown in Figure 2(a).  likelihood conditioned on the current state of the  sampler. This is simply a least squares regression  Dictionaries A dictionary can be viewed as a many  using the topic assignments  to many mapping, where each entry  or more words in one language  Prediction on documents for which we don't have  an observed  i in another language. Entries were taken  d is equivalent to marginalizing over  from an English-German dictionary (Richter, 2008)  yd and sampling topics for the document from  Equaa Chinese-English dictionary (Denisowski, 1997),  tions 3 and 5. The prediction for yd is then the dot  and a Chinese-German dictionary (Hefti, 2005). As  product of and the empirical topic distribution zd.  with WordNet, the words in entries for English and  We initially optimized all hyperparameters using  German were stemmed to improve coverage. An  slice sampling. However, we found that the example for German is shown in Figure 2(b).  sion variance 2 was not stable. Optimizing 2 seems  to balance between modeling the language in the Algorithmic Connections In addition to hand-uments and the prediction, and thus is sensitive to  curated connections across languages, one could also  documents' length. Given this sensitivity, we did  consider automatic means of mapping across  lannot optimize 2 for our prediction experiments in  guages, such as using edit distance or local  conSection 4, but instead kept it fixed at 0.25. We leave  text (Haghighi et al., 2008; Rapp, 1995) or  usoptimizing this variable, either through cross ing a lexical translation table obtained from paral-tion or adapting the model, to future work.  lel text (Melamed, 1998). While we experimented  abstraction.n.06  option.n.02  option  option  act  speech_act.n.01  wish.n.04  wish  (a) GermaNet  (b) Dictionary  Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word  wunsch in GermaNet are shown. On the right, paths to the English word room are shown. Both English and German words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language expression.  with these techniques, constructing appropriate (as there is no associated response variable for Eu-archies from these resources required many arbitrary  roparl documents); this experiment is to demonstrate  decisions about cutoffs and which words to include. the effectiveness of the multilingual aspect of the Thus, we do not consider them in this paper.  model. To test whether the topics learned by the  each document using the probability distribution d  over topic assignments. Each  LSLDA on three criteria: how well  d is a vector of length  it can discover consistent topics across languages  K and is a language-independent representation of  the document.  for matching parallel documents, how well it can  discover sentiment-correlated word lists from  nonFor each document in one language, we computed  aligned text, and how well it can predict sentiment.  the Hellinger distance between it and all of the  documents in the other language and sorted the documents  4.1 Matching on Multilingual Topics  by decreasing distance. The translation of the  document is somewhere in that set; the higher the  normalWe took the 1996 documents from the Europarl ized rank (the percentage of documents with a rank pus (Koehn, 2005) using three bridges: GermaNet, lower than the translation of the document), the better dictionary, and the uninformative flat matching.4 The  the underlying topic model connects languages.  model is unaware that the translations of documents  in one language are present in the other language.  We compare three bridges against what is to our  Note that this does not use the supervised framework  knowledge the only other topic model for unaligned  text, Multilingual Topics for Unaligned Text  (Boyd4For English and German documents in all experiments,  we removed stop words (Loper and Bird, 2002), stemmed  words (Porter and Boulton, 1970), and created a vocabulary  5The bipartite matching was initialized with the dictionary  of the most frequent 5000 words per language (this vocabulary  weights as specified by the Multilingual Topics for Unaligned  limit was mostly done to ensure that the dictionary-based bridge Text algorithm. The matching size was limited to 250 and the  was of manageable size). Documents shorter than fifty content  bipartite matching was only updated on the initial iteration then words were excluded.  held fixed. This yielded results comparable to when the matching  a WordNet-like resource is used as the bridge, the Dictionary  resulting topics are distributions over synsets, not just  As our demonstration corpus, we used the Amherst  Dictionary  Sentiment Corpus (Constant et al., 2009), as it has  documents in multiple languages (English, Chinese,  and German) with numerical assessments of  sentiment (number of stars assigned to the review). We  Dictionary  segmented the Chinese text (Tseng et al., 2005) and  Bridge  used a classifier trained on character n-grams to  remove English-language documents that were mixed  Dictionary  in among the Chinese and German language reviews.  Figure 4 shows extracted topics from  GermanEnglish and German-Chinese corpora. M  is able to distinguish sentiment-bearing topics from  content bearing topics. For example; in the  GermanEnglish corpus, food and children topics are  not associated with a consistent sentiment signal,  Figure 3: Average rank of paired translation document  while religion is associated with a more negative  recovered from the multilingual topic model. Random  sentiment. In contrast, in the German-Chinese  corguessing would yield 0.5; MLSLDA with a dictionary  pus, the religion/society topic is more neutral, and  based matching performed best.  the gender-oriented topic is viewed more negatively.  Negative sentiment-bearing topics have reasonable  Figure 3 shows the results of this experiment. The  words such as pages, k ong p (Chinese for I'm  dictionary-based bridge had the best performance on  afraid that . . . ) and tuo (Chienese for discard ),  the task, ranking a large proportion of documents  and positive sentiment-bearing topics have  reason(0.95) below the translated document once enough  able words such as great, good, and juwel  (Gertopics were available. Although GermaNet is richer, man for jewel ).  its coverage is incomplete; the dictionary structure  The qualitative topics also betray some of the  had a much larger vocabulary and could build a more  weaknesses of the model. For example, in one of  complete multilingual topics. Using comparable the negative sentiment topics, the German word gut  put information, this more flexible model performed  (good) is present. Because topics are distributions  better on the matching task than the existing over words, they can encode the presence of nega-lingual topic model available for unaligned text. The  tions like kein (no) and nicht (not), but not  collodegenerate flat bridge did no better than the baseline  cations like nicht gut. More elaborate topic models  of random guessing, as expected.  that can model local syntax and collocations  (Johnson, 2010) provide options for addressing such  prob4.2 Qualitative Sentiment-Correlated Topics  One of the key tasks in sentiment analysis has been  We do not report the results for sentiment  predicthe collection of lists of words that convey tion for this corpus because the baseline of predicting ment (Wilson, 2008; Riloff et al., 2003). These  a positive review is so strong; most algorithms do  exresources are often created using or in reference  tremely well by always predicting a positive review,  to resources like WordNet (Whitelaw et al., 2005;  ours included.  Baccianella and Sebastiani, 2010). MLSLDA 4.3 Sentiment Prediction vides a method for extracting topical and sentiment-correlated word lists from multilingual corpora. If  We gathered 330 film reviews from a German film  review site (Vetter et al., 2000) and combined them  was updated more frequently.  with a much larger English film review corpus of over  (harry) harry (harry)  (lord) herr  (both)  (universe) all (both)  (community)  life art  healthy rezepte  people thema  ([I'm afraid that...])  (that) dass (both)  (story) story (treasure)  (mostly)  (female) (attractive) attraktiv (both)  (good) gut ([really isn't])  books geschichte parents baby  separate story  (soon) bald (both)  (a) German / English  (b) German / Chinese  Figure 4: Topics, along with associated regression coefficient from a learned 25-topic model on German-English (left) and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with good, great, h ao (good) and  berzeugt (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of gut  (good) in one of the negative sentiment topics, showing the difficulty of learning collocations.  Train  Test GermaNet Dictionary Flat  One would expect that prediction improves with a  larger training set. For this model, such an  improveTable 1: Mean squared error on a film review corpus.  ment is seen even when the training set includes no  All results are on the same German test data, varying the  documents in the target language. Note that even the  training data. Over-fitting prevents the model learning on  the German data alone; adding English data to the mix  ful information. After introducing English data, the  allows the model to make better predictions.  model learns to prefer smaller regression parameters  (this can be seen as a form of regularization).  5000 film reviews (Pang and Lee, 2005) to create a  multilingual film review corpus.6  Performance is best when a reasonably large  corThe results for predicting sentiment in German  pus is available including some data in the target  documents with 25 topics are presented in Table 1. language. For each bridge, performance improves On a small monolingual corpus, prediction is very  dramatically, showing that MLSLDA is successfully  poor. The model over-fits, especially when it has  able to incorporate information learned from both  the entire vocabulary to select from. The slightly  languages to build a single, coherent picture of how  better performance using GermaNet and a dictionary  sentiment is expressed in both languages. With the  as topic priors can be viewed as basic feature GermaNet bridge, performance is better than both tion, removing proper names from the vocabulary to  the degenerate and dictionary based bridges, showing  6We followed Pang and Lee's method for creating a  nuthat the model is sharing information both through  merical score between 0 and 1 from a star rating.  the multilingual topics and the regression parameters.  then converted that to an integer by multiplying by 100;  Performance on English prediction is comparable  this was done because initial data preprocessing assumed  to previously published results on this dataset (Blei  integer values (although downstream processing did not  asand McAuliffe, 2007); with enough data, a  monolinsume integer values). The German movie review corpus  is available at http://www.umiacs.umd.edu/  gual model is no longer helped by adding additional  static/downloads_and_media.html  5 Relationship to Previous Research  learned from a corpus, as a prior to seed topics so  that they attract other sentiment bearing words (Mei  The advantages of MLSLDA reside largely in the  et al., 2007; Lin and He, 2009). Other approaches  assumptions that it makes and does not make: view sentiment or perspective as a perturbation of ments need not be parallel, sentiment is a normally  a log-linear topic model (Lin et al., 2008). Such  distributed document-level property, words are techniques could be combined with the multilingual changeable, and sentiment can be predicted as a approach presented here by using distributions over gression on a K-dimensional vector.  words that not only bridge different languages but  By not assuming parallel text, this approach can  also encode additional information. For example, the  be applied to a broad class of corpora. Other vocabulary hierarchies could be structured to encour-tilingual topic models require parallel text, either at  age topics that encourage correlation among similar  sentiment-bearing words (e.g. clustering words  assoor word-level (Kim and Khudanpur, 2004; Zhao and  ciated with price, size, etc.). Future work could also  Xing, 2006). Similarly, other multilingual sentiment  more rigorously validate that the multilingual topics  approaches also require parallel text, often supplied  discovered by MLSLDA are sentiment-bearing via  via automatic translation; after the translated text  is available, either monolingual analysis (Denecke,  In contrast, MLSLDA draws on techniques that  2008) or co-training is applied (Wan, 2009). In view sentiment as a regression problem based on the trast, our approach requires fewer resources for a topics used in a document, as in supervised latent guage: a dictionary (or similar knowledge structure  relating words to nodes in a graph) and comparable  2007) or in finer-grained parts of a document (Titov  text, instead of parallel text or a machine translation  and McDonald, 2008). Extending these models to  multilingual data would be more straightforward.  Rather than viewing one language through the  lens of another language, MLSLDA views all 6 Conclusions  guages through the lens of the topics present in a  document. This is a modeling decision with pros and  MLSLDA is a holistic statistical model for  multicons. It allows a language agnostic decision about  sentiment to be made, but it restricts the or expensive multilingual resources. It discovers ness of the model in terms of sentiment in two ways. connections across languages that can recover la-First, it throws away information important to tent structure in parallel corpora, discover sentiment-timent analysis like syntactic constructions (Greene  correlated word lists in multiple languages, and make  and Resnik, 2009) and document structure accurate predictions across languages that improve ald et al., 2007) that may impact the sentiment rating. with more multilingual data, as demonstrated in the Second, a single real number is not always sufficient  context of sentiment analysis.  to capture the nuances of sentiment. Less critically,  More generally, MLSLDA provides a formalism  assuming that sentiment is normally distributed is not  that can be used to incorporate the many insights of  true of all real-world corpora; review corpora often  topic modeling-driven sentiment analysis to  multihave a skew toward positive reviews. We standardize  lingual corpora by tying together word distributions  responses by the mean and variance of the training  across languages. MLSLDA can also contribute to  data to partially address this issue, but other response  the development of word list-based sentiment  sysdistributions are possible, such as generalized linear  tems: the topics discovered by MLSLDA can serve  models (Blei and McAuliffe, 2007) and vector as a first-pass means of sentiment-based word lists chines (Zhu et al., 2009), which would allow more  for languages that might lack annotated resources.  traditional classification predictions.  MLSLDA also can be viewed as a  sentimentOther probabilistic models for sentiment informed multilingual word sense disambiguation cation view sentiment as a word level feature. Some  (WSD) algorithm. When the multilingual bridge is an  models use sentiment word lists, either given or  explicit representation of sense such as WordNet, part  of the generative process is an explicit assignment Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.  of every word to sense (the path latent variable );  2007. A topic model for word sense disambiguation.  this is discovered during inference. The  dictionaryIn EMNLP.  based technique may be viewed as a disambiguation  Jonathan Chang and David M. Blei. 2009. Relational  via a transfer dictionary. How sentiment prediction  topic models for document networks. In AISTATS.  impacts the implicit WSD is left to future work.  Shay B. Cohen and Noah A. Smith. 2009. Shared  logistic normal distributions for soft parameter tying in  Better capturing local syntax and meaningful  colunsupervised grammar induction. In NAACL.  locations would also improve the model's ability to  Noah Constant, Christopher Davis, Christopher Potts, and  predict sentiment and model multilingual topics, as  Florian Schwarz. 2009. The pragmatics of expressive  would providing a better mechanism for  representcontent: Evidence from large corpora. Sprache und  ing words not included in our bridges. We intend to  develop such models as future work.  gual sentiment analysis. In ICDEW 2008.  7 Acknowledgments  CEDICT.  This research was funded in part by the Army Jean Diebolt and Eddie H.S. Ip, 1996. Markov Chain search Laboratory through ARL Cooperative Agree-Monte Carlo in Practice, chapter Stochastic EM:  ment W911NF-09-2-0072 and by the Office of the  method and application. Chapman and Hall, London.  Director of National Intelligence (ODNI), Alexander Geyken. 2007. The DWDS corpus: A ref-gence Advanced Research Projects Activity (IARPA),  erence corpus for the German language of the 20th  through the Army Research Laboratory. All  statecentury. In Idioms and Collocations: Corpus-based  Linguistic, Lexicographic Studies. Continuum Press.  ments of fact, opinion or conclusions contained  Stephan Greene and Philip Resnik. 2009. More than  herein are those of the authors and should not be  words: Syntactic packaging and implicit sentiment. In  construed as representing the official views or  policies of ARL, IARPA, the ODNI, or the U.S. Thomas L. Griffiths and Mark Steyvers. 2004. Finding ment. The authors thank the anonymous reviewers,  scientific topics. PNAS, 101(Suppl 1):5228 5235.  Jonathan Chang, Christiane Fellbaum, and Lawrence  Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and  Watts for helpful comments. The authors especially  Dan Klein. 2008. Learning bilingual lexicons from  thank Chris Potts for providing help in obtaining and  monolingual corpora. In ACL, Columbus, Ohio.  processing reviews.  the Japanese WordNet. In LREC.  Mark Johnson. 2010. PCFGs, topic models, adaptor  grammars and learning topical collocations and the  2009. Incorporating domain knowledge into topic  modstructure of proper names. In ACL.  eling via Dirichlet forest priors. In ICML.  triggers and latent semantic analysis for cross-lingual  2010. Sentiwordnet 3.0: An enhanced lexical resource  for sentiment analysis and opinion mining. In LREC.  Philipp Koehn. 2005. Europarl: A parallel corpus  Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer  for statistical machine translation. In MT Summit.  Hassan. 2008. Multilingual subjectivity analysis using  machine translation. In EMNLP.  David M. Blei and John D. Lafferty. 2005. Correlated  ing WordNets in a web-compliant format: The case of  topic models. In NIPS.  GermaNet. In Workshop on Wordnets Structures and  David M. Blei and Jon D. McAuliffe. 2007. Supervised  topic models. In NIPS. MIT Press.  Chenghua Lin and Yulan He. 2009. Joint sentiment/topic  model for sentiment analysis. In CIKM.  Latent Dirichlet allocation. JMLR, 3:993 1022.  2008. A joint topic and perspective model for  ideoEdward Loper and Steven Bird. 2002. NLTK: the natu-Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel  Jural language toolkit. In Tools and methodologies for  rafsky, and Christopher Manning. 2005. A conditional  teaching. ACL.  random field word segmenter. In SIGHAN Workshop  on Chinese Language Processing.  Wells, and Jeff Reynar. 2007. Structured models for  University of Oxford.  British  Nafine-to-coarse sentiment analysis. In ACL.  tional  Corpus.  Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and  ChengXiang Zhai. 2007. Topic sentiment mixture:  Tobias Vetter, Manfred Sauer, and Philipp Wallutat.  modeling facets and opinions in weblogs. In WWW.  Ilya Dan Melamed. 1998. Empirical methods for  exploiting parallel texts. Ph.D. thesis, University of  PennsylXiaojun Wan. 2009. Co-training for cross-lingual  sentiment classification. In ACL.  George A. Miller. 1990. Nouns in WordNet: A lexical  inheritance system. International Journal of  Lexicogneous image classification and annotation. In CVPR.  raphy, 3(4):245 264.  Xing Wei and Bruce Croft. 2006. LDA-based document  Smith, and Andrew McCallum. 2009. Polylingual  Casey Whitelaw, Navendu Garg, and Shlomo Argamon.  2005. Using appraisal groups for sentiment analysis.  In CIKM.  2009. Mining multilingual topics from Wikipedia. In  Theresa Ann Wilson. 2008. Fine-grained Subjectivity and  Sentiment Analysis: Recognizing the Intensity, Polarity,  Noam Ordan and Shuly Wintner. 2007. Hebrew  Wordand Attitudes of Private States. Ph.D. thesis, University  Net: a test case of aligning lexical databases across  lanof Pittsburgh.  guages. International Journal of Translation, 19(1):39  topic admixture models for word alignment. In ACL.  Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting  Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda:  class relationships for sentiment categorization with  maximum margin supervised topic models for  regresrespect to rating scales. In ACL.  sion and classification. In ICML.  Sentiment Analysis. Now Publishers Inc.  Martin Porter and Richard Boulton. 1970. Snowball  Matthew Purver, Konrad K rding, Thomas L. Griffiths,  and Joshua Tenenbaum. 2006. Unsupervised topic  modelling for multi-party spoken discourse. In ACL.  Reinhard Rapp. 1995. Identifying word translations in  Philip Resnik. 1995. Using information content to  evaluate semantic similarity in a taxonomy. In IJCAI, pages  Frank Richter. 2008. Dictionary nice grep.  http://wwwEllen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.  Learning subjective nouns using extraction pattern  bootMichal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,  and Padhraic Smyth. 2004. The author-topic model for  authors and documents. In UAI.  Beno t Sagot and Darja Fi er. 2008. Building a Free  French WordNet from Multilingual Resources. In  OntoLex.  Ivan Titov and Ryan McDonald. 2008. A joint model of  text and aspect ratings for sentiment summarization. In ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation.Empirical Methods in Natural Language Processing, 2010, 11 pages.Author = {Jordan Boyd-Graber and Philip Resnik},  Booktitle = {Empirical Methods in Natural Language Processing}, Location = {Cambridge, MA},  Title = {Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation},  Downloaded from http://cs.colorado.edu/~jbg/docs/jbg-mlslda-2010.pdf  Holistic Sentiment Analysis Across Languages:  Multilingual Supervised Latent Dirichlet Allocation  UMD iSchool  Department of Linguistics  and UMIACS  and UMIACS  University of Maryland  University of Maryland  College Park, MD  Up to this point, multiple languages have been  addressed in sentiment analysis primarily by  transIn this paper, we develop multilingual  superferring knowledge from a resource-rich language to  vised latent Dirichlet allocation (MLSLDA),  a probabilistic generative model that allows  a less rich language (Banea et al., 2008), or by  iginsights gleaned from one language's data to  noring differences in languages via translation into  inform how the model captures properties of  English (Denecke, 2008).These approaches are  limother languages.MLSLDA accomplishes this  ited to a view of sentiment that takes place through  by jointly modeling two aspects of text: how  an English-centric lens, and they ignore the  potenmultilingual concepts are clustered into  themattial to share information between languages.Ideically coherent topics and how topics  associally, learning sentiment cues holistically, across  lanated with text connect to an observed  regresguages, would result in a richer and more globally  sion variable (such as ratings on a sentiment  scale).Concepts are represented in a general  hierarchical framework that is flexible enough  In this paper, we introduce Multilingual  Superto express semantic ontologies, dictionaries,  vised Latent Dirichlet Allocation (M  clustering constraints, and, as a special,  degenerate case, conventional topic models.Both  model for sentiment analysis on a multilingual  corthe topics and the regression are discovered  pus.MLSLDA discovers a consistent, unified picture  via posterior inference from corpora.We show  of sentiment across multiple languages by learning  MLSLDA can build topics that are consistent  topics, probabilistic partitions of the vocabulary  across languages, discover sensible bilingual  that are consistent in terms of both meaning and  relevance to observed sentiment.Our approach makes  gual corpora to better predict sentiment.neither parallel corpora nor machine translation.Sentiment analysis (Pang and Lee, 2008) offers  the promise of automatically discerning how people  The rest of the paper proceeds as follows.In  Secfeel about a product, person, organization, or issue  tion 1, we describe the probabilistic tools that we use  based on what they write online, which is potentially  to create consistent topics bridging across languages  of great value to businesses and other organizations. and the MLSLDA model.In Section 2, we present However, the vast majority of sentiment resources  the inference process.We discuss our set of  semanand algorithms are limited to a single language, tic bridges between languages in Section 3, and our ally English (Wilson, 2008; Baccianella and experiments in Section 4 demonstrate that this ap-tiani, 2010).Since no single language captures a  proach functions as an effective multilingual topic  majority of the content online, adopting such a model, discovers sentiment-biased topics, and uses ited approach in an increasingly global community  risks missing important details and trends that might  dictions across languages.Sections 5 and 6 discuss  only be available when text in multiple languages is  related research and discusses future work,  respectaken into account.  tively.1 Predictions from Multilingual Topics  p(h ao|z) all tend to be high at the same time, or low  at the same time.More generally, the structure of our  As its name suggests, MLSLDA is an extension of  model must encourage topics to be consistent across  Latent Dirichlet allocation (LDA) (Blei et al., 2003), languages, and Dirichlet distributions cannot encode a modeling approach that takes a corpus of correlations between elements.notated documents as input and produces two  outOne possible solution to this problem is to use the  puts, a set of topics and assignments of documents  multivariate normal distribution, which can produce  to topics.Both the topics and the assignments are  correlated multinomials (Blei and Lafferty, 2005),  probabilistic: a topic is represented as a probability  in place of the Dirichlet distribution.This has been  distribution over words in the corpus, and each done successfully in multilingual settings (Cohen ument is assigned a probability distribution over all  and Smith, 2009).However, such models complicate  the topics.Topic models built on the foundations of  inference by not being conjugate.LDA are appealing for sentiment analysis because  Instead, we appeal to tree-based extensions of the  the learned topics can cluster together Dirichlet distribution, which has been used to induce bearing words, and because topic distributions are a  correlation in semantic ontologies (Boyd-Graber et  parsimonious way to represent a document.1  al., 2007) and to encode clustering constraints  (AnLDA has been used to discover latent structure  drzejewski et al., 2009).The key idea in this  approach is to assume the vocabularies of all languages  al., 2006) and authorship (Rosen-Zvi et al., 2004)). are organized according to some shared semantic MLSLDA extends the approach by ensuring that this  structure that can be represented as a tree.For  conlatent structure the underlying topics is creteness in this section, we will use WordNet (Miller, tent across languages.We discuss multilingual topic  1990) as the representation of this multilingual  semodeling in Section 1.1, and in Section 1.2 we show  mantic bridge, since it is well known, offers  convehow this enables supervised regression regardless of  nient and intuitive terminology, and demonstrates the  full flexibility of our approach.However, the model  1.1 Capturing Semantic Correlations  we describe generalizes to any tree-structured  representation of multilingual knowledge; we discuss  Topic models posit a straightforward generative some alternatives in Section 3.cess that creates an observed corpus.For each  docuWordNet organizes a vocabulary into a rooted,  diment d, some distribution d over unobserved topics  rected acyclic graph of nodes called synsets, short for  is chosen.Then, for each word position in the synonym sets.A synset is a child of another synset ument, a topic z is selected.Finally, the word for  if it satisfies a hyponomy relationship; each child is  that position is generated by selecting from the topic  a more specific instantiation of its parent concept  indexed by z.(Recall that in LDA, a topic is a  (thus, hyponomy is often called an isa relationship).  distribution over words).For example, a dog is a canine is an animal is  In monolingual topic models, the topic distribution  a living thing, etc.As an approximation, it is not  is usually drawn from a Dirichlet distribution.unreasonable to assume that WordNet's structure of ing Dirichlet distributions makes it easy to specify  meaning is language independent, i.e. the concept  sparse priors, and it also simplifies posterior encoded by a synset can be realized using terms in ence because Dirichlet distributions are conjugate  different languages that share the same meaning.In  to multinomial distributions.However, drawing practice, this organization has been used to create ics from Dirichlet distributions will not suffice if  many alignments of international WordNets to the  our vocabulary includes multiple languages.If we  original English WordNet (Ordan and Wintner, 2007;  are working with English, German, and Chinese at  the same time, a Dirichlet prior has no way to  faUsing the structure of WordNet, we can now  devor distributions z such that p(good|z), p(gut|z), and scribe a generative process that produces a distribu-1The latter property has also made LDA popular for  infortion over a multilingual vocabulary, which  encourmation retrieval (Wei and Croft, 2006)).ages correlations between words with similar  meanings regardless of what language each word is in.Topic 1 is about baseball in English and about travel For each synset h, we create a multilingual word  in German).Separating path from emission helps  distribution for that synset as follows:  ensure that topics are consistent across languages.1. Draw transition probabilities  Having defined topic distributions in a way that can  2.Draw stop probabilities  preserve cross-language correspondences, we now  3.For each language l, draw emission probabilities for  use this distribution within a larger model that can  that synset  discover cross-language patterns of use that predict  For conciseness in the rest of the paper, we will refer  to this generative process as multilingual Dirichlet  1.2 The MLSLDA Model  hierarchy, or MULTDIRHIER( , , ).2 Each  observed token can be viewed as the end result of a  We will view sentiment analysis as a regression  probsequence of visited synsets  lem: given an input document, we want to predict  . At each node in the  tree, the path can end at node  a real-valued observation y that represents the  sentii with probability i,1,  or it can continue to a child synset with probability  ment of a document.Specifically, we build on  supervised latent Dirichlet allocation (SLDA, (Blei and  i,0.If the path continues to another child synset, it  visits child  McAuliffe, 2007)), which makes predictions based  j with probability i,j.If the path ends at  on the topics expressed in a document; this can be  k with probability i,l,k.3  The probability of a word being emitted from a path  thought of projecting the words in a document to low  with visited synsets  dimensional space of dimension equal to the number  r and final synset h in language  of topics.Blei et al. showed that using this latent  l is therefore  topic structure can offer improved predictions over  regressions based on words alone, and the approach fits  well with our current goals, since word-level cues are  unlikely to be identical across languages.In addition  to text, SLDA has been successfully applied to other  domains such as social networks (Chang and Blei,  Note that the stop probability h is independent of  2009) and image classification (Wang et al., 2009).language, but the emission h,l is dependent on the  The key innovation in this paper is to extend SLDA  language.This is done to prevent the following by creating topics that are globally consistent across nario: while synset A is highly probable in a topic  languages, using the bridging approach above.  and words in language 1 attached to that synset have  We express our model in the form of a  probabilishigh probability, words in language 2 have low tic generative latent-variable model that generates ability.If this could happen for many synsets in  documents in multiple languages and assigns a  reala topic, an entire language would be effectively valued score to each document.The score comes lenced, which would lead to inconsistent topics (e.g. from a normal distribution whose sum is the dot prod-2Variables  uct between a regression parameter that encodes  h, h,l, and h are hyperparameters.Their mean  is fixed, but their magnitude is sampled during inference (i.e.  the influence of each topic on the observation and  is constant, but h,i is not).For the bushier bridges,  a variance 2.With this model in hand, we use  sta(e.g. dictionary and flat), their mean is uniform.For GermaNet, tistical inference to determine the distribution over  we took frequencies from two balanced corpora of German and  English: the British National Corpus (University of Oxford,  latent variables that, given the model, best explains  2006) and the Kern Corpus of the Digitales W rterbuch der  The generative model is as follows:  We took these frequencies and propagated them through the  multilingual hierarchy, following LDAWN's (Boyd-Graber et  1.For each topic i = 1 . ..K, draw a topic distribution  al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior.The variance of the priors was initialized to be  1.0, but could be sampled during inference.2. For each document d = 1 . ..M with language ld:  3Note that the language and word are taken as given, but the  (a) Choose a distribution over topics d  path through the semantic hierarchy is a latent random variable.(b) For each word in the document n = 1 . ..Nd, choose a topic assignment  and a path d,n ending at word wd,n according  to Equation 1 using { z ,  3.Choose a response variable from y  z, 2 , where  Crucially, note that the topics are not  independent of the sentiment task; the regression encourages  terms with similar effects on the observation y to  be in the same topic.The consistency of topics  described above allows the same regression to be done  for the entire corpus regardless of the language of the  Sentiment Prediction  Figure 1: Graphical model representing MLSLDA.2 Inference  Shaded nodes represent observations, plates denote  replication, and lines show probabilistic dependencies.Finding the model parameters most likely to explain  the data is a problem of statistical inference.We  employ stochastic EM (Diebolt and Ip, 1996), using a  probability of taking a path r is then  Gibbs sampler for the E-step to assign words to paths  and topics.After randomly initializing the topics,  we alternate between sampling the topic and path  of a word (z  d,n, d,n) and finding the regression  parameters that maximize the likelihood.We jointly  sample the topic and path conditioning on all of the  other path and document assignments in the corpus,  Transition  selecting a path and topic with probability  Each of these three terms reflects a different influence  Equation 3 reflects the multilingual aspect of this  on the topics from the vocabulary structure, the model.The conditional topic distribution for  ument's topics, and the response variable.In the next  SLDA (Blei and McAuliffe, 2007) replaces this term  paragraphs, we will expand each of them to derive  with the standard Multinomial-Dirichlet.However,  the full conditional topic distribution.we believe this is the first published SLDA-style  As discussed in Section 1.1, the structure of the  model using MCMC inference, as prior work has  topic distribution encourages terms with the same  used variational inference (Blei and McAuliffe, 2007;  meaning to be in the same topic, even across Chang and Blei, 2009; Wang et al., 2009).  guages.During inference, we marginalize over  posBecause the observed response variable depends  sible multinomial distributions , , and , using  on the topic assignments of a document, the  condithe observed transitions from i to j in topic k; Tk,i,j, tional topic distribution is shifted toward topics that stop counts in synset i in topic k, Ok,i,0; continue  explain the observed response.Topics that move the  counts in synsets i in topic k, Ok,i,1; and emission  predicted response yd toward the true yd will be  facounts in synset i in language l in topic k, Fk,i,l.The  vored.We drop terms that are constant across all  topics for the effect of the response variable,  3 Bridges Across Languages  In Section 1.1, we described connections across  languages as offered by semantic networks in a general  way, using WordNet as an example.In this section,  we provide more specifics, as well as alternative ways  Other words' influence  of building semantic connections across languages.Flat First, we can consider a degenerate mapping  that is nearly equivalent to running SLDA  independently across multiple languages, relating topics only  This word's influence  based on the impact on the response variable.ConThe above equation represents the supervised aspect  sider a degenerate tree with only one node, with all  of the model, which is inherited from SLDA.words in all languages associated with that node.This  Finally, there is the effect of the topics already  is consistent with our model, but there is really no  assigned to a document; the conditional distribution  shared semantic space, as all emitted words must  favors topics already assigned in a document,  come from this degenerate synset and the model  only represents the output distribution for this single  WordNet We took the alignment of GermaNet to  This term represents the document focus of this  moved all synsets that were had no mapped German  model; it is present in all Gibbs sampling inference  words.Any German synsets that did not have English  schemes for LDA (Griffiths and Steyvers, 2004).translations had their words mapped to the lowest  Multiplying together Equations 3, 4, and 5 allows  extant English hypernym (e.g. beinbruch, a  brous to sample a topic using the conditional distribution  ken leg, was mapped to fracture ).We stemmed  from Equation 2, based on the topic and path of the  all words to account for inflected forms not being  other words in all languages.After sampling the  present (Porter and Boulton, 1970).An example  path and topic for each word in a document, we then  of the paths for the German word wunsch (wish,  find new regression parameters that maximize the  request) is shown in Figure 2(a).likelihood conditioned on the current state of the  sampler.This is simply a least squares regression  Dictionaries A dictionary can be viewed as a many  using the topic assignments  to many mapping, where each entry  or more words in one language  Prediction on documents for which we don't have  an observed  i in another language.Entries were taken  d is equivalent to marginalizing over  from an English-German dictionary (Richter, 2008)  yd and sampling topics for the document from  Equaa Chinese-English dictionary (Denisowski, 1997),  tions 3 and 5.The prediction for yd is then the dot  and a Chinese-German dictionary (Hefti, 2005).As  product of and the empirical topic distribution zd.  with WordNet, the words in entries for English and  We initially optimized all hyperparameters using  German were stemmed to improve coverage.An  slice sampling.However, we found that the example for German is shown in Figure 2(b).sion variance 2 was not stable.Optimizing 2 seems  to balance between modeling the language in the Algorithmic Connections In addition to hand-uments and the prediction, and thus is sensitive to  curated connections across languages, one could also  documents' length.Given this sensitivity, we did  consider automatic means of mapping across  lannot optimize 2 for our prediction experiments in  guages, such as using edit distance or local  conSection 4, but instead kept it fixed at 0.25.We leave  text (Haghighi et al., 2008; Rapp, 1995) or  usoptimizing this variable, either through cross ing a lexical translation table obtained from paral-tion or adapting the model, to future work.lel text (Melamed, 1998).While we experimented  abstraction.n.06  option.n.02  option  option  act  speech_act.n.01  wish.n.04  wish  (a) GermaNet  (b) Dictionary  Figure 2: Two methods for constructing multilingual distributions over words.On the left, paths to the German word  wunsch in GermaNet are shown.On the right, paths to the English word room are shown.Both English and German words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines).Note that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language expression.  with these techniques, constructing appropriate (as there is no associated response variable for Eu-archies from these resources required many arbitrary  roparl documents); this experiment is to demonstrate  decisions about cutoffs and which words to include.the effectiveness of the multilingual aspect of the Thus, we do not consider them in this paper.  model.To test whether the topics learned by the  each document using the probability distribution d  over topic assignments.Each  LSLDA on three criteria: how well  d is a vector of length  it can discover consistent topics across languages  K and is a language-independent representation of  the document.for matching parallel documents, how well it can  discover sentiment-correlated word lists from  nonFor each document in one language, we computed  aligned text, and how well it can predict sentiment.the Hellinger distance between it and all of the  documents in the other language and sorted the documents  4.1 Matching on Multilingual Topics  by decreasing distance.The translation of the  document is somewhere in that set; the higher the  normalWe took the 1996 documents from the Europarl ized rank (the percentage of documents with a rank pus (Koehn, 2005) using three bridges: GermaNet, lower than the translation of the document), the better dictionary, and the uninformative flat matching.4 The  the underlying topic model connects languages.model is unaware that the translations of documents  in one language are present in the other language.We compare three bridges against what is to our  Note that this does not use the supervised framework  knowledge the only other topic model for unaligned  text, Multilingual Topics for Unaligned Text  (Boyd4For English and German documents in all experiments,  we removed stop words (Loper and Bird, 2002), stemmed  words (Porter and Boulton, 1970), and created a vocabulary  5The bipartite matching was initialized with the dictionary  of the most frequent 5000 words per language (this vocabulary  weights as specified by the Multilingual Topics for Unaligned  limit was mostly done to ensure that the dictionary-based bridge Text algorithm.The matching size was limited to 250 and the  was of manageable size).Documents shorter than fifty content  bipartite matching was only updated on the initial iteration then words were excluded.held fixed.This yielded results comparable to when the matching  a WordNet-like resource is used as the bridge, the Dictionary  resulting topics are distributions over synsets, not just  As our demonstration corpus, we used the Amherst  Dictionary  Sentiment Corpus (Constant et al., 2009), as it has  documents in multiple languages (English, Chinese,  and German) with numerical assessments of  sentiment (number of stars assigned to the review).We  Dictionary  segmented the Chinese text (Tseng et al., 2005) and  Bridge  used a classifier trained on character n-grams to  remove English-language documents that were mixed  Dictionary  in among the Chinese and German language reviews.Figure 4 shows extracted topics from  GermanEnglish and German-Chinese corpora.M  is able to distinguish sentiment-bearing topics from  content bearing topics.For example; in the  GermanEnglish corpus, food and children topics are  not associated with a consistent sentiment signal,  Figure 3: Average rank of paired translation document  while religion is associated with a more negative  recovered from the multilingual topic model.Random  sentiment.In contrast, in the German-Chinese  corguessing would yield 0.5; MLSLDA with a dictionary  pus, the religion/society topic is more neutral, and  based matching performed best.the gender-oriented topic is viewed more negatively.Negative sentiment-bearing topics have reasonable  Figure 3 shows the results of this experiment.The  words such as pages, k ong p (Chinese for I'm  dictionary-based bridge had the best performance on  afraid that . ..) and tuo (Chienese for discard ),  the task, ranking a large proportion of documents  and positive sentiment-bearing topics have  reason(0.95) below the translated document once enough  able words such as great, good, and juwel  (Gertopics were available. Although GermaNet is richer, man for jewel ).its coverage is incomplete; the dictionary structure  The qualitative topics also betray some of the  had a much larger vocabulary and could build a more  weaknesses of the model.For example, in one of  complete multilingual topics.Using comparable the negative sentiment topics, the German word gut  put information, this more flexible model performed  (good) is present.Because topics are distributions  better on the matching task than the existing over words, they can encode the presence of nega-lingual topic model available for unaligned text.The  tions like kein (no) and nicht (not), but not  collodegenerate flat bridge did no better than the baseline  cations like nicht gut.More elaborate topic models  of random guessing, as expected.  that can model local syntax and collocations  (Johnson, 2010) provide options for addressing such  prob4.2 Qualitative Sentiment-Correlated Topics  One of the key tasks in sentiment analysis has been  We do not report the results for sentiment  predicthe collection of lists of words that convey tion for this corpus because the baseline of predicting ment (Wilson, 2008; Riloff et al., 2003).These  a positive review is so strong; most algorithms do  exresources are often created using or in reference  tremely well by always predicting a positive review,  to resources like WordNet (Whitelaw et al., 2005;  ours included.  Baccianella and Sebastiani, 2010).MLSLDA 4.3 Sentiment Prediction vides a method for extracting topical and sentiment-correlated word lists from multilingual corpora.If  We gathered 330 film reviews from a German film  review site (Vetter et al., 2000) and combined them  was updated more frequently.  with a much larger English film review corpus of over  (harry) harry (harry)  (lord) herr  (both)  (universe) all (both)  (community)  life art  healthy rezepte  people thema  ([I'm afraid that...])(that) dass (both)  (story) story (treasure)  (mostly)  (female) (attractive) attraktiv (both)  (good) gut ([really isn't])  books geschichte parents baby  separate story  (soon) bald (both)  (a) German / English  (b) German / Chinese  Figure 4: Topics, along with associated regression coefficient from a learned 25-topic model on German-English (left) and German-Chinese (right) documents.Notice that theme-related topics have regression parameter near zero, topics discussing the number of pages have negative regression parameters, topics with good, great, h ao (good) and  berzeugt (convinced) have positive regression parameters.For the German-Chinese corpus, note the presence of gut  (good) in one of the negative sentiment topics, showing the difficulty of learning collocations.Train  Test GermaNet Dictionary Flat  One would expect that prediction improves with a  larger training set.For this model, such an  improveTable 1: Mean squared error on a film review corpus.ment is seen even when the training set includes no  All results are on the same German test data, varying the  documents in the target language.Note that even the  training data.Over-fitting prevents the model learning on  the German data alone; adding English data to the mix  ful information.After introducing English data, the  allows the model to make better predictions.model learns to prefer smaller regression parameters  (this can be seen as a form of regularization).5000 film reviews (Pang and Lee, 2005) to create a  multilingual film review corpus.6  Performance is best when a reasonably large  corThe results for predicting sentiment in German  pus is available including some data in the target  documents with 25 topics are presented in Table 1. language.For each bridge, performance improves On a small monolingual corpus, prediction is very  dramatically, showing that MLSLDA is successfully  poor.The model over-fits, especially when it has  able to incorporate information learned from both  the entire vocabulary to select from.The slightly  languages to build a single, coherent picture of how  better performance using GermaNet and a dictionary  sentiment is expressed in both languages.With the  as topic priors can be viewed as basic feature GermaNet bridge, performance is better than both tion, removing proper names from the vocabulary to  the degenerate and dictionary based bridges, showing  6We followed Pang and Lee's method for creating a  nuthat the model is sharing information both through  merical score between 0 and 1 from a star rating.the multilingual topics and the regression parameters.then converted that to an integer by multiplying by 100;  Performance on English prediction is comparable  this was done because initial data preprocessing assumed  to previously published results on this dataset (Blei  integer values (although downstream processing did not  asand McAuliffe, 2007); with enough data, a  monolinsume integer values).The German movie review corpus  is available at http://www.umiacs.umd.edu/  gual model is no longer helped by adding additional  static/downloads_and_media.html  5 Relationship to Previous Research  learned from a corpus, as a prior to seed topics so  that they attract other sentiment bearing words (Mei  The advantages of MLSLDA reside largely in the  et al., 2007; Lin and He, 2009).Other approaches  assumptions that it makes and does not make: view sentiment or perspective as a perturbation of ments need not be parallel, sentiment is a normally  a log-linear topic model (Lin et al., 2008).Such  distributed document-level property, words are techniques could be combined with the multilingual changeable, and sentiment can be predicted as a approach presented here by using distributions over gression on a K-dimensional vector.words that not only bridge different languages but  By not assuming parallel text, this approach can  also encode additional information.For example, the  be applied to a broad class of corpora.Other vocabulary hierarchies could be structured to encour-tilingual topic models require parallel text, either at  age topics that encourage correlation among similar  sentiment-bearing words (e.g. clustering words  assoor word-level (Kim and Khudanpur, 2004; Zhao and  ciated with price, size, etc.). Future work could also  Xing, 2006).Similarly, other multilingual sentiment  more rigorously validate that the multilingual topics  approaches also require parallel text, often supplied  discovered by MLSLDA are sentiment-bearing via  via automatic translation; after the translated text  is available, either monolingual analysis (Denecke,  In contrast, MLSLDA draws on techniques that  2008) or co-training is applied (Wan, 2009).In view sentiment as a regression problem based on the trast, our approach requires fewer resources for a topics used in a document, as in supervised latent guage: a dictionary (or similar knowledge structure  relating words to nodes in a graph) and comparable  2007) or in finer-grained parts of a document (Titov  text, instead of parallel text or a machine translation  and McDonald, 2008).Extending these models to  multilingual data would be more straightforward.Rather than viewing one language through the  lens of another language, MLSLDA views all 6 Conclusions  guages through the lens of the topics present in a  document.This is a modeling decision with pros and  MLSLDA is a holistic statistical model for  multicons.It allows a language agnostic decision about  sentiment to be made, but it restricts the or expensive multilingual resources.It discovers ness of the model in terms of sentiment in two ways.connections across languages that can recover la-First, it throws away information important to tent structure in parallel corpora, discover sentiment-timent analysis like syntactic constructions (Greene  correlated word lists in multiple languages, and make  and Resnik, 2009) and document structure accurate predictions across languages that improve ald et al., 2007) that may impact the sentiment rating. with more multilingual data, as demonstrated in the Second, a single real number is not always sufficient  context of sentiment analysis.to capture the nuances of sentiment.Less critically,  More generally, MLSLDA provides a formalism  assuming that sentiment is normally distributed is not  that can be used to incorporate the many insights of  true of all real-world corpora; review corpora often  topic modeling-driven sentiment analysis to  multihave a skew toward positive reviews.We standardize  lingual corpora by tying together word distributions  responses by the mean and variance of the training  across languages.MLSLDA can also contribute to  data to partially address this issue, but other response  the development of word list-based sentiment  sysdistributions are possible, such as generalized linear  tems: the topics discovered by MLSLDA can serve  models (Blei and McAuliffe, 2007) and vector as a first-pass means of sentiment-based word lists chines (Zhu et al., 2009), which would allow more  for languages that might lack annotated resources.traditional classification predictions.MLSLDA also can be viewed as a  sentimentOther probabilistic models for sentiment informed multilingual word sense disambiguation cation view sentiment as a word level feature.Some  (WSD) algorithm.When the multilingual bridge is an  models use sentiment word lists, either given or  explicit representation of sense such as WordNet, part  of the generative process is an explicit assignment Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.  of every word to sense (the path latent variable );  2007.A topic model for word sense disambiguation.this is discovered during inference.The  dictionaryIn EMNLP.based technique may be viewed as a disambiguation  Jonathan Chang and David M. Blei.2009. Relational  via a transfer dictionary.How sentiment prediction  topic models for document networks.In AISTATS.impacts the implicit WSD is left to future work.Shay B. Cohen and Noah A. Smith.2009. Shared  logistic normal distributions for soft parameter tying in  Better capturing local syntax and meaningful  colunsupervised grammar induction.In NAACL.locations would also improve the model's ability to  Noah Constant, Christopher Davis, Christopher Potts, and  predict sentiment and model multilingual topics, as  Florian Schwarz.2009. The pragmatics of expressive  would providing a better mechanism for  representcontent: Evidence from large corpora.Sprache und  ing words not included in our bridges.We intend to  develop such models as future work.gual sentiment analysis.In ICDEW 2008.7 Acknowledgments  CEDICT.This research was funded in part by the Army Jean Diebolt and Eddie H.S.Ip, 1996.Markov Chain search Laboratory through ARL Cooperative Agree-Monte Carlo in Practice, chapter Stochastic EM:  ment W911NF-09-2-0072 and by the Office of the  method and application.Chapman and Hall, London.Director of National Intelligence (ODNI), Alexander Geyken.2007. The DWDS corpus: A ref-gence Advanced Research Projects Activity (IARPA),  erence corpus for the German language of the 20th  through the Army Research Laboratory.All  statecentury.In Idioms and Collocations: Corpus-based  Linguistic, Lexicographic Studies.Continuum Press.ments of fact, opinion or conclusions contained  Stephan Greene and Philip Resnik.2009. More than  herein are those of the authors and should not be  words: Syntactic packaging and implicit sentiment.In  construed as representing the official views or  policies of ARL, IARPA, the ODNI, or the U.S. Thomas L. Griffiths and Mark Steyvers.2004. Finding ment.The authors thank the anonymous reviewers,  scientific topics.PNAS, 101(Suppl 1):5228 5235.Jonathan Chang, Christiane Fellbaum, and Lawrence  Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and  Watts for helpful comments.The authors especially  Dan Klein.2008. Learning bilingual lexicons from  thank Chris Potts for providing help in obtaining and  monolingual corpora.In ACL, Columbus, Ohio.processing reviews.the Japanese WordNet.In LREC.Mark Johnson.2010. PCFGs, topic models, adaptor  grammars and learning topical collocations and the  2009.Incorporating domain knowledge into topic  modstructure of proper names.In ACL.eling via Dirichlet forest priors.In ICML.triggers and latent semantic analysis for cross-lingual  2010.Sentiwordnet 3.0: An enhanced lexical resource  for sentiment analysis and opinion mining.In LREC.Philipp Koehn.2005. Europarl: A parallel corpus  Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer  for statistical machine translation.In MT Summit.Hassan.2008. Multilingual subjectivity analysis using  machine translation.In EMNLP.David M. Blei and John D. Lafferty.2005. Correlated  ing WordNets in a web-compliant format: The case of  topic models.In NIPS.GermaNet.In Workshop on Wordnets Structures and  David M. Blei and Jon D.McAuliffe.2007. Supervised  topic models.In NIPS.MIT Press.Chenghua Lin and Yulan He.2009. Joint sentiment/topic  model for sentiment analysis.In CIKM.Latent Dirichlet allocation.JMLR, 3:993 1022.2008. A joint topic and perspective model for  ideoEdward Loper and Steven Bird.2002. NLTK: the natu-Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel  Jural language toolkit.In Tools and methodologies for  rafsky, and Christopher Manning.2005. A conditional  teaching.ACL.random field word segmenter.In SIGHAN Workshop  on Chinese Language Processing.Wells, and Jeff Reynar.2007. Structured models for  University of Oxford.British  Nafine-to-coarse sentiment analysis.In ACL.tional  Corpus.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and  ChengXiang Zhai.2007. Topic sentiment mixture:  Tobias Vetter, Manfred Sauer, and Philipp Wallutat.modeling facets and opinions in weblogs.In WWW.Ilya Dan Melamed.1998. Empirical methods for  exploiting parallel texts.Ph.D. thesis, University of  PennsylXiaojun Wan.2009. Co-training for cross-lingual  sentiment classification.In ACL.George A. Miller.1990. Nouns in WordNet: A lexical  inheritance system.International Journal of  Lexicogneous image classification and annotation.In CVPR.raphy, 3(4):245 264.Xing Wei and Bruce Croft.2006. LDA-based document  Smith, and Andrew McCallum.2009. Polylingual  Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005. Using appraisal groups for sentiment analysis.In CIKM.2009. Mining multilingual topics from Wikipedia.In  Theresa Ann Wilson.2008. Fine-grained Subjectivity and  Sentiment Analysis: Recognizing the Intensity, Polarity,  Noam Ordan and Shuly Wintner.2007. Hebrew  Wordand Attitudes of Private States.Ph.D. thesis, University  Net: a test case of aligning lexical databases across  lanof Pittsburgh.  guages.International Journal of Translation, 19(1):39  topic admixture models for word alignment.In ACL.Bo Pang and Lillian Lee.2005. Seeing stars: Exploiting  Jun Zhu, Amr Ahmed, and Eric P. Xing.2009. Medlda:  class relationships for sentiment categorization with  maximum margin supervised topic models for  regresrespect to rating scales.In ACL.sion and classification.In ICML.Sentiment Analysis.Now Publishers Inc.Martin Porter and Richard Boulton.1970. Snowball  Matthew Purver, Konrad K rding, Thomas L. Griffiths,  and Joshua Tenenbaum.2006. Unsupervised topic  modelling for multi-party spoken discourse.In ACL.Reinhard Rapp.1995. Identifying word translations in  Philip Resnik.1995. Using information content to  evaluate semantic similarity in a taxonomy.In IJCAI, pages  Frank Richter.2008. Dictionary nice grep.http://wwwEllen Riloff, Janyce Wiebe, and Theresa Wilson.2003.  Learning subjective nouns using extraction pattern  bootMichal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,  and Padhraic Smyth.2004. The author-topic model for  authors and documents.In UAI.Beno t Sagot and Darja Fi er.2008. Building a Free  French WordNet from Multilingual Resources.In  OntoLex.Ivan Titov and Ryan McDonald.2008. A joint model of  text and aspect ratings for sentiment summarization.In"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Joint Parsing and Named Entity Recognition  Jenny Rose Finkel and Christopher D. Manning  Computer Science Department  Stanford University  Stanford, CA 94305  fortunately, it is still common practice to cobble  together independent systems for the various types of  For many language technology applications,  annotation, and there is no guarantee that their  outsuch as question answering, the overall  sysputs will be consistent.  This paper begins to address this problem by  the data (such as a named entity recognizer, a  coreference system, and a parser). This  easbuilding a joint model of both parsing and named  ily results in inconsistent annotations, which  entity recognition. Vapnik has observed (Vapnik,  are harmful to the performance of the  aggre1998; Ng and Jordan, 2002) that one should solve  gate system. We begin to address this  probthe problem directly and never solve a more  genlem with a joint model of parsing and named  eral problem as an intermediate step, implying that  entity recognition, based on a discriminative  building a joint model of two phenomena is more  feature-based constituency parser. Our model  likely to harm performance on the individual tasks  produces a consistent output, where the named  than to help it. Indeed, it has proven very  diffientity spans do not conflict with the phrasal  cult to build a joint model of parsing and  semanspans of the parse tree. The joint  representic role labeling, either with PCFG trees (Sutton and  tation also allows the information from each  McCallum, 2005) or with dependency trees. The  type of annotation to improve performance  CoNLL 2008 shared task (Surdeanu et al., 2008)  on the other, and, in experiments with the  OntoNotes corpus, we found improvements of  was intended to be about joint dependency parsing  up to 1.36% absolute F1 for parsing, and up to  and semantic role labeling, but the top performing  9.0% F1 for named entity recognition.  systems decoupled the tasks and outperformed the  systems which attempted to learn them jointly.  Despite these earlier results, we found that combining  ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","Joint Parsing and Named Entity Recognition fortunately, it is still common practice to cobble together independent systems for the various types of For many language technology applications, annotation, and there is no guarantee that their outsuch as question answering, the overall sysputs will be consistent. This paper begins to address this problem by  the data (such as a named entity recognizer, a  coreference system, and a parser).This  easbuilding a joint model of both parsing and named  ily results in inconsistent annotations, which  entity recognition.Vapnik has observed (Vapnik,  are harmful to the performance of the  aggre1998; Ng and Jordan, 2002) that one should solve  gate system.We begin to address this  probthe problem directly and never solve a more  genlem with a joint model of parsing and named  eral problem as an intermediate step, implying that  entity recognition, based on a discriminative  building a joint model of two phenomena is more  feature-based constituency parser.Our model  likely to harm performance on the individual tasks  produces a consistent output, where the named  than to help it.Indeed, it has proven very  diffientity spans do not conflict with the phrasal  cult to build a joint model of parsing and  semanspans of the parse tree.The joint  representic role labeling, either with PCFG trees (Sutton and  tation also allows the information from each  McCallum, 2005) or with dependency trees.The  type of annotation to improve performance  CoNLL 2008 shared task (Surdeanu et al., 2008)  on the other, and, in experiments with the  OntoNotes corpus, we found improvements of  was intended to be about joint dependency parsing  up to 1.36% absolute F1 for parsing, and up to  and semantic role labeling, but the top performing  9.0% F1 for named entity recognition.systems decoupled the tasks and outperformed the  systems which attempted to learn them jointly.Despite these earlier results, we found that combining"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Seeing stars when there aren't many stars: Graph-based semi-supervised learning for sentiment categorization Andrew B. Goldberg  Computer Sciences Department  Computer Sciences Department  University of Wisconsin-Madison  University of Wisconsin-Madison  Madison, W.I. 53706  Madison, W.I. 53706  ment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as We present a graph-based semi-supervised  consumer product or movie reviews. In particular  learning algorithm to address the  sentiPang and Lee proposed the rating-inference problem ment analysis task of rating inference.  (2005). Rating inference is harder than binary posi-Given a set of documents (e.g., movie  tive / negative opinion classification. The goal is to reviews) and accompanying ratings (e.g.,  infer a numerical rating from reviews, for example  4 stars ), the task calls for inferring  nuthe number of stars that a critic gave to a movie.  merical ratings for unlabeled documents  Pang and Lee showed that supervised machine learn-based on the perceived sentiment  exing techniques (classification and regression) work pressed by their text.  In particular, we  well for rating inference with large amounts of train-are interested in the situation where  labeled data is scarce. We place this task  However, review documents often do not come  in the semi-supervised setting and  demonwith numerical ratings. We call such documents un-strate that considering unlabeled reviews  labeled data. Standard supervised machine learning in the learning process can improve rating-algorithms cannot learn from unlabeled data.  Asinference performance. We do so by  creatsigning labels can be a slow and expensive process ing a graph on both labeled and unlabeled  because manual inspection and domain expertise are data to encode certain assumptions for this  needed. Often only a small portion of the documents task. We then solve an optimization prob-can be labeled within resource constraints, so most lem to obtain a smooth rating function  documents remain unlabeled. Supervised learning  over the whole graph. When only  limalgorithms trained on small labeled sets suffer in ited labeled data is available, this method  performance. Can one use the unlabeled reviews to achieves significantly better predictive ac-improve rating-inference? Pang and Lee (2005) sug-curacy over other methods that ignore the  gested that doing so should be useful.  unlabeled examples during training.  We demonstrate that the answer is Yes.' Our  approach is graph-based semi-supervised learning.  Semi-supervised learning is an active research area 1  ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","ment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as We present a graph-based semi-supervised consumer product or movie reviews. In particular  learning algorithm to address the  sentiPang and Lee proposed the rating-inference problem ment analysis task of rating inference.(2005).Rating inference is harder than binary posi-Given a set of documents (e.g., movie  tive / negative opinion classification.The goal is to reviews) and accompanying ratings (e.g.,  infer a numerical rating from reviews, for example  4 stars ), the task calls for inferring  nuthe number of stars that a critic gave to a movie.merical ratings for unlabeled documents  Pang and Lee showed that supervised machine learn-based on the perceived sentiment  exing techniques (classification and regression) work pressed by their text.In particular, we  well for rating inference with large amounts of train-are interested in the situation where  labeled data is scarce.We place this task  However, review documents often do not come  in the semi-supervised setting and  demonwith numerical ratings.We call such documents un-strate that considering unlabeled reviews  labeled data.Standard supervised machine learning in the learning process can improve rating-algorithms cannot learn from unlabeled data.Asinference performance.We do so by  creatsigning labels can be a slow and expensive process ing a graph on both labeled and unlabeled  because manual inspection and domain expertise are data to encode certain assumptions for this  needed.Often only a small portion of the documents task.We then solve an optimization prob-can be labeled within resource constraints, so most lem to obtain a smooth rating function  documents remain unlabeled.Supervised learning  over the whole graph.When only  limalgorithms trained on small labeled sets suffer in ited labeled data is available, this method  performance.Can one use the unlabeled reviews to achieves significantly better predictive ac-improve rating-inference?Pang and Lee (2005) sug-curacy over other methods that ignore the  gested that doing so should be useful.unlabeled examples during training.We demonstrate that the answer is Yes.'Our  approach is graph-based semi-supervised learning.Semi-supervised learning is an active research area 1"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," A study of Information Retrieval weighting schemes for sentiment analysis Georgios Paltoglou  Mike Thelwall  University of Wolverhampton  University of Wolverhampton  Wolverhampton, United Kingdom  Wolverhampton, United Kingdom  Most of the work in sentiment analysis has  focused on supervised learning techniques  (SebasMost sentiment analysis approaches use as  tiani, 2002), although there are some notable ex-baseline a support vector machines (SVM)  ceptions (Turney, 2002; Lin and He, 2009).  Preclassifier with binary unigram weights.  vious research has shown that in general the per-In this paper, we explore whether more  formance of the former tend to be superior to that sophisticated feature weighting schemes  of the latter (Mullen and Collier, 2004; Lin and from Information Retrieval can enhance  He, 2009). One of the main issues for supervised classification accuracy. We show that vari-approaches has been the representation of  docuants of the classic tf.idf scheme adapted  ments. Usually a bag of words representation is to sentiment analysis provide significant  adopted, according to which a document is  modincreases in accuracy, especially when  useled as an unordered collection of the words that ing a sublinear function for term frequency  it contains. Early research by Pang et al. (2002) in weights and document frequency smooth-sentiment analysis showed that a binary unigraming. The techniques are tested on a wide  based representation of documents, according to selection of data sets and produce the best  which a document is modeled only by the  presaccuracy to our knowledge.  ence or absence of words, provides the best base-1  ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","Most of the work in sentiment analysis has focused on supervised learning techniques (SebasMost sentiment analysis approaches use as tiani, 2002), although there are some notable ex-baseline a support vector machines (SVM) Preclassifier with binary unigram weights.vious research has shown that in general the per-In this paper, we explore whether more  formance of the former tend to be superior to that sophisticated feature weighting schemes  of the latter (Mullen and Collier, 2004; Lin and from Information Retrieval can enhance  He, 2009).One of the main issues for supervised classification accuracy.We show that vari-approaches has been the representation of  docuants of the classic tf.idf scheme adapted  ments.Usually a bag of words representation is to sentiment analysis provide significant  adopted, according to which a document is  modincreases in accuracy, especially when  useled as an unordered collection of the words that ing a sublinear function for term frequency  it contains.Early research by Pang et al.(2002) in weights and document frequency smooth-sentiment analysis showed that a binary unigraming.The techniques are tested on a wide  based representation of documents, according to selection of data sets and produce the best  which a document is modeled only by the  presaccuracy to our knowledge.ence or absence of words, provides the best base-1"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales  Bo Pang and Lillian Lee  (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract  become aware of the scientific challenges posed and  the scope of new applications enabled by the  proWe address the rating-inference problem,  cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.) Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ). This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivasdard multi-class text categorization  besiloglou, 2003). But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.  the task.  Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.  We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.  mission is mediocre (middling rating). Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.  be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1 ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales become aware of the scientific challenges posed and the scope of new applications enabled by the proWe address the rating-inference problem, cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.)Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ).This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivasdard multi-class text categorization  besiloglou, 2003).But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.the task.Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.mission is mediocre (middling rating).Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts  Department of Computer Science  Cornell University  Ithaca, NY 14853-7501  ter; and then (2) apply a standard machine-learning Sentiment analysis seeks to identify the view-classifier to the resulting extract. This can prevent point(s) underlying a text span; an example appli-the polarity classifier from considering irrelevant or cation is classifying a movie review as thumbs up  even potentially misleading text: for example, al-or thumbs down . To determine this sentiment po-though the sentence The protagonist tries to pro-larity, we propose a novel machine-learning method tect her good name contains the word good , it that applies text-categorization techniques to just tells us nothing about the author's opinion and in the subjective portions of the document. Extracting fact could well be embedded in a negative movie these portions can be implemented using efficient review. Also, as mentioned above, subjectivity ex-techniques for finding minimum cuts in graphs; this tracts can be provided to users as a summary of the greatly facilitates incorporation of cross-sentence sentiment-oriented content of the document.  Our results show that the subjectivity extracts we create accurately represent the sentiment in-Publication info: Proceedings of the ACL, 2004.  formation of the originating documents in a much 1 ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts ter; and then (2) apply a standard machine-learning Sentiment analysis seeks to identify the view-classifier to the resulting extract. This can prevent point(s) underlying a text span; an example appli-the polarity classifier from considering irrelevant or cation is classifying a movie review as thumbs up  even potentially misleading text: for example, al-or thumbs down . To determine this sentiment po-though the sentence The protagonist tries to pro-larity, we propose a novel machine-learning method tect her good name contains the word good , it that applies text-categorization techniques to just tells us nothing about the author's opinion and in the subjective portions of the document.Extracting fact could well be embedded in a negative movie these portions can be implemented using efficient review.Also, as mentioned above, subjectivity ex-techniques for finding minimum cuts in graphs; this tracts can be provided to users as a summary of the greatly facilitates incorporation of cross-sentence sentiment-oriented content of the document.Our results show that the subjectivity extracts we create accurately represent the sentiment in-Publication info: Proceedings of the ACL, 2004.formation of the originating documents in a much 1"
" Learning Word Vectors for Sentiment Analysis Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts  Stanford University  Stanford, CA 94305  [amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract  recognition, part of speech tagging, and document  retrieval (Turney and Pantel, 2010; Collobert and  Unsupervised vector-based approaches to  seWeston, 2008; Turian et al., 2010).  mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.  important for a wide range of NLP tasks. We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents. However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.  The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations. We instantiate the model to utilize the  tion. For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings). We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.  pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification. We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007). This  "," Multiple Aspect Ranking using the Good Grief Algorithm Benjamin Snyder and Regina Barzilay  Computer Science and Artificial Intelligence Laboratory  Massachusetts Institute of Technology  restaurant. Rather than lumping these aspects into a  single score, we would like to capture each aspect of  We address the problem of analyzing  multhe writer's opinion separately, thereby providing a  tiple related opinions in a text. For  inmore fine-grained view of opinions in the review.  stance, in a restaurant review such  opinTo this end, we aim to predict a set of numeric  ions may include food, ambience and  serranks that reflects the user's satisfaction for each  asvice. We formulate this task as a multiple  pect. In the example above, we would assign a  nuaspect ranking problem, where the goal is  meric rank from 1-5 for each of: food quality,  serto produce a set of numerical scores, one  vice, and ambience.  for each aspect. We present an algorithm  A straightforward approach to this task would be  that jointly learns ranking models for  into rank1 the text independently for each aspect,  usdividual aspects by modeling the  depening standard ranking techniques such as regression  dencies between assigned ranks. This  alor classification. However, this approach fails to  exgorithm guides the prediction of  individual rankers by analyzing meta-relations  ments across different aspects. Knowledge of these  between opinions, such as agreement and  contrast. We prove that our  agreementranks, as a user's opinions on one aspect can  influbased joint model is more expressive than  ence his or her opinions on others.  The algorithm presented in this paper models  results further confirm the strength of the  the dependencies between different labels via the  model: the algorithm provides significant  agreement relation. The agreement relation captures improvement over both individual rankers  whether the user equally likes all aspects of the item  and a state-of-the-art joint ranking model.  or whether he or she expresses different degrees of  satisfaction. Since this relation can often be  deter1 ",1,"[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu Abstract recognition, part of speech tagging, and document Unsupervised vector-based approaches to mantics can model rich lexical meanings, but  In this paper, we present a model to capture both  they largely fail to capture sentiment  information that is central to many word meanings and  semantic and sentiment similarities among words.important for a wide range of NLP tasks.We  The semantic component of our model learns word  vectors via an unsupervised probabilistic model of vised and supervised techniques to learn word  documents.However, in keeping with linguistic and vectors capturing semantic term document in-cognitive research arguing that expressive content formation as well as rich sentiment content.The proposed model can leverage both  conplan, 1999; Jay, 2000; Potts, 2007), we find that  this basic model misses crucial sentiment  informaformation as well as non-sentiment  annotations.We instantiate the model to utilize the  tion.For example, while it learns that wonderful document-level sentiment polarity annotations  and amazing are semantically close, it doesn't cap-present in many online documents (e.g. star  ture the fact that these are both very strong positive ratings).We evaluate the model using small,  sentiment words, at the opposite end of the spectrum widely used sentiment and subjectivity cor-from terrible and awful.pora and find it out-performs several  previously introduced methods for sentiment  clasThus, we extend the model with a supervised  sification.We also introduce a large dataset  sentiment component that is capable of embracing  of movie reviews to serve as a more robust  many social and attitudinal aspects of meaning (Wil-benchmark for work in this area.  and Zhu, 2006; Snyder and Barzilay, 2007).This","restaurant. Rather than lumping these aspects into a  single score, we would like to capture each aspect of  We address the problem of analyzing  multhe writer's opinion separately, thereby providing a  tiple related opinions in a text.For  inmore fine-grained view of opinions in the review.stance, in a restaurant review such  opinTo this end, we aim to predict a set of numeric  ions may include food, ambience and  serranks that reflects the user's satisfaction for each  asvice.We formulate this task as a multiple  pect.In the example above, we would assign a  nuaspect ranking problem, where the goal is  meric rank from 1-5 for each of: food quality,  serto produce a set of numerical scores, one  vice, and ambience.for each aspect.We present an algorithm  A straightforward approach to this task would be  that jointly learns ranking models for  into rank1 the text independently for each aspect,  usdividual aspects by modeling the  depening standard ranking techniques such as regression  dencies between assigned ranks.This  alor classification.However, this approach fails to  exgorithm guides the prediction of  individual rankers by analyzing meta-relations  ments across different aspects.Knowledge of these  between opinions, such as agreement and  contrast.We prove that our  agreementranks, as a user's opinions on one aspect can  influbased joint model is more expressive than  ence his or her opinions on others.The algorithm presented in this paper models  results further confirm the strength of the  the dependencies between different labels via the  model: the algorithm provides significant  agreement relation.The agreement relation captures improvement over both individual rankers  whether the user equally likes all aspects of the item  and a state-of-the-art joint ranking model.  or whether he or she expresses different degrees of  satisfaction.Since this relation can often be  deter1"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Predicting the Semantic Orientation of Adjectives  Vasileios Hatzivassiloglou and Kathleen R. McKeown  Department of Computer Science  450 Computer Science Building  Columbia University  New York, N.Y. 10027, USA  {vh, kathy) cs, columbia, edu  We identify and validate from a large pus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achiev-ing 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many jectives, a clustering algorithm separates the adjectives into groups of different tations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of tions in the corpus.  ",1,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","Predicting the Semantic Orientation of Adjectives McKeown  Department of Computer Science  450 Computer Science Building  Columbia University  New York, N.Y. 10027, USA  {vh, kathy) cs, columbia, edu  We identify and validate from a large pus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives.A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achiev-ing 82% accuracy in this task when each conjunction is considered independently.Combining the constraints across many jectives, a clustering algorithm separates the adjectives into groups of different tations, and finally, adjectives are labeled positive or negative.Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of tions in the corpus."
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Extracting Aspect-Evaluation and Aspect-of Relations in Opinion Mining Nozomi Kobayashi Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science,  Nara Institute of Science and Technology  reviews) can be classified into two approaches:  DocThe technology of opinion extraction allows  ument classification and information extraction. The  users to retrieve and analyze people's  opinformer is the task of classifying documents or  passages according to their semantic orientation such as  positive vs. negative. This direction has been  forming of the opinion holder, the subject being  ing the mainstream of research on opinion-sensitive  evaluated, the part or the attribute in which  text processing (Pang et al., 2002; Turney, 2002,  the subject is evaluated, and the value of the  etc.). The latter, on the other hand, focuses on the  evaluation that expresses a positive or  negtask of extracting opinions consisting of information  ative assessment. We use this definition as  about, for example, hwho feels how about which as-the basis for our opinion extraction task. We  pect of what producti from unstructured text data.  focus on two important subtasks of opinion  In this paper, we refer to this information  extractionextraction: (a) extracting aspect-evaluation  oriented task as opinion extraction. In contrast to relations, and (b) extracting aspect-of re-sentiment classification, opinion extraction aims at  lations, and we approach each task using  producing richer information and requires an  inmethods which combine contextual and  stadepth analysis of opinions, which has only recently  tistical clues. Our experiments on Japanese  been attempted by a growing but still relatively small  weblog posts show that the use of  contexresearch community (Yi et al., 2003; Hu and Liu,  tual clues improve the performance for both  Most previous work on customer opinion  extraction assumes the source of information to be  ",1,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","reviews) can be classified into two approaches: DocThe technology of opinion extraction allows ument classification and information extraction. The  users to retrieve and analyze people's  opinformer is the task of classifying documents or  passages according to their semantic orientation such as  positive vs. negative.This direction has been  forming of the opinion holder, the subject being  ing the mainstream of research on opinion-sensitive  evaluated, the part or the attribute in which  text processing (Pang et al., 2002; Turney, 2002,  the subject is evaluated, and the value of the  etc.).The latter, on the other hand, focuses on the  evaluation that expresses a positive or  negtask of extracting opinions consisting of information  ative assessment.We use this definition as  about, for example, hwho feels how about which as-the basis for our opinion extraction task.We  pect of what producti from unstructured text data.focus on two important subtasks of opinion  In this paper, we refer to this information  extractionextraction: (a) extracting aspect-evaluation  oriented task as opinion extraction.In contrast to relations, and (b) extracting aspect-of re-sentiment classification, opinion extraction aims at  lations, and we approach each task using  producing richer information and requires an  inmethods which combine contextual and  stadepth analysis of opinions, which has only recently  tistical clues.Our experiments on Japanese  been attempted by a growing but still relatively small  weblog posts show that the use of  contexresearch community (Yi et al., 2003; Hu and Liu,  tual clues improve the performance for both  Most previous work on customer opinion  extraction assumes the source of information to be"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al. (1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  ento recognize and discard flames (Spertus, 1997).  tropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  ",0,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","79-86.Association for Computational Linguistics.Thumbs up?Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use.Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al.(1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization.Moreover, there are also  perform human-produced baselines.Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  ento recognize and discard flames (Spertus, 1997).tropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem.A challenging aspect of this categorization.We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner.For example, the"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Extracting Product Features and Opinions from Reviews Ana-Maria Popescu and Oren Etzioni  Department of Computer Science and Engineering  University of Washington  Seattle, WA 98195-2350  {amp, etzioni}@cs.washington.edu  We decompose the problem of review mining into the  Consumers are often forced to wade  following main subtasks:  through many on-line reviews in  I. Identify product features.  order to make an informed  prodII. Identify opinions regarding product features.  uct choice.  This paper introduces  III. Determine the polarity of opinions.  OPINE, an unsupervised  informationIV. Rank opinions based on their strength.  extraction system which mines  reThis paper introduces  views in order to build a model of  imOPINE, an unsupervised  information extraction system that embodies a solution to each portant product features, their evalu-of the above subtasks.  ation by reviewers, and their relative  OPINE is built on top of the  KnowItAll Web information-extraction system (Etzioni et al., quality across products.  2005) as detailed in Section 3.  Compared to previous work, OPINE  achieves 22% higher precision (with  Given a particular product and a corresponding set of  only 3% lower recall) on the feature  reviews, OPINE solves the opinion mining tasks outlined  extraction task.  above and outputs a set of product features, each accom-OPINE's novel use of  relaxation labeling for finding the  sepanied by a list of associated opinions which are ranked mantic orientation of words in con-based on strength ( e.g. , abominable is stronger than text leads to strong performance on  bad). This output information can then be used to  genthe tasks of finding opinion phrases  erate various types of opinion summaries.  and their polarity.  This paper focuses on the first 3 review mining  subtasks and our contributions are as follows:  ",1,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","{amp, etzioni}@cs.washington.edu We decompose the problem of review mining into the Consumers are often forced to wade following main subtasks: through many on-line reviews in I. Identify product features.order to make an informed  prodII.Identify opinions regarding product features.uct choice.This paper introduces  III.Determine the polarity of opinions.OPINE, an unsupervised  informationIV.Rank opinions based on their strength.extraction system which mines  reThis paper introduces  views in order to build a model of  imOPINE, an unsupervised  information extraction system that embodies a solution to each portant product features, their evalu-of the above subtasks.ation by reviewers, and their relative  OPINE is built on top of the  KnowItAll Web information-extraction system (Etzioni et al., quality across products.  2005) as detailed in Section 3.Compared to previous work, OPINE  achieves 22% higher precision (with  Given a particular product and a corresponding set of  only 3% lower recall) on the feature  reviews, OPINE solves the opinion mining tasks outlined  extraction task.above and outputs a set of product features, each accom-OPINE's novel use of  relaxation labeling for finding the  sepanied by a list of associated opinions which are ranked mantic orientation of words in con-based on strength ( e.g. , abominable is stronger than text leads to strong performance on  bad).This output information can then be used to  genthe tasks of finding opinion phrases  erate various types of opinion summaries.  and their polarity.This paper focuses on the first 3 review mining  subtasks and our contributions are as follows:"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Topic Identification for Fine-Grained Opinion Analysis  Veselin Stoyanov and Claire Cardie  Department of Computer Science  Cornell University  the form How/what does entity X feel/think about  topic Y? , for which document-level opinion  analWithin the area of general-purpose  fineysis methods can be problematic.  grained subjectivity analysis, opinion topic  identification has, to date, received little  Fine-grained subjectivity analyses typically  attention due to both the difficulty of the  identify SUBJECTIVE EXPRESSIONS in context,  charactask and the lack of appropriately  annoterize their POLARITY (e.g. positive, neutral or  negtated resources. In this paper, we  proative) and INTENSITY (e.g. weak, medium, strong,  vide an operational definition of opinion  extreme), and identify the associated SOURCE, or  topic and present an algorithm for opinion  OPINION HOLDER, as well as the TOPIC, or TARGET, of  topic identification that, following our new  the opinion. While substantial progress has been  definition, treats the task as a problem in  made in automating some of these tasks, opinion  topic coreference resolution. We develop a  topic identification has received by far the least  atmethodology for the manual annotation of  tention due to both the difficulty of the task and the  opinion topics and use it to annotate topic  lack of appropriately annotated resources.1  information for a portion of an existing  This paper addresses the problem of topic  idengeneral-purpose opinion corpus. In  expertification for fine-grained opinion analysis of  geniments using the corpus, our topic  identieral text.2 We begin by providing a new,  operafication approach statistically significantly  tional definition of opinion topic in which the topic  outperforms several non-trivial baselines  of an opinion depends on the context in which  according to three evaluation measures.  its associated opinion expression occurs. We also  present a novel method for general-purpose  opin1 ",0,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","Topic Identification for Fine-Grained Opinion Analysis the form How/what does entity X feel/think about topic Y? , for which document-level opinion  analWithin the area of general-purpose  fineysis methods can be problematic.grained subjectivity analysis, opinion topic  identification has, to date, received little  Fine-grained subjectivity analyses typically  attention due to both the difficulty of the  identify SUBJECTIVE EXPRESSIONS in context,  charactask and the lack of appropriately  annoterize their POLARITY (e.g. positive, neutral or  negtated resources.In this paper, we  proative) and INTENSITY (e.g. weak, medium, strong,  vide an operational definition of opinion  extreme), and identify the associated SOURCE, or  topic and present an algorithm for opinion  OPINION HOLDER, as well as the TOPIC, or TARGET, of  topic identification that, following our new  the opinion.While substantial progress has been  definition, treats the task as a problem in  made in automating some of these tasks, opinion  topic coreference resolution.We develop a  topic identification has received by far the least  atmethodology for the manual annotation of  tention due to both the difficulty of the task and the  opinion topics and use it to annotate topic  lack of appropriately annotated resources.1  information for a portion of an existing  This paper addresses the problem of topic  idengeneral-purpose opinion corpus.In  expertification for fine-grained opinion analysis of  geniments using the corpus, our topic  identieral text.2 We begin by providing a new,  operafication approach statistically significantly  tional definition of opinion topic in which the topic  outperforms several non-trivial baselines  of an opinion depends on the context in which  according to three evaluation measures.its associated opinion expression occurs.We also  present a novel method for general-purpose  opin1"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Extracting Semantic Orientations of Phrases from Dictionary Hiroya Takamura  Precision and Intelligence Laboratory  Integrated Research Institute  Tokyo Institute of Technology  Tokyo Institute of Technology  Precision and Intelligence Laboratory  Tokyo Institute of Technology  The most fundamental step for sentiment  analysis is to acquire the semantic orientations of words:  We propose a method for extracting  semantic orientations of phrases (pairs of an  example, the word beautiful is positive, while the  adjective and a noun): positive, negative,  word dirty is negative. Many researchers have  deor neutral. Given an adjective, the  semanveloped several methods for this purpose and  obtic orientation classification of phrases can  tained good results. One of the next problems to be  be reduced to the classification of words.  solved is to acquire semantic orientations of phrases, We construct a lexical network by con-or multi-term expressions, such as high+risk and  necting similar/related words. In the  net light+laptop-computer . Indeed the semantic  oriwork, each node has one of the three  orientations of phrases depend on context just as the  seentation values and the neighboring nodes  mantic orientations of words do, but we would like  tend to have the same value. We adopt  to obtain the orientations of phrases as basic units  the Potts model for the probability model  for sentiment analysis. We believe that we can use  of the lexical network. For each  adjecthe obtained basic orientations of phrases for affect  tive, we estimate the states of the nodes,  analysis of higher linguistic units such as sentences  which indicate the semantic orientations  of the adjective-noun pairs. Unlike  exA computational model for the semantic  orientaisting methods for phrase classification,  tions of phrases has been proposed by Takamura et  the proposed method can classify phrases  al. (2006). However, their method cannot deal with  consisting of unseen words. We also  prothe words that did not appear in the training data.  pose to use unlabeled data for a seed set of  The purpose of this paper is to propose a method for  extracting semantic orientations of phrases, which is  ation shows the effectiveness of the  proapplicable also to expressions consisting of unseen  words. In our method, we regard this task as the  noun classification problem for each adjective; the  ",0,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","Precision and Intelligence Laboratory Precision and Intelligence Laboratory The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: We propose a method for extracting semantic orientations of phrases (pairs of an example, the word beautiful is positive, while the adjective and a noun): positive, negative, word dirty is negative. Many researchers have  deor neutral.Given an adjective, the  semanveloped several methods for this purpose and  obtic orientation classification of phrases can  tained good results.One of the next problems to be  be reduced to the classification of words.solved is to acquire semantic orientations of phrases, We construct a lexical network by con-or multi-term expressions, such as high+risk and  necting similar/related words.In the  net light+laptop-computer . Indeed the semantic  oriwork, each node has one of the three  orientations of phrases depend on context just as the  seentation values and the neighboring nodes  mantic orientations of words do, but we would like  tend to have the same value.We adopt  to obtain the orientations of phrases as basic units  the Potts model for the probability model  for sentiment analysis.We believe that we can use  of the lexical network.For each  adjecthe obtained basic orientations of phrases for affect  tive, we estimate the states of the nodes,  analysis of higher linguistic units such as sentences  which indicate the semantic orientations  of the adjective-noun pairs.Unlike  exA computational model for the semantic  orientaisting methods for phrase classification,  tions of phrases has been proposed by Takamura et  the proposed method can classify phrases  al.(2006).However, their method cannot deal with  consisting of unseen words.We also  prothe words that did not appear in the training data.pose to use unlabeled data for a seed set of  The purpose of this paper is to propose a method for  extracting semantic orientations of phrases, which is  ation shows the effectiveness of the  proapplicable also to expressions consisting of unseen  words.In our method, we regard this task as the  noun classification problem for each adjective; the"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Extracting Semantic Orientations of Words using Spin Model Hiroya Takamura  Precision and Intelligence Laboratory  Tokyo Institute of Technology  have a positive attitude on the topic. The goal of this paper is to propose a method for automatically cre-We propose a method for extracting  seating such a word list from glosses (i.e., definition  mantic orientations of words: desirable  or explanation sentences ) in a dictionary, as well as or undesirable. Regarding semantic ori-from a thesaurus and a corpus. For this purpose, we  entations as spins of electrons, we use  use spin model, which is a model for a set of elec-the mean field approximation to compute  trons with spins. Just as each electron has a  directhe approximate probability function of  tion of spin (up or down), each word has a semantic  the system instead of the intractable  acorientation (positive or negative). We therefore  retual probability function.  We also  progard words as a set of electrons and apply the mean  pose a criterion for parameter selection on  field approximation to compute the average  orientathe basis of magnetization. Given only  tion of each word. We also propose a criterion for  a small number of seed words, the  proparameter selection on the basis of magnetization, a  posed method extracts semantic  orientanotion in statistical physics. Magnetization indicates tions with high accuracy in the exper-the global tendency of polarization.  iments on English lexicon.  The result  We empirically show that the proposed method  is comparable to the best value ever  reworks well even with a small number of seed words.  ported.  Related Work  ",1,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","Precision and Intelligence Laboratory have a positive attitude on the topic. The goal of this paper is to propose a method for automatically cre-We propose a method for extracting  seating such a word list from glosses (i.e., definition  mantic orientations of words: desirable  or explanation sentences ) in a dictionary, as well as or undesirable.Regarding semantic ori-from a thesaurus and a corpus.For this purpose, we  entations as spins of electrons, we use  use spin model, which is a model for a set of elec-the mean field approximation to compute  trons with spins.Just as each electron has a  directhe approximate probability function of  tion of spin (up or down), each word has a semantic  the system instead of the intractable  acorientation (positive or negative).We therefore  retual probability function.We also  progard words as a set of electrons and apply the mean  pose a criterion for parameter selection on  field approximation to compute the average  orientathe basis of magnetization.Given only  tion of each word.We also propose a criterion for  a small number of seed words, the  proparameter selection on the basis of magnetization, a  posed method extracts semantic  orientanotion in statistical physics.Magnetization indicates tions with high accuracy in the exper-the global tendency of polarization.iments on English lexicon.The result  We empirically show that the proposed method  is comparable to the best value ever  reworks well even with a small number of seed words.  ported.Related Work"
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 417-424.  Thumbs Up or Thumbs Down? Semantic Orientation Applied to  Unsupervised Classification of Reviews  Peter D. Turney  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches. It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion. With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down). The  classifisuch summary statistics. This is the motivation for  cation of a review is predicted by the  the research described here. Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs. A phrase has a positive  ing new kinds of search tools (Hearst, 1992).  semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended. The algorithm  bad associations (e.g., very cavalier ). In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output. The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994). The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997). A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive. The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ). The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations). The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view. If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.  Otherwise, the prediction is that the item is not  recommended.  ",0,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","417-424.Thumbs Up or Thumbs Down?Semantic Orientation Applied to  Unsupervised Classification of Reviews  Peter D. Turney  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches.It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion.With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down).The  classifisuch summary statistics.This is the motivation for  cation of a review is predicted by the  the research described here.Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs.A phrase has a positive  ing new kinds of search tools (Hearst, 1992).semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended.The algorithm  bad associations (e.g., very cavalier ).In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output.The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994).The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997).A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive.The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ).The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations).The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view.If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.Otherwise, the prediction is that the item is not  recommended."
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. ",1,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1."
" Opinion Word Expansion and Target Extraction through Double Propagation  Zhejiang University, China  University of Illinois at Chicago  Zhejiang University, China  Zhejiang University, China  Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.  1. "," Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences Hong Yu  Department of Computer Science  Department of Computer Science  Columbia University  Columbia University  New York, NY 10027, USA  New York, NY 10027, USA  trieving only editorials in favor of a particular policy Opinion question answering is a challenging task  for natural language processing. In this paper, we Our motivation for building the opinion detec-discuss a necessary component for an opinion question answering system: separating opinions from  tion and classification system described in this pa-fact, at both the document and sentence level. We per is the need for organizing information in the present a Bayesian classifier for discriminating be-context of question answering for complex  questween documents with a preponderance of opinions  such as editorials from regular news stories, and tions.  Unlike questions like Who was the first  describe three unsupervised, statistical techniques man on the moon? which can be answered with  for the significantly harder task of detecting opin-a simple phrase, more intricate questions such as ions at the sentence level. We also present a first model for classifying opinion sentences as positive  What are the reasons for the US-Iraq war? require or negative in terms of the main perspective be-long answers that must be constructed from  multiing expressed in the opinion. Results from a large ple sources. In such a context, it is imperative that collection of news stories and a human evaluation of 400 sentences are reported, indicating that we the question answering system can discriminate be-achieve very high performance in document  classitween opinions and facts, and either use the appro-fication (upwards of 97% precision and recall), and respectable performance in detecting opinions and priate type depending on the question or combine  classifying them at the sentence level as positive, them in a meaningful presentation. Perspective in-negative, or neutral (up to 91% accuracy).  formation can also help highlight contrasts and con-tradictions between different sources there will be significant disparity in the material collected for the 1  ",0,"Opinion Word Expansion and Target Extraction through Double Propagation Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identi.ed using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.1.","trieving only editorials in favor of a particular policy Opinion question answering is a challenging task for natural language processing. In this paper, we Our motivation for building the opinion detec-discuss a necessary component for an opinion question answering system: separating opinions from  tion and classification system described in this pa-fact, at both the document and sentence level.We per is the need for organizing information in the present a Bayesian classifier for discriminating be-context of question answering for complex  questween documents with a preponderance of opinions  such as editorials from regular news stories, and tions.Unlike questions like Who was the first  describe three unsupervised, statistical techniques man on the moon?which can be answered with  for the significantly harder task of detecting opin-a simple phrase, more intricate questions such as ions at the sentence level.We also present a first model for classifying opinion sentences as positive  What are the reasons for the US-Iraq war?require or negative in terms of the main perspective be-long answers that must be constructed from  multiing expressed in the opinion.Results from a large ple sources.In such a context, it is imperative that collection of news stories and a human evaluation of 400 sentences are reported, indicating that we the question answering system can discriminate be-achieve very high performance in document  classitween opinions and facts, and either use the appro-fication (upwards of 97% precision and recall), and respectable performance in detecting opinions and priate type depending on the question or combine  classifying them at the sentence level as positive, them in a meaningful presentation.Perspective in-negative, or neutral (up to 91% accuracy).formation can also help highlight contrasts and con-tradictions between different sources there will be significant disparity in the material collected for the 1"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al. (1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  ",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","79-86.Association for Computational Linguistics.Thumbs up?Sentiment Classification using Machine Learning Techniques  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.Ithaca, NY 14853 USA  San Jose, CA 95120 USA  use.Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al.(1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  chine learning techniques definitively  outsentiment categorization.Moreover, there are also  perform human-produced baselines.Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem.A challenging aspect of this categorization.We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner.For example, the"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis  Yejin Choi and Claire Cardie  Department of Computer Science  Cornell University  Ithaca, NY 14853  1: [I did [not] have any [doubt] about it.]+  2: [The report [eliminated] my [doubt] .]+  Determining the polarity of a  sentiment3: [They could [not] [eliminate] my [doubt] .]  bearing expression requires more than a  simple bag-of-words approach.  words or constituents within the expression  In the first example, doubt in isolation carries  can interact with each other to yield a  particua negative sentiment, but the overall polarity of the lar overall polarity. In this paper, we view such  sentence is positive because there is a negator not , subsentential interactions in light of composi-which flips the polarity. In the second example, both tional semantics, and present a novel  learning eliminated and doubt carry negative sentiment  based approach that incorporates structural  inin isolation, but the overall polarity of the sentence ference motivated by compositional seman-is positive because eliminated acts as a negator for tics into the learning procedure. Our exper-its argument doubt . In the last example, there are iments show that (1) simple heuristics based  on compositional semantics can perform  beteffectively two negators not and eliminated  ter than learning-based methods that do not  inwhich reverse the polarity of doubt twice,  resultcorporate compositional semantics (accuracy  ing in the negative polarity for the overall sentence.  of 89.7% vs. 89.1%), but (2) a method that  These examples demonstrate that words or  conintegrates compositional semantics into  learnstituents interact with each other to yield the  ing performs better than all other  alternaexpression-level polarity. And a system that  simWe also find that  contentply takes the majority vote of the polarity of  indiword negators , not widely employed in  previous work, play an important role in  devidual words will not work well on the above  examtermining expression-level polarity. Finally,  ples. Indeed, much of the previous learning-based  in contrast to conventional wisdom, we find  research on this topic tries to incorporate salient in-that expression-level classification accuracy  teractions by encoding them as features. One  apuniformly decreases as additional, potentially  proach includes features based on contextual  vadisambiguating, context is considered.  lence shifters1 (Polanyi and Zaenen, 2004), which  are words that affect the polarity or intensity of sen-1  ",0,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis 1: [I did [not] have any [doubt] about it.]+ 2: [The report [eliminated] my [doubt] .]+ Determining the polarity of a sentiment3: [They could [not] [eliminate] my [doubt] .] bearing expression requires more than a simple bag-of-words approach. words or constituents within the expression  In the first example, doubt in isolation carries  can interact with each other to yield a  particua negative sentiment, but the overall polarity of the lar overall polarity.In this paper, we view such  sentence is positive because there is a negator not , subsentential interactions in light of composi-which flips the polarity.In the second example, both tional semantics, and present a novel  learning eliminated and doubt carry negative sentiment  based approach that incorporates structural  inin isolation, but the overall polarity of the sentence ference motivated by compositional seman-is positive because eliminated acts as a negator for tics into the learning procedure.Our exper-its argument doubt . In the last example, there are iments show that (1) simple heuristics based  on compositional semantics can perform  beteffectively two negators not and eliminated  ter than learning-based methods that do not  inwhich reverse the polarity of doubt twice,  resultcorporate compositional semantics (accuracy  ing in the negative polarity for the overall sentence.  of 89.7% vs. 89.1%), but (2) a method that  These examples demonstrate that words or  conintegrates compositional semantics into  learnstituents interact with each other to yield the  ing performs better than all other  alternaexpression-level polarity.And a system that  simWe also find that  contentply takes the majority vote of the polarity of  indiword negators , not widely employed in  previous work, play an important role in  devidual words will not work well on the above  examtermining expression-level polarity.Finally,  ples.Indeed, much of the previous learning-based  in contrast to conventional wisdom, we find  research on this topic tries to incorporate salient in-that expression-level classification accuracy  teractions by encoding them as features.One  apuniformly decreases as additional, potentially  proach includes features based on contextual  vadisambiguating, context is considered.lence shifters1 (Polanyi and Zaenen, 2004), which  are words that affect the polarity or intensity of sen-1"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," An Application to Opinion Mining  cipline that deals with the quantitative and  qualitative analysis of text for the purpose of determining  This paper presents an application of  PageRits opinion-related properties (ORPs). An important  ank, a random-walk model originally  depart of this research has been the work on the  autovised for ranking Web search results, to  matic determination of the ORPs of terms, as e.g.,  ranking WordNet synsets in terms of how  in determining whether an adjective tends to give a  strongly they possess a given semantic  proppositive, a negative, or a neutral nature to the noun  erty. The semantic properties we use for  exphrase it appears in. While many works (Esuli and  emplifying the approach are positivity and  Sebastiani, 2005; Hatzivassiloglou and McKeown,  negativity, two properties of central  importance in sentiment analysis. The idea derives  Turney and Littman, 2003) view the properties of  from the observation that WordNet may be  positivity and negativity as categorical (i.e., a term is seen as a graph in which synsets are con-either positive or it is not), others (Andreevskaia and nected through the binary relation a term  belonging to synset sk occurs in the gloss  Hovy, 2004; Subasic and Huettner, 2001) view them  of synset si , and on the hypothesis that  as graded (i.e., a term may be positive to a certain  this relation may be viewed as a  transmitdegree), with the underlying interpretation varying  ter of such semantic properties. The data  from fuzzy to probabilistic.  for this relation can be obtained from  eXSome authors go a step further and attach these  tended WordNet, a publicly available  senseproperties not to terms but to term senses  (typdisambiguated version of WordNet. We  arically: WordNet synsets), on the assumption that  gue that this relation is structurally akin to  different senses of the same term may have  difthe relation between hyperlinked Web pages,  ferent opinion-related properties (Andreevskaia and  and thus lends itself to PageRank analysis.  We report experimental results supporting  2006; Wiebe and Mihalcea, 2006).  In this paper we contribute to this latter literature  with a novel method for ranking the entire set of  ",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","An Application to Opinion Mining cipline that deals with the quantitative and qualitative analysis of text for the purpose of determining This paper presents an application of PageRits opinion-related properties (ORPs). An important  ank, a random-walk model originally  depart of this research has been the work on the  autovised for ranking Web search results, to  matic determination of the ORPs of terms, as e.g.,  ranking WordNet synsets in terms of how  in determining whether an adjective tends to give a  strongly they possess a given semantic  proppositive, a negative, or a neutral nature to the noun  erty.The semantic properties we use for  exphrase it appears in.While many works (Esuli and  emplifying the approach are positivity and  Sebastiani, 2005; Hatzivassiloglou and McKeown,  negativity, two properties of central  importance in sentiment analysis.The idea derives  Turney and Littman, 2003) view the properties of  from the observation that WordNet may be  positivity and negativity as categorical (i.e., a term is seen as a graph in which synsets are con-either positive or it is not), others (Andreevskaia and nected through the binary relation a term  belonging to synset sk occurs in the gloss  Hovy, 2004; Subasic and Huettner, 2001) view them  of synset si , and on the hypothesis that  as graded (i.e., a term may be positive to a certain  this relation may be viewed as a  transmitdegree), with the underlying interpretation varying  ter of such semantic properties.The data  from fuzzy to probabilistic.for this relation can be obtained from  eXSome authors go a step further and attach these  tended WordNet, a publicly available  senseproperties not to terms but to term senses  (typdisambiguated version of WordNet.We  arically: WordNet synsets), on the assumption that  gue that this relation is structurally akin to  different senses of the same term may have  difthe relation between hyperlinked Web pages,  ferent opinion-related properties (Andreevskaia and  and thus lends itself to PageRank analysis.We report experimental results supporting  2006; Wiebe and Mihalcea, 2006).In this paper we contribute to this latter literature  with a novel method for ranking the entire set of"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Learning to Shift the Polarity of Words for Sentiment Classification Daisuke Ikeda  Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology ikeda@lr.pi.titech.ac.jp  Department of Computer Science, University of Illinois at Urbana-Champaign ratinov2@uiuc.edu  Precision and Intelligence Laboratory, Tokyo Institute of Technology  because this task is fundamental and has a wide  apWe propose a machine learning based  plicability in sentiment analysis. For example, we  method of sentiment classification of  sencan retrieve individuals' opinions that are related to  tences using word-level polarity. The  polaria product and can find whether they have the positive  ties of words in a sentence are not always the  attitude to the product.  same as that of the sentence, because there  There has been much work on the identification of  can be polarity-shifters such as negation  exsentiment polarity of words. For instance,  beautiThe proposed method models  ful is positively oriented, while dirty is negatively the polarity-shifters.  oriented. We use the term sentiment words to refer trained in two different ways: word-wise and  to those words that are listed in a predefined  polarsentence-wise learning.  In sentence-wise  ity dictionary. Sentiment words are a basic resource  learning, the model can be trained so that the  for sentiment analysis and thus believed to have a  prediction of sentence polarities should be  great potential for applications. However, it is still  accurate. The model can also be combined  an open problem how we can effectively use  senwith features used in previous work such  timent words to improve performance of sentiment  as bag-of-words and n-grams. We  empiriclassification of sentences or documents.  cally show that our method almost always  The simplest way for that purpose would be the  improves the performance of sentiment  clasmajority voting by the number of positive words and  sification of sentences especially when we  the number of negative words in the given sentence.  have only small amount of training data.  However, the polarities of words in a sentence are  not always the same as that of the sentence,  be",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","because this task is fundamental and has a wide plicability in sentiment analysis. For example, we  method of sentiment classification of  sencan retrieve individuals' opinions that are related to  tences using word-level polarity.The  polaria product and can find whether they have the positive  ties of words in a sentence are not always the  attitude to the product.same as that of the sentence, because there  There has been much work on the identification of  can be polarity-shifters such as negation  exsentiment polarity of words.For instance,  beautiThe proposed method models  ful is positively oriented, while dirty is negatively the polarity-shifters.  oriented.We use the term sentiment words to refer trained in two different ways: word-wise and  to those words that are listed in a predefined  polarsentence-wise learning.In sentence-wise  ity dictionary.Sentiment words are a basic resource  learning, the model can be trained so that the  for sentiment analysis and thus believed to have a  prediction of sentence polarities should be  great potential for applications.However, it is still  accurate.The model can also be combined  an open problem how we can effectively use  senwith features used in previous work such  timent words to improve performance of sentiment  as bag-of-words and n-grams.We  empiriclassification of sentences or documents.cally show that our method almost always  The simplest way for that purpose would be the  improves the performance of sentiment  clasmajority voting by the number of positive words and  sification of sentences especially when we  the number of negative words in the given sentence.have only small amount of training data.However, the polarities of words in a sentence are  not always the same as that of the sentence,  be"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Crystal: Analyzing Predictive Opinions on the Web Soo-Min Kim and Eduard Hovy  USC Information Sciences Institute  4676 Admiralty Way, Marina del Rey, CA 90292  first category Judgment Opinions and the second Abstract  (those discussing the future) Predictive Opinions.  Judgment opinions express positive or negative  In this paper, we present an election  predicsentiment about a topic such as, for example,  retion system ( Crystal) based on web users'  views about cameras, movies, books, or hotels, and  opinions posted on an election prediction  discussions about topics like abortion and war. In  website. Given a prediction message,  Cryscontrast, predictive opinions express a person's  tal first identifies which party the message  opinion about the future of a topic or event such as  predicts to win and then aggregates  predicthe housing market, a popular sports match, and  tion analysis results of a large amount of  national election, based on his or her belief and  opinions to project the election results. We  knowledge.  collect past election prediction messages  Due to the different nature of these two  categofrom the Web and automatically build a  ries of opinion, each has different valences.  Judggold standard. We focus on capturing  leximent opinions have core valences of positive and  cal patterns that people frequently use  negative. For example, liking a product and  when they express their predictive opinions  supporting abortion have the valence positive  about a coming election. To predict  electoward each topic (namely a product and  abortion results, we apply SVM-based  supertion ). Predictive opinions have the core valence of  vised learning. To improve performance,  likely or unlikely predicated on the event. For ex-we propose a novel technique which  generample, a sentence Housing prices will go down  soon carries the valence of likely for the event  tal results show that Crystal significantly  of housing prices go down .  outperforms several baselines as well as a  The two types of opinions can co-appear. The  non-generalized n-gram approach. Crystal  sentence I like Democrats but I think they are not  predicts future elections with 81.68 %  acculikely to win considering the war issue contains  both types of opinion: positive valence towards  Democrats and unlikely valence towards the  1 ",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","(those discussing the future) Predictive Opinions. Judgment opinions express positive or negative  In this paper, we present an election  predicsentiment about a topic such as, for example,  retion system ( Crystal) based on web users'  views about cameras, movies, books, or hotels, and  opinions posted on an election prediction  discussions about topics like abortion and war.In  website.Given a prediction message,  Cryscontrast, predictive opinions express a person's  tal first identifies which party the message  opinion about the future of a topic or event such as  predicts to win and then aggregates  predicthe housing market, a popular sports match, and  tion analysis results of a large amount of  national election, based on his or her belief and  opinions to project the election results.We  knowledge.collect past election prediction messages  Due to the different nature of these two  categofrom the Web and automatically build a  ries of opinion, each has different valences.Judggold standard.We focus on capturing  leximent opinions have core valences of positive and  cal patterns that people frequently use  negative.For example, liking a product and  when they express their predictive opinions  supporting abortion have the valence positive  about a coming election.To predict  electoward each topic (namely a product and  abortion results, we apply SVM-based  supertion ).Predictive opinions have the core valence of  vised learning.To improve performance,  likely or unlikely predicated on the event.For ex-we propose a novel technique which  generample, a sentence Housing prices will go down  soon carries the valence of likely for the event  tal results show that Crystal significantly  of housing prices go down .  outperforms several baselines as well as a  The two types of opinions can co-appear.The  non-generalized n-gram approach.Crystal  sentence I like Democrats but I think they are not  predicts future elections with 81.68 %  acculikely to win considering the war issue contains  both types of opinion: positive valence towards  Democrats and unlikely valence towards the  1"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables  National Institute of Information and Communications Technology  Tohoku University  Kyoto University  2002), p.168). For example, when a document  contains some domain-specific words, the document  In this paper, we present a dependency  treebased method for sentiment classification of  will probably belong to the domain. However, in  Japanese and English subjective sentences  using conditional random fields with hidden  reversed. For example, let us consider the sentence  Subjective sentences often  con The medicine kills cancer cells. While the phrase  tain words which reverse the sentiment  pocancer cells has negative polarity, the word kills re-larities of other words.  Therefore,  interacverses the polarity, and the whole sentence has  postions between words need to be considered  itive polarity. Thus, in sentiment classification, a  in sentiment classification, which is difficult  sentence which contains positive (or negative)  polarto be handled with simple bag-of-words  approaches, and the syntactic dependency  strucity words does not necessarily have the same  polartures of subjective sentences are exploited in  ity as a whole, and we need to consider interactions  our method. In the method, the sentiment  polarity of each dependency subtree in a  sendently.  tence, which is not observable in training data,  Recently, several methods have been proposed to  is represented by a hidden variable. The  pocope with the problem (Zaenen, 2004; Ikeda et al.,  larity of the whole sentence is calculated in  consideration of interactions between the  hid2008). However, these methods are based on flat  bag-of-features representation, and do not consider  tion is used for inference. Experimental  resyntactic structures which seem essential to infer  sults of sentiment classification for Japanese  the polarity of a whole sentence. Other methods  and English subjective sentences showed that  have been proposed which utilize composition of  the method performs better than other  methsentences (Moilanen and Pulman, 2007; Choi and  ods based on bag-of-features.  Cardie, 2008; Jia et al., 2009), but these methods  use rules to handle polarity reversal, and whether  po",0,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables 2002), p.168). For example, when a document  contains some domain-specific words, the document  In this paper, we present a dependency  treebased method for sentiment classification of  will probably belong to the domain.However, in  Japanese and English subjective sentences  using conditional random fields with hidden  reversed.For example, let us consider the sentence  Subjective sentences often  con The medicine kills cancer cells.While the phrase  tain words which reverse the sentiment  pocancer cells has negative polarity, the word kills re-larities of other words.Therefore,  interacverses the polarity, and the whole sentence has  postions between words need to be considered  itive polarity.Thus, in sentiment classification, a  in sentiment classification, which is difficult  sentence which contains positive (or negative)  polarto be handled with simple bag-of-words  approaches, and the syntactic dependency  strucity words does not necessarily have the same  polartures of subjective sentences are exploited in  ity as a whole, and we need to consider interactions  our method.In the method, the sentiment  polarity of each dependency subtree in a  sendently.tence, which is not observable in training data,  Recently, several methods have been proposed to  is represented by a hidden variable.The  pocope with the problem (Zaenen, 2004; Ikeda et al.,  larity of the whole sentence is calculated in  consideration of interactions between the  hid2008).However, these methods are based on flat  bag-of-features representation, and do not consider  tion is used for inference.Experimental  resyntactic structures which seem essential to infer  sults of sentiment classification for Japanese  the polarity of a whole sentence.Other methods  and English subjective sentences showed that  have been proposed which utilize composition of  the method performs better than other  methsentences (Moilanen and Pulman, 2007; Choi and  ods based on bag-of-features.Cardie, 2008; Jia et al., 2009), but these methods  use rules to handle polarity reversal, and whether  po"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales  Bo Pang and Lillian Lee  (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract  become aware of the scientific challenges posed and  the scope of new applications enabled by the  proWe address the rating-inference problem,  cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.) Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ). This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivassiloglou, 2003). But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.  the task.  Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.  We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.  mission is mediocre (middling rating). Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.  be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1 ",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales become aware of the scientific challenges posed and the scope of new applications enabled by the proWe address the rating-inference problem, cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.)Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ).This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivassiloglou, 2003).But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.the task.Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.mission is mediocre (middling rating).Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts  Bo Pang and Lillian Lee  Department of Computer Science  Cornell University  Ithaca, NY 14853-7501  ter; and then (2) apply a standard machine-learning Sentiment analysis seeks to identify the view-classifier to the resulting extract. This can prevent point(s) underlying a text span; an example appli-the polarity classifier from considering irrelevant or cation is classifying a movie review as thumbs up  even potentially misleading text: for example, al-or thumbs down . To determine this sentiment po-though the sentence The protagonist tries to pro-larity, we propose a novel machine-learning method tect her good name contains the word good , it that applies text-categorization techniques to just tells us nothing about the author's opinion and in the subjective portions of the document. Extracting fact could well be embedded in a negative movie these portions can be implemented using efficient review. Also, as mentioned above, subjectivity ex-techniques for finding minimum cuts in graphs; this tracts can be provided to users as a summary of the greatly facilitates incorporation of cross-sentence sentiment-oriented content of the document.  Our results show that the subjectivity extracts we create accurately represent the sentiment in-Publication info: Proceedings of the ACL, 2004.  formation of the originating documents in a much 1 ",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts ter; and then (2) apply a standard machine-learning Sentiment analysis seeks to identify the view-classifier to the resulting extract. This can prevent point(s) underlying a text span; an example appli-the polarity classifier from considering irrelevant or cation is classifying a movie review as thumbs up  even potentially misleading text: for example, al-or thumbs down . To determine this sentiment po-though the sentence The protagonist tries to pro-larity, we propose a novel machine-learning method tect her good name contains the word good , it that applies text-categorization techniques to just tells us nothing about the author's opinion and in the subjective portions of the document.Extracting fact could well be embedded in a negative movie these portions can be implemented using efficient review.Also, as mentioned above, subjectivity ex-techniques for finding minimum cuts in graphs; this tracts can be provided to users as a summary of the greatly facilitates incorporation of cross-sentence sentiment-oriented content of the document.Our results show that the subjectivity extracts we create accurately represent the sentiment in-Publication info: Proceedings of the ACL, 2004.formation of the originating documents in a much 1"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Multiple Aspect Ranking using the Good Grief Algorithm Benjamin Snyder and Regina Barzilay  Computer Science and Artificial Intelligence Laboratory  Massachusetts Institute of Technology  restaurant. Rather than lumping these aspects into a  single score, we would like to capture each aspect of  We address the problem of analyzing  multhe writer's opinion separately, thereby providing a  tiple related opinions in a text. For  inmore fine-grained view of opinions in the review.  stance, in a restaurant review such  opinTo this end, we aim to predict a set of numeric  ions may include food, ambience and  serranks that reflects the user's satisfaction for each  asvice. We formulate this task as a multiple  pect. In the example above, we would assign a  nuaspect ranking problem, where the goal is  meric rank from 1-5 for each of: food quality,  serto produce a set of numerical scores, one  vice, and ambience.  for each aspect. We present an algorithm  A straightforward approach to this task would be  that jointly learns ranking models for  into rank1 the text independently for each aspect,  usdividual aspects by modeling the  depening standard ranking techniques such as regression  dencies between assigned ranks. This  alor classification. However, this approach fails to  exgorithm guides the prediction of  individual rankers by analyzing meta-relations  ments across different aspects. Knowledge of these  between opinions, such as agreement and  contrast. We prove that our  agreementranks, as a user's opinions on one aspect can  influbased joint model is more expressive than  ence his or her opinions on others.  The algorithm presented in this paper models  results further confirm the strength of the  the dependencies between different labels via the  model: the algorithm provides significant  agreement relation. The agreement relation captures improvement over both individual rankers  whether the user equally likes all aspects of the item  and a state-of-the-art joint ranking model.  or whether he or she expresses different degrees of  satisfaction. Since this relation can often be  deter1 ",0,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","restaurant. Rather than lumping these aspects into a  single score, we would like to capture each aspect of  We address the problem of analyzing  multhe writer's opinion separately, thereby providing a  tiple related opinions in a text.For  inmore fine-grained view of opinions in the review.stance, in a restaurant review such  opinTo this end, we aim to predict a set of numeric  ions may include food, ambience and  serranks that reflects the user's satisfaction for each  asvice.We formulate this task as a multiple  pect.In the example above, we would assign a  nuaspect ranking problem, where the goal is  meric rank from 1-5 for each of: food quality,  serto produce a set of numerical scores, one  vice, and ambience.for each aspect.We present an algorithm  A straightforward approach to this task would be  that jointly learns ranking models for  into rank1 the text independently for each aspect,  usdividual aspects by modeling the  depening standard ranking techniques such as regression  dencies between assigned ranks.This  alor classification.However, this approach fails to  exgorithm guides the prediction of  individual rankers by analyzing meta-relations  ments across different aspects.Knowledge of these  between opinions, such as agreement and  contrast.We prove that our  agreementranks, as a user's opinions on one aspect can  influbased joint model is more expressive than  ence his or her opinions on others.The algorithm presented in this paper models  results further confirm the strength of the  the dependencies between different labels via the  model: the algorithm provides significant  agreement relation.The agreement relation captures improvement over both individual rankers  whether the user equally likes all aspects of the item  and a state-of-the-art joint ranking model.  or whether he or she expresses different degrees of  satisfaction.Since this relation can often be  deter1"
" Semi-Supervised Recursive Autoencoders  for Predicting Sentiment Distributions  Jeffrey Pennington  Christopher D. Manning  Computer Science Department, Stanford University, Stanford, CA 94305, USA  SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA richard@socher.org  Predicted  We introduce a novel machine learning  frameSorry, Hugs You Rock Teehee I Understand Wow, Just Wow work based on recursive autoencoders for  Distribution  sentence-level prediction of sentiment label  distributions. Our method learns vector space  Representations  representations for multi-word phrases.  Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules. We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases. Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue). Then they are recursively  on confessions from the experience project.  merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation. The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.  gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.  manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules). This limits the  applicability of these methods to a broader range of  "," Word representations:  A simple and general method for semi-supervised learning Joseph Turian  Department of  Computer Science  University of Illinois at  already been induced plug these word features  into an existing system, and observe a significant If we take an existing supervised NLP sys-increase in accuracy. But which word features are tem, a simple and general way to improve  good for what tasks? Should we prefer certain  accuracy is to use unsupervised word  word features? Can we combine them?  representations as extra word features. We  A word representation is a mathematical object  evaluate Brown clusters, Collobert and  associated with each word, often a vector. Each  Weston (2008) embeddings, and HLBL  dimension's value corresponds to a feature and  might even have a semantic or grammatical  of words on both NER and chunking.  interpretation, so we call it a word feature.  We use near state-of-the-art supervised  Conventionally, supervised lexicalized NLP  apbaselines, and find that each of the three  proaches take a word and convert it to a symbolic word representations improves the accu-ID, which is then transformed into a feature vector racy of these baselines. We find further  using a one-hot representation: The feature vector improvements by combining different  has the same length as the size of the vocabulary, word representations. You can download  and only one dimension is on.  However, the  our word features, for off-the-shelf use  one-hot representation of a word suffers from data in existing NLP systems, as well as our  sparsity: Namely, for words that are rare in the code,  labeled training data, their corresponding model com/projects/wordreprs/  parameters will be poorly estimated. Moreover,  at test time, the model cannot handle words that 1  ",1,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions Predicted We introduce a novel machine learning Distribution sentence-level prediction of sentiment label distributions. Our method learns vector space  Representations  representations for multi-word phrases.Indices  sentiment prediction tasks these  represeni walked into a parked car Words  tations outperform other state-of-the-art  approaches on commonly used datasets, such as  movie reviews, without using any pre-defined  Figure 1: Illustration of our recursive autoencoder  archisentiment lexica or polarity shifting rules.We  tecture which learns semantic vector representations of  also evaluate the model's ability to predict  phrases.Word indices (orange) are first mapped into a  sentiment distributions on a new dataset based  semantic vector space (blue).Then they are recursively  on confessions from the experience project.merged by the same autoencoder network into a fixed  The dataset consists of personal user stories  length sentence representation.The vectors at each node  annotated with multiple labels which, when  are used as features to predict a distribution over  sentithat captures emotional reactions.gorithm can more accurately predict  distri2010) that can capture such phenomena use many  butions over such labels compared to several  competitive baselines.manually constructed resources (sentiment lexica,  parsers, polarity-shifting rules).This limits the  applicability of these methods to a broader range of","Word representations: Department of Computer Science already been induced plug these word features But which word features are tem, a simple and general way to improve  good for what tasks?Should we prefer certain  accuracy is to use unsupervised word  word features?Can we combine them?representations as extra word features.We  A word representation is a mathematical object  evaluate Brown clusters, Collobert and  associated with each word, often a vector.Each  Weston (2008) embeddings, and HLBL  dimension's value corresponds to a feature and  might even have a semantic or grammatical  of words on both NER and chunking.interpretation, so we call it a word feature.We use near state-of-the-art supervised  Conventionally, supervised lexicalized NLP  apbaselines, and find that each of the three  proaches take a word and convert it to a symbolic word representations improves the accu-ID, which is then transformed into a feature vector racy of these baselines.We find further  using a one-hot representation: The feature vector improvements by combining different  has the same length as the size of the vocabulary, word representations.You can download  and only one dimension is on.However, the  our word features, for off-the-shelf use  one-hot representation of a word suffers from data in existing NLP systems, as well as our  sparsity: Namely, for words that are rare in the code,  labeled training data, their corresponding model com/projects/wordreprs/  parameters will be poorly estimated.Moreover,  at test time, the model cannot handle words that 1"
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Distributional Memory: A General  Framework for Corpus-Based Semantics  University of Trento  University of Pisa  Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this one task, one model approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.  1. ",0,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Distributional Memory: A General Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this one task, one model approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.1."
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," A Structured Vector Space Model for Word Meaning in Context Katrin Erk  Department of Linguistics  Department of Linguistics  University of Texas at Austin  Stanford University  all its possible usages (Landauer and Dumais, 1997; Lund and Burgess, 1996). Since the meaning of  We address the task of computing vector space  words can vary substantially between occurrences  representations for the meaning of word  oc(e.g., for polysemous words), the next necessary step currences, which can vary widely according to  is to characterize the meaning of individual words in context. This task is a crucial step towards a  robust, vector-based compositional account of  sentence meaning. We argue that existing  modThere have been several approaches in the  literels for this task do not take syntactic structure  sufficiently into account.  2001; McDonald and Brew, 2004; Mitchell and  LaWe present a novel structured vector space  pata, 2008) that compute meaning in context from  model that addresses these issues by  incorpolemma vectors. Most of these studies phrase the prob-rating the selectional preferences for words'  lem as one of vector composition: The meaning of a argument positions. This makes it possible to  target occurrence a in context b is a single new vector integrate syntax into the computation of word  c that is a function (for example, the centroid) of the meaning in context. In addition, the model per-vectors: c = a b.  forms at and above the state of the art for  modeling the contextual adequacy of paraphrases.  The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of  catch combined with ball is similar to grab, while in 1  ",1,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","A Structured Vector Space Model for Word Meaning in Context Katrin Erk Since the meaning of  We address the task of computing vector space  words can vary substantially between occurrences  representations for the meaning of word  oc(e.g., for polysemous words), the next necessary step currences, which can vary widely according to  is to characterize the meaning of individual words in context.This task is a crucial step towards a  robust, vector-based compositional account of  sentence meaning.We argue that existing  modThere have been several approaches in the  literels for this task do not take syntactic structure  sufficiently into account.2001; McDonald and Brew, 2004; Mitchell and  LaWe present a novel structured vector space  pata, 2008) that compute meaning in context from  model that addresses these issues by  incorpolemma vectors.Most of these studies phrase the prob-rating the selectional preferences for words'  lem as one of vector composition: The meaning of a argument positions.This makes it possible to  target occurrence a in context b is a single new vector integrate syntax into the computation of word  c that is a function (for example, the centroid) of the meaning in context.In addition, the model per-vectors: c = a b.  forms at and above the state of the art for  modeling the contextual adequacy of paraphrases.The context b can consist of as little as one word, as shown in Example (1).In (1a), the meaning of  catch combined with ball is similar to grab, while in 1"
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Experimental Support for a Categorical Compositional Distributional Model of Meaning  University of Oxford  University of Oxford  Department of Computer Science  Department of Computer Science  Wolfson Building, Parks Road  Wolfson Building, Parks Road  Oxford OX1 3QD, UK  Oxford OX1 3QD, UK  tences. Discovering the process of meaning  assignment in natural language is among the most  Modelling compositional meaning for  senchallenging and foundational questions of  linguistences using empirical distributional methods  has been a challenge for computational  lintics and computer science. The findings thereof will guists. We implement the abstract categorical  increase our understanding of cognition and intelli-model of Coecke et al. (2010) using data from  gence and shall assist in applications to automating the BNC and evaluate it. The implementation  language-related tasks such as document search.  is based on unsupervised learning of matrices  Compositional type-logical approaches  (Monfor relational words and applying them to the  tague, 1974; Lambek, 2008) and distributional  modvectors of their arguments. The evaluation is  based on the word disambiguation task  develels of lexical semantics (Schutze, 1998; Firth, 1957) oped by Mitchell and Lapata (2008) for intran-have provided two partial orthogonal solutions to the sitive sentences, and on a similar new experi-question. Compositional formal semantic models  stem from classical ideas from mathematical logic,  model matches the results of its competitors  mainly Frege's principle that the meaning of a  senin the first experiment, and betters them in the  tence is a function of the meaning of its parts (Frege, second. The general improvement in results  1892). Distributional models are more recent and  with increase in syntactic complexity  showcases the compositional power of our model.  can be related to Wittgenstein's later philosophy of  meaning is use', whereby meanings of words can be  determined from their context (Wittgenstein, 1953).  1 ",1,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Experimental Support for a Categorical Compositional Distributional Model of Meaning tences. Discovering the process of meaning  assignment in natural language is among the most  Modelling compositional meaning for  senchallenging and foundational questions of  linguistences using empirical distributional methods  has been a challenge for computational  lintics and computer science.The findings thereof will guists.We implement the abstract categorical  increase our understanding of cognition and intelli-model of Coecke et al.(2010) using data from  gence and shall assist in applications to automating the BNC and evaluate it.The implementation  language-related tasks such as document search.  is based on unsupervised learning of matrices  Compositional type-logical approaches  (Monfor relational words and applying them to the  tague, 1974; Lambek, 2008) and distributional  modvectors of their arguments.The evaluation is  based on the word disambiguation task  develels of lexical semantics (Schutze, 1998; Firth, 1957) oped by Mitchell and Lapata (2008) for intran-have provided two partial orthogonal solutions to the sitive sentences, and on a similar new experi-question.Compositional formal semantic models  stem from classical ideas from mathematical logic,  model matches the results of its competitors  mainly Frege's principle that the meaning of a  senin the first experiment, and betters them in the  tence is a function of the meaning of its parts (Frege, second. The general improvement in results  1892).Distributional models are more recent and  with increase in syntactic complexity  showcases the compositional power of our model.can be related to Wittgenstein's later philosophy of  meaning is use', whereby meanings of words can be  determined from their context (Wittgenstein, 1953).1"
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Multi-Step Regression Learning for Compositional Distributional  University of Oxford Department of Computer Science  University of Trento Center for Mind/Brain Sciences  University of Tokyo Department of Computer Science  We present a model for compositional distributional semantics related to the framework of  Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of  Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also  renders it suitable for solving more subtle problems compositional distributional models might face.  ",0,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Multi-Step Regression Learning for Compositional Distributional We present a model for compositional distributional semantics related to the framework of (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors.We introduce a new learning method for tensors, generalising the approach of  Baroni and Zamparelli (2010).We evaluate it on two benchmark data sets, and find it to outperform existing leading methods.We argue in our analysis that the nature of this learning method also  renders it suitable for solving more subtle problems compositional distributional models might face."
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Improving Word Representations via Global Context and Multiple Word Prototypes  Eric H. Huang, Richard Socher , Christopher D. Manning, Andrew Y. Ng Computer Science Department, Stanford University, Stanford, CA 94305, USA  {ehhuang,manning,ang}@stanford.edu, richard@socher.org Abstract  Despite their usefulness, most VSMs share a  common problem that each word is only  repreUnsupervised word representations are very  sented with one vector, which clearly fails to capture useful in NLP tasks both as inputs to learning  homonymy and polysemy. Reisinger and Mooney  algorithms and as extra word features in NLP  (2010b) introduced a multi-prototype VSM where  systems. However, most of these models are  word sense discrimination is first applied by clus-built with only local context and one  representering contexts, and then prototypes are built using tation per word. This is problematic because  words are often polysemous and global  conthe contexts of the sense-labeled words. However, in text can also provide useful information for  order to cluster accurately, it is important to capture learning word meanings. We present a new  both the syntax and semantics of words. While many neural network architecture which 1) learns  approaches use local contexts to disambiguate word word embeddings that better capture the se-meaning, global contexts can also provide useful  mantics of words by incorporating both local  topical information (Ng and Zelle, 1997). Several and global document context, and 2) accounts  studies in psychology have also shown that global for homonymy and polysemy by learning multiple embeddings per word. We introduce a  new dataset with human judgments on pairs of  words in sentential context, and evaluate our  We introduce a new neural-network-based  lanmodel on it, showing that our model  outperguage model that distinguishes and uses both local forms competitive baselines and other neural  and global context via a joint training objective. The language models. 1  model learns word representations that better capture the semantics of words, while still keeping syn-1  ",1,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Improving Word Representations via Global Context and Multiple Word Prototypes {ehhuang,manning,ang}@stanford.edu, richard@socher.org Abstract Despite their usefulness, most VSMs share a common problem that each word is only repreUnsupervised word representations are very homonymy and polysemy. Reisinger and Mooney  algorithms and as extra word features in NLP  (2010b) introduced a multi-prototype VSM where  systems.However, most of these models are  word sense discrimination is first applied by clus-built with only local context and one  representering contexts, and then prototypes are built using tation per word.This is problematic because  words are often polysemous and global  conthe contexts of the sense-labeled words.However, in text can also provide useful information for  order to cluster accurately, it is important to capture learning word meanings.We present a new  both the syntax and semantics of words.While many neural network architecture which 1) learns  approaches use local contexts to disambiguate word word embeddings that better capture the se-meaning, global contexts can also provide useful  mantics of words by incorporating both local  topical information (Ng and Zelle, 1997).Several and global document context, and 2) accounts  studies in psychology have also shown that global for homonymy and polysemy by learning multiple embeddings per word.We introduce a  new dataset with human judgments on pairs of  words in sentential context, and evaluate our  We introduce a new neural-network-based  lanmodel on it, showing that our model  outperguage model that distinguishes and uses both local forms competitive baselines and other neural  and global context via a joint training objective.The language models.1  model learns word representations that better capture the semantics of words, while still keeping syn-1"
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Accurate Unlexicalized Parsing  Christopher D. Manning  Computer Science Department  Computer Science Department  Stanford University  Stanford University  Stanford, CA 94305-9040  Stanford, CA 94305-9040  mance of an unlexicalized PCFG over the Penn  treebank could be improved enormously simply by  anWe demonstrate that an unlexicalized PCFG can  notating each node by its parent category. The Penn  parse much more accurately than previously shown,  treebank covering PCFG is a poor tool for parsing  beby making use of simple, linguistically motivated  state splits, which break down false independence  cause the context-freedom assumptions it embodies  assumptions latent in a vanilla treebank grammar.  are far too strong, and weakening them in this way  Indeed, its performance of 86.36% (LP/LR F1) is  makes the model much better. More recently, Gildea  better than that of early lexicalized PCFG models, (2001) discusses how taking the bilexical probabil-and surprisingly close to the current  state-of-theart. This result has potential uses beyond  establishities out of a good current lexicalized PCFG parser  ing a strong lower bound on the maximum  possihurts performance hardly at all: by at most 0.5% for  ble accuracy of unlexicalized models: an  unlexicaltest text from the same domain as the training data,  ized PCFG is much more compact, easier to  repliand not at all for test text from a different domain.1  cate, and easier to interpret than more complex  lexical models, and the parsing algorithms are simpler,  But it is precisely these bilexical dependencies that  more widely understood, of lower asymptotic  combacked the intuition that lexicalized PCFGs should be  plexity, and easier to optimize.  very successful, for example in Hindle and Rooth's  demonstration from PP attachment. We take this as a  In the early 1990s, as probabilistic methods swept  reflection of the fundamental sparseness of the  lexNLP, parsing work revived the investigation of  probical dependency information available in the Penn  abilistic context-free grammars (PCFGs) (Booth and  Treebank. As a speech person would say, one  milThomson, 1973; Baker, 1979). However, early  relion words of training data just isn't enough. Even  sults on the utility of PCFGs for parse  disambiguafor topics central to the treebank's Wall Street  Jourtion and language modeling were somewhat  disapnal text, such as stocks, many very plausible depen-pointing. A conviction arose that lexicalized PCFGs dencies occur only once, for example stocks  stabi(where head words annotate phrasal nodes) were  lized, while many others occur not at all, for exam-the key tool for high performance PCFG parsing.  This approach was congruent with the great success  The best-performing lexicalized PCFGs have  inof word n-gram models in speech recognition, and  creasingly made use of subcategorization 3 of the  drew strength from a broader interest in lexicalized  1There are minor differences, but all the current best-known grammars, as well as demonstrations that lexical de-lexicalized PCFGs employ both monolexical statistics, which pendencies were a key tool for resolving ambiguities  describe the phrasal categories of arguments and adjuncts that such as PP attachments (Ford et al., 1982; Hindle and  appear around a head lexical item, and bilexical statistics, or de-Rooth, 1993). In the following decade, great success  pendencies, which describe the likelihood of a head word taking as a dependent a phrase headed by a certain other word.  in terms of parse disambiguation and even language  2This observation motivates various or  similaritymodeling was achieved by various lexicalized PCFG  based approaches to combating sparseness, and this remains a models (Magerman, 1995; Charniak, 1997; Collins,  promising avenue of work, but success in this area has proven 1999; Charniak, 2000; Charniak, 2001).  somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpolate However, several results have brought into ques-with syntactic category-based estimates when they are not.  tion how large a role lexicalization plays in such  3In this paper we use the term subcategorization in the origi-parsers.  Johnson (1998) showed that the  perfornal general sense of Chomsky (1965), for where a syntactic  catcategories appearing in the Penn treebank. Charniak  constants. An unlexicalized PCFG parser is much  (2000) shows the value his parser gains from  parentsimpler to build and optimize, including both  stanannotation of nodes, suggesting that this  informadard code optimization techniques and the  investigation is at least partly complementary to information  tion of methods for search space pruning (Caraballo  derivable from lexicalization, and Collins (1999)  and Charniak, 1998; Charniak et al., 1998).  uses a range of linguistically motivated and  careIt is not our goal to argue against the use of lex-fully hand-engineered subcategorizations to break  icalized probabilities in high-performance  probabidown wrong context-freedom assumptions of the  listic parsing. It has been comprehensively  demonnaive Penn treebank covering PCFG, such as  differstrated that lexical dependencies are useful in  reentiating base NPs from noun phrases with phrasal  solving major classes of sentence ambiguities, and a  modifiers, and distinguishing sentences with empty  parser should make use of such information where  subjects from those where there is an overt subject  We focus here on using unlexicalized,  NP. While he gives incomplete experimental results  structural context because we feel that this  inforas to their efficacy, we can assume that these features  mation has been underexploited and  underappreciwere incorporated because of beneficial effects on  ated. We see this investigation as only one part of  parsing that were complementary to lexicalization.  the foundation for state-of-the-art parsing which  emIn this paper, we show that the parsing  perforploys both lexical and structural conditioning.  mance that can be achieved by an unlexicalized  PCFG is far higher than has previously been  demonstrated, and is, indeed, much higher than community  To facilitate comparison with previous work, we  wisdom has thought possible. We describe several  trained our models on sections 2 21 of the WSJ  secsimple, linguistically motivated annotations which  tion of the Penn treebank. We used the first 20 files  do much to close the gap between a vanilla PCFG  (393 sentences) of section 22 as a development set  and state-of-the-art lexicalized models. Specifically,  ( devset). This set is small enough that there is no-we construct an unlexicalized PCFG which  outperticeable variance in individual results, but it allowed  forms the lexicalized PCFGs of Magerman (1995)  rapid search for good features via continually  reparsand Collins (1996) (though not more recent models,  ing the devset in a partially manual hill-climb. All of  such as Charniak (1997) or Collins (1999)).  section 23 was used as a test set for the final model.  One benefit of this result is a much-strengthened  For each model, input trees were annotated or  translower bound on the capacity of an unlexicalized  formed in some way, as in Johnson (1998). Given  PCFG. To the extent that no such strong baseline has  a set of transformed trees, we viewed the local trees  been provided, the community has tended to greatly  as grammar rewrite rules in the standard way, and  overestimate the beneficial effect of lexicalization in  probabilistic parsing, rather than looking critically  for rule probabilities.5 To parse the grammar, we  at where lexicalized probabilities are both needed to used a simple array-based Java implementation of  make the right decision and available in the training a generalized CKY parser, which, for our final best  data. Secondly, this result affirms the value of  linmodel, was able to exhaustively parse all sentences  guistic analysis for feature discovery. The result has  in section 23 in 1GB of memory, taking  approxiother uses and advantages: an unlexicalized PCFG is  mately 3 sec for average length sentences.6  easier to interpret, reason about, and improve than  the more complex lexicalized models. The grammar  The tagging probabilities were smoothed to accommodate  unknown words.  The quantity P( t ag|w or d) was estimated representation is much more compact, no longer re-as follows: words were split into one of several categories quiring large structures that store lexicalized proba-w or dclass, based on capitalization, suffix, digit, and other bilities. The parsing algorithms have lower asymp-character features. For each of these categories, we took the totic complexity4 and have much smaller grammar  maximum-likelihood estimate of P( t ag|w or dclass). This distribution was used as a prior against which observed taggings, if any, were taken, giving P( t ag|w or d) = [ c( t ag, w or d) +  egory is divided into several subcategories, for example  divid P( t ag|w or dclass)]/[ c(w or d)+ ]. This was then inverted to ing verb phrases into finite and non-finite verb phrases, rather give P(w or d| t ag). The quality of this tagging model impacts than in the modern restricted usage where the term refers only all numbers; for example the raw treebank grammar's devset F1  to the syntactic argument frames of predicators.  is 72.62 with it and 72.09 without it.  4 O( n 3) vs. O( n 5) for a naive implementation, or vs. O( n 4) 6The parser is available for download as open source at:  if using the clever approach of Eisner and Satta (1999).  Figure 1: The v=1, h=1 markovization of VP VBZ NP PP.  Figure 2: Markovizations: F1 and grammar size.  The traditional starting point for unlexicalized  parsing is the raw n-ary treebank grammar read from  child always matters). It is a historical accident that  training trees (after removing functional tags and  the default notion of a treebank PCFG grammar takes  null elements). This basic grammar is imperfect in  v = 1 (only the current node matters vertically) and  two well-known ways. First, the category symbols  h = (rule right hand sides do not decompose at  are too coarse to adequately render the expansions  all). On this view, it is unsurprising that increasing  independent of the contexts. For example, subject  v and decreasing h have historically helped.  NP expansions are very different from object NP  exAs an example, consider the case of v = 1,  pansions: a subject NP is 8.7 times more likely than  h = 1. If we start with the rule VP VBZ NP  an object NP to expand as just a pronoun. Having  PP PP, it will be broken into several stages, each a  separate symbols for subject and object NPs allows  binary or unary rule, which conceptually represent  this variation to be captured and used to improve  a head-outward generation of the right hand size, as  parse scoring. One way of capturing this kind of  shown in figure 1. The bottom layer will be a unary  external context is to use parent annotation, as pre-over the head declaring the goal: hVP: [VBZ]i  sented in Johnson (1998). For example, NPs with S  VBZ. The square brackets indicate that the VBZ is  parents (like subjects) will be marked NP S, while  the head, while the angle brackets hXi indicates that  NPs with VP parents (like objects) will be NP VP.  the symbol hXi is an intermediate symbol  (equivThe second basic deficiency is that many rule  alently, an active or incomplete state).  The next  types have been seen only once (and therefore have  layer up will generate the first rightward sibling of  their probabilities overestimated), and many rules  the head child: hVP: [VBZ]. . . NPi hVP: [VBZ]i  which occur in test sentences will never have been  NP. Next, the PP is generated: hVP: [VBZ]. . . PPi  seen in training (and therefore have their  probabilihVP: [VBZ]. . . NPi PP. We would then branch off left  ties underestimated see Collins (1999) for  analysiblings if there were any.7 Finally, we have another  sis). Note that in parsing with the unsplit grammar,  unary to finish the VP. Note that while it is  convenient to think of this as a head-outward process,  failure, but rather a possibly very weird parse  (Charthese are just PCFG rewrites, and so the actual scores  niak, 1996). One successful method of combating  attached to each rule will correspond to a downward  sparsity is to markovize the rules (Collins, 1999). In generation order.  particular, we follow that work in markovizing out  Figure 2 presents a grid of horizontal and  vertifrom the head child, despite the grammar being  uncal markovizations of the grammar. The raw  treelexicalized, because this seems the best way to  capbank grammar corresponds to v = 1, h = (the  ture the traditional linguistic insight that phrases are  upper right corner), while the parent annotation in  organized around a head (Radford, 1988).  (Johnson, 1998) corresponds to v = 2, h = , and  Both parent annotation (adding context) and RHS  the second-order model in Collins (1999), is broadly  markovization (removing it) can be seen as two  ina smoothed version of v = 2, h = 2. In  addistances of the same idea. In parsing, every node has  tion to exact n th-order models, we tried  variablea vertical history, including the node itself, parent,  grandparent, and so on. A reasonable assumption is  7In our system, the last few right children carry over as pre-that only the past v vertical ancestors matter to the  ceding context for the left children, distinct from common practice. We found this wrapped horizon to be beneficial, and it current expansion. Similarly, only the previous h  also unifies the infinite order model with the unmarkovized raw horizontal ancestors matter (we assume that the head  ROOT  Revenue was  CD  CD  including  CONJP  SPLIT-AUX  SPLIT-CC  down slightly from $  CD  CD  Figure 4: An error which can be resolved with the  UNARYINTERNAL annotation (incorrect baseline parse shown).  grammar. Although it does not necessarily jump out  RIGHT-REC-NP  of the grid at first glance, this point represents the  best compromise between a compact grammar and  Figure 3: Size and devset performance of the cumulatively an-useful markov histories.  notated models, starting with the markovized baseline. The  right two columns show the change in F1 from the baseline for each annotation introduced, both cumulatively and for each sin-3  External vs. Internal Annotation  gle annotation applied to the baseline in isolation.  The two major previous annotation strategies,  parent annotation and head lexicalization, can be seen  history models similar in intent to those described  as instances of external and internal annotation,  rein Ron et al. (1994). For variable horizontal  hisspectively.  Parent annotation lets us indicate an  important feature of the external environment of a  occurrences of a symbol. For example, if the symbol  node which influences the internal expansion of that  hVP: [VBZ]. . . PP PPi were too rare, we would  colnode. On the other hand, lexicalization is a  (radilapse it to hVP: [VBZ]. . . PPi. For vertical histories,  cal) method of marking a distinctive aspect of the  we used a cutoff which included both frequency and  otherwise hidden internal contents of a node which  mutual information between the history and the  exinfluence the external distribution. Both kinds of  anpansions (this was not appropriate for the horizontal  notation can be useful. To identify split states, we  case because MI is unreliable at such low counts).  add suffixes of the form -X to mark internal content  Figure 2 shows parsing accuracies as well as the  features, and X to mark external features.  number of symbols in each markovization. These  To illustrate the difference, consider unary  prosymbol counts include all the intermediate states  ductions. In the raw grammar, there are many  unarwhich represent partially completed constituents.  ies, and once any major category is constructed over  The general trend is that, in the absence of further  a span, most others become constructible as well  usannotation, more vertical annotation is better even  ing unary chains (see Klein and Manning (2001) for  exhaustive grandparent annotation. This is not true  discussion). Such chains are rare in real treebank  for horizontal markovization, where the  variabletrees: unary rewrites only appear in very specific  order second-order model was superior. The best  contexts, for example S complements of verbs where  entry, v = 3, h 2, has an F1 of 79.74, already  the S has an empty, controlled subject. Figure 4  a substantial improvement over the baseline.  shows an erroneous output of the parser, using the  In the remaining sections, we discuss other  anbaseline markovized grammar. Intuitively, there are  notations which increasingly split the symbol space.  several reasons this parse should be ruled out, but  Since we expressly do not smooth the grammar, not  one is that the lower S slot, which is intended  priall splits are guaranteed to be beneficial, and not all  marily for S complements of communication verbs,  sets of useful splits are guaranteed to co-exist well.  is not a unary rewrite position (such complements  In particular, while v = 3, h 2 markovization is  usually have subjects). It would therefore be natural  good on its own, it has a large number of states and  to annotate the trees so as to confine unary  producdoes not tolerate further splitting well. Therefore,  tions to the contexts in which they are actually  apwe base all further exploration on the v 2, h 2  propriate. We tried two annotations. First,  UNARYINTERNAL marks (with a -U) any nonterminal node  which has only one child. In isolation, this resulted  in an absolute gain of 0.55% (see figure 3). The  to VB  to  same sentence, parsed using only the baseline and  UNARY-INTERNAL, is parsed correctly, because the  VP rewrite in the incorrect parse ends with an S  VPU with very low probability.8  Alternately, UNARY-EXTERNAL, marked nodes  which had no siblings with  It was similar to  Figure 5: An error resolved with the TAG-PA annotation (of the but provided far less marginal benefit on top of  IN tag): (a) the incorrect baseline parse and (b) the correct TAG-other later features (none at all on top of  UNARYPA parse. SPLIT-IN also resolves this error.  INTERNAL for our top models), and was discarded.9  One restricted place where external unary  annotasomewhat regularly occurs in a non-canonical  position was very useful, however, was at the  pretermition, its distribution is usually distinct. For example,  nal level, where internal annotation was  meaningthe most common adverbs directly under ADVP are  less. One distributionally salient tag conflation in  also (1599) and now (544). Under VP, they are n't the Penn treebank is the identification of  demonstra(3779) and not (922). Under NP, only (215) and just tives ( that, those) and regular determiners ( the, a).  (132), and so on. TAG-PA brought F  Splitting  DT tags based on whether they were only  tially, to 80.62%.  children (UNARY-DT) captured this distinction. The  In addition to the adverb case, the Penn tag set  same external unary annotation was even more  efconflates various grammatical distinctions that are  fective when applied to adverbs (UNARY-RB),  discommonly made in traditional and generative  gramtinguishing, for example, as well from also). Be-mar, and from which a parser could hope to get  useyond these cases, unary tag marking was  detrimenful information. For example, subordinating  contal. The F1 after UNARY-INTERNAL, UNARY-DT,  junctions ( while, as, if ), complementizers ( that, for), and UNARY-RB was 78.86%.  and prepositions ( of, in, from) all get the tag IN.  Many of these distinctions are captured by  PA (subordinating conjunctions occur under S and  The idea that part-of-speech tags are not fine-grained  prepositions under PP), but are not (both  suborenough to abstract away from specific-word  bedinating conjunctions and complementizers appear  haviour is a cornerstone of lexicalization.  The  Also, there are exclusively  nounUNARY-DT annotation, for example, showed that the  modifying prepositions ( of ), predominantly  verbdeterminers which occur alone are usefully  distinmodifying ones ( as), and so on.  The annotation  guished from those which occur with other  nomiSPLIT-IN does a linguistically motivated 6-way split  nal material. This marks the DT nodes with a single  of the IN tag, and brought the total to 81.19%.  bit about their immediate external context: whether  Figure 5 shows an example error in the baseline  there are sisters. Given the success of parent  annowhich is equally well fixed by either TAG-PA or  tation for nonterminals, it makes sense to parent  anSPLIT-IN. In this case, the more common nominal  notate tags, as well (TAG-PA). In fact, as figure 3  use of works is preferred unless the IN tag is anno-shows, exhaustively marking all preterminals with  tated to allow if to prefer S complements.  their parent category was the most effective single  We also got value from three other annotations  annotation we tried. Why should this be useful?  which subcategorized tags for specific lexemes.  Most tags have a canonical category. For example,  First we split off auxiliary verbs with the  SPLITNNS tags occur under NP nodes (only 234 of 70855  AUX annotation, which appends BE to all forms  do not, mostly mistakes).  However, when a tag  of be and HAVE to all forms of have.10 More minorly, SPLIT-CC marked conjunction tags to indicate  8Note that when we show such trees, we generally only  show one annotation on top of the baseline at a time.  More10This is an extended uniform version of the partial  auxilover, we do not explicitly show the binarization implicit by the iary annotation of Charniak (1997), wherein all auxiliaries are horizontal markovization.  marked as AUX and a -G is added to gerund auxiliaries and  9These two are not equivalent even given infinite data.  whether or not they were the strings [ Bb] ut or &, VP S  each of which have distinctly different distributions  from other conjunctions. Finally, we gave the  perto  cent sign (%) its own tag, in line with the dollar sign  appear  appear  ($) already having its own. Together these three  anCD NNS IN  CD NNS IN NP PP last  notations brought the F  three times on NNP JJ  1 to 81.81%.  three times on NNP  What is an Unlexicalized Grammar?  Around this point, we must address exactly what we  Figure 6: An error resolved with the TMP-NP annotation: (a) mean by an unlexicalized PCFG. To the extent that  the incorrect baseline parse and (b) the correct TMP-NP parse.  we go about subcategorizing POS categories, many  of them might come to represent a single word. One  tively means that the subcategories that we break off  might thus feel that the approach of this paper is to  must themselves be very frequent in the language.  walk down a slippery slope, and that we are merely  In such a framework, if we try to annotate  catearguing degrees. However, we believe that there is a  gories with any detailed lexical information, many  fundamental qualitative distinction, grounded in  linsentences either entirely fail to parse, or have only  guistic practice, between what we see as permitted  extremely weird parses. The resulting battle against  in an unlexicalized PCFG as against what one finds  sparsity means that we can only afford to make a few  and hopes to exploit in lexicalized PCFGs. The  didistinctions which have major distributional impact.  vision rests on the traditional distinction between  Even with the individual-lexeme annotations in this  function words (or closed-class words) and content section, the grammar still has only 9255 states com-words (or open class or lexical words). It is  stanpared to the 7619 of the baseline model.  dard practice in linguistics, dating back decades,  to annotate phrasal nodes with important  functionAnnotations Already in the Treebank  word distinctions, for example to have a CP[ for]  or a PP[ to], whereas content words are not part of  At this point, one might wonder as to the wisdom  grammatical structure, and one would not have  speof stripping off all treebank functional tags, only  cial rules or constraints for an NP[ stocks], for exam-to heuristically add other such markings back in to  ple. We follow this approach in our model: various  the grammar. By and large, the treebank out-of-the  closed classes are subcategorized to better represent  package tags, such as PP-LOC or ADVP-TMP, have  important distinctions, and important features  comnegative utility. Recall that the raw treebank  grammonly expressed by function words are annotated  mar, with no annotation or markovization, had an F1  onto phrasal nodes (such as whether a VP is finite,  of 72.62% on our development set. With the  funcor a participle, or an infinitive clause). However, no  tional annotation left in, this drops to 71.49%. The  use is made of lexical class words, to provide either  dropped even further, all the way to 72.87%, when  At any rate, we have kept ourselves honest by  esthese annotations were included.  timating our models exclusively by maximum  likeNonetheless, some distinctions present in the raw  lihood estimation over our subcategorized  gramtreebank trees were valuable. For example, an NP  mar, without any form of interpolation or  shrinkwith an S parent could be either a temporal NP or a  age to unsubcategorized categories (although we do  subject. For the annotation TMP-NP, we retained the  markovize rules, as explained above). This  effecoriginal -TMP tags on NPs, and, furthermore,  propagated the tag down to the tag of the head of the NP.  11It should be noted that we started with four tags in the Penn This is illustrated in figure 6, which also shows an  treebank tagset that rewrite as a single word: EX ( there), WP$  example of its utility, clarifying that CNN last night  ( whose), # (the pound sign), and TO), and some others such as  is not a plausible compound and facilitating the  othWP, POS, and some of the punctuation tags, which rewrite  as barely more. To the extent that we subcategorize tags, there erwise unusual high attachment of the smaller NP.  will be more such cases, but many of them already exist in other TMP-NP brought the cumulative F1 to 82.25%. Note  tag sets. For instance, many tag sets, such as the Brown and that this technique of pushing the functional tags  CLAWS (c5) tagsets give a separate sets of tags to each form of the verbal auxiliaries be, do, and have, most of which rewrite as down to preterminals might be useful more gener-only a single word (and any corresponding contractions).  ally; for example, locative PPs expand roughly the  ROOT  ROOT  Error analysis at this point suggested that many  remaining errors were attachment level and  conjunction scope. While these kinds of errors are  undoubtThis  This  edly profitable targets for lexical preference, most  attachment mistakes were overly high attachments,  indicating that the overall right-branching tendency  of English was not being captured. Indeed, this  tenFigure 7: An error resolved with the SPLIT-VP annotation: (a) dency is a difficult trend to capture in a PCFG be-the incorrect baseline parse and (b) the correct SPLIT-VP parse.  cause often the high and low attachments involve the  very same rules. Even if not, attachment height is  same way as all other PPs (usually as IN NP), but  not modeled by a PCFG unless it is somehow  exthey do tend to have different prepositions below IN.  plicitly encoded into category labels. More  comA second kind of information in the original  plex parsing models have indirectly overcome this  trees is the presence of empty elements. Following  by modeling distance (rather than height).  Collins (1999), the annotation GAPPED-S marks S  Linear distance is difficult to encode in a PCFG  nodes which have an empty subject (i.e., raising and  marking nodes with the size of their yields  mascontrol constructions). This brought F  sively multiplies the state space.13 Therefore, we  1 to 82.28%.  wish to find indirect indicators that distinguish high  Head Annotation  attachments from low ones. In the case of two PPs  following a NP, with the question of whether the  The notion that the head word of a constituent can  second PP is a second modifier of the leftmost NP  affect its behavior is a useful one. However, often  or should attach lower, inside the first PP, the  imthe head tag is as good (or better) an indicator of how  portant distinction is usually that the lower site is a  a constituent will behave.12 We found several head  non-recursive base NP. Collins (1999) captures this  annotations to be particularly effective. First,  posnotion by introducing the notion of a base NP, in  sessive NPs have a very different distribution than  which any NP which dominates only preterminals is  other NPs in particular, NP NP rules are only  marked with a -B. Further, if an NP-B does not have  used in the treebank when the leftmost child is  posa non-base NP parent, it is given one with a unary  sessive (as opposed to other imaginable uses like for  production. This was helpful, but substantially less  New York lawyers, which is left flat). To address this, effective than marking base NPs without introducing POSS-NP marked all possessive NPs. This brought  the unary, whose presence actually erased a useful  the total F1 to 83.06%. Second, the VP symbol is  internal indicator base NPs are more frequent in  very overloaded in the Penn treebank, most severely  subject position than object position, for example. In  in that there is no distinction between finite and  inisolation, the Collins method actually hurt the  basefinitival VPs. An example of the damage this  conline (absolute cost to F1 of 0.37%), while skipping  flation can do is given in figure 7, where one needs  the unary insertion added an absolute 0.73% to the  to capture the fact that present-tense verbs do not  baseline, and brought the cumulative F1 to 86.04%.  generally take bare infinitive VP complements. To  In the case of attachment of a PP to an NP  eiallow the finite/non-finite distinction, and other verb  ther above or inside a relative clause, the high NP  type distinctions, SPLIT-VP annotated all VP nodes  is distinct from the low one in that the already  modwith their head tag, merging all finite forms to a  sinified one contains a verb (and the low one may be  gle tag VBF. In particular, this also accomplished  a base NP as well). This is a partial explanation of  Charniak's gerund-VP marking. This was extremely  the utility of verbal distance in Collins (1999). To  useful, bringing the cumulative F1 to 85.72%, 2.66%  absolute improvement (more than its solo  improve13The inability to encode distance naturally in a naive PCFG  ment over the baseline).  is somewhat ironic. In the heart of any PCFG parser, the fundamental table entry or chart item is a label over a span, for ex-12This is part of the explanation of why (Charniak, 2000)  ample an NP from position 0 to position 5. The concrete use of finds that early generation of head tags as in (Collins, 1999) a grammar rule is to take two adjacent span-marked labels and is so beneficial. The rest of the benefit is presumably in the combine them (for example NP[0,5] and VP[5,12] into S[0,12]).  availability of the tags for smoothing purposes.  Yet, only the labels are used to score the combination.  Length 40  CB  0 CB  Acknowledgements  Collins (1996)  This paper is based on work supported in part by the  this paper  National Science Foundation under Grant No.  IISCollins (1999)  0085896, and in part by an IBM Faculty Partnership  Award to the second author.  Length 100  CB  0 CB  this paper  Figure 8: Results of the final model on the test set (section 23).  James K. Baker. 1979. Trainable grammars for speech recognition. In D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society capture this, DOMINATES-V marks all nodes which  of America, pages 547 550.  dominate any verbal node (V*, MD) with a -V. This  Taylor L. Booth and Richard A. Thomson. 1973. Applying  brought the cumulative F  probability measures to abstract languages. IEEE Transac-1 to 86.91%. We also tried  tions on Computers, C-22:442 450.  marking nodes which dominated prepositions and/or  Sharon A. Caraballo and Eugene Charniak. 1998. New figures  conjunctions, but these features did not help the  cuof merit for best-first probabilistic chart parsing. Computa-mulative hill-climb.  tional Linguistics, 24:275 298.  Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998.  The final distance/depth feature we used was an  Edge-based best-first chart parsing. In Proceedings of the explicit attempt to model depth, rather than use  Sixth Workshop on Very Large Corpora, pages 127 133.  distance and linear intervention as a proxy. With  Eugene Charniak. 1996. Tree-bank grammars. In Proc. of  the 13th National Conference on Artificial Intelligence, pp.  RIGHT-REC-NP, we marked all NPs which contained  another NP on their right periphery (i.e., as a  rightEugene Charniak. 1997. Statistical parsing with a context-free most descendant). This captured some further at-grammar and word statistics. In Proceedings of the 14th Na-tachment trends, and brought us to a final  developEugene Charniak. 2000. A maximum-entropy-inspired parser.  1 of 87.04%.  Eugene Charniak. 2001. Immediate-head parsing for language  Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT  We took the final model and used it to parse  secMichael John Collins. 1996. A new statistical parser based on tion 23 of the treebank.  Figure 8 shows the  results. The test set F1 is 86.32% for 40 words,  M. Collins. 1999. Head-Driven Statistical Models for Natural already higher than early lexicalized models, though  Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.  of course lower than the state-of-the-art parsers.  Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars. In ACL 37, pages 457 464.  Conclusion  Marilyn Ford, Joan Bresnan, and Ronald M. Kaplan. 1982. A  competence-based theory of syntactic closure. In Joan  BresThe advantages of unlexicalized grammars are clear  nan, editor, The Mental Representation of Grammatical Relations, pages 727 796. MIT Press, Cambridge, MA.  enough easy to estimate, easy to parse with, and  Daniel Gildea. 2001. Corpus variation and parser performance.  and space-efficient. However, the dismal  perIn 2001 Conference on Empirical Methods in Natural  Lanformance of basic unannotated unlexicalized  gramguage Processing (EMNLP).  mars has generally rendered those advantages  irrelDonald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103 120.  evant. Here, we have shown that, surprisingly, the  Mark Johnson. 1998. PCFG models of linguistic tree represen-maximum-likelihood estimate of a compact  unlexitations. Computational Linguistics, 24:613 632.  calized PCFG can parse on par with early lexicalized  Dan Klein and Christopher D. Manning. 2001. Parsing with  parsers. We do not want to argue that lexical  setreebank grammars: Empirical bounds, theoretical models,  and the structure of the Penn treebank. In ACL 39/EACL 10.  lection is not a worthwhile component of a  state-ofDavid M. Magerman. 1995. Statistical decision-tree models for the-art parser certain attachments, at least, require  it though perhaps its necessity has been overstated.  Andrew Radford. 1988. Transformational Grammar. Cam-Rather, we have shown ways to improve parsing,  bridge University Press, Cambridge.  Dana Ron, Yoram Singer, and Naftali Tishby. 1994. The power some easier than lexicalization, and others of which  of amnesia. Advances in Neural Information Processing Sys-are orthogonal to it, and could presumably be used  to benefit lexicalized parsers as well. ",1,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Accurate Unlexicalized Parsing treebank could be improved enormously simply by notating each node by its parent category. The Penn  parse much more accurately than previously shown,  treebank covering PCFG is a poor tool for parsing  beby making use of simple, linguistically motivated  state splits, which break down false independence  cause the context-freedom assumptions it embodies  assumptions latent in a vanilla treebank grammar.  are far too strong, and weakening them in this way  Indeed, its performance of 86.36% (LP/LR F1) is  makes the model much better.More recently, Gildea  better than that of early lexicalized PCFG models, (2001) discusses how taking the bilexical probabil-and surprisingly close to the current  state-of-theart.This result has potential uses beyond  establishities out of a good current lexicalized PCFG parser  ing a strong lower bound on the maximum  possihurts performance hardly at all: by at most 0.5% for  ble accuracy of unlexicalized models: an  unlexicaltest text from the same domain as the training data,  ized PCFG is much more compact, easier to  repliand not at all for test text from a different domain.1  cate, and easier to interpret than more complex  lexical models, and the parsing algorithms are simpler,  But it is precisely these bilexical dependencies that  more widely understood, of lower asymptotic  combacked the intuition that lexicalized PCFGs should be  plexity, and easier to optimize.very successful, for example in Hindle and Rooth's  demonstration from PP attachment.We take this as a  In the early 1990s, as probabilistic methods swept  reflection of the fundamental sparseness of the  lexNLP, parsing work revived the investigation of  probical dependency information available in the Penn  abilistic context-free grammars (PCFGs) (Booth and  Treebank.As a speech person would say, one  milThomson, 1973; Baker, 1979).However, early  relion words of training data just isn't enough.Even  sults on the utility of PCFGs for parse  disambiguafor topics central to the treebank's Wall Street  Jourtion and language modeling were somewhat  disapnal text, such as stocks, many very plausible depen-pointing.A conviction arose that lexicalized PCFGs dencies occur only once, for example stocks  stabi(where head words annotate phrasal nodes) were  lized, while many others occur not at all, for exam-the key tool for high performance PCFG parsing.This approach was congruent with the great success  The best-performing lexicalized PCFGs have  inof word n-gram models in speech recognition, and  creasingly made use of subcategorization 3 of the  drew strength from a broader interest in lexicalized  1There are minor differences, but all the current best-known grammars, as well as demonstrations that lexical de-lexicalized PCFGs employ both monolexical statistics, which pendencies were a key tool for resolving ambiguities  describe the phrasal categories of arguments and adjuncts that such as PP attachments (Ford et al., 1982; Hindle and  appear around a head lexical item, and bilexical statistics, or de-Rooth, 1993).In the following decade, great success  pendencies, which describe the likelihood of a head word taking as a dependent a phrase headed by a certain other word.in terms of parse disambiguation and even language  2This observation motivates various or  similaritymodeling was achieved by various lexicalized PCFG  based approaches to combating sparseness, and this remains a models (Magerman, 1995; Charniak, 1997; Collins,  promising avenue of work, but success in this area has proven 1999; Charniak, 2000; Charniak, 2001).somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpolate However, several results have brought into ques-with syntactic category-based estimates when they are not.tion how large a role lexicalization plays in such  3In this paper we use the term subcategorization in the origi-parsers.Johnson (1998) showed that the  perfornal general sense of Chomsky (1965), for where a syntactic  catcategories appearing in the Penn treebank.Charniak  constants.An unlexicalized PCFG parser is much  (2000) shows the value his parser gains from  parentsimpler to build and optimize, including both  stanannotation of nodes, suggesting that this  informadard code optimization techniques and the  investigation is at least partly complementary to information  tion of methods for search space pruning (Caraballo  derivable from lexicalization, and Collins (1999)  and Charniak, 1998; Charniak et al., 1998).uses a range of linguistically motivated and  careIt is not our goal to argue against the use of lex-fully hand-engineered subcategorizations to break  icalized probabilities in high-performance  probabidown wrong context-freedom assumptions of the  listic parsing.It has been comprehensively  demonnaive Penn treebank covering PCFG, such as  differstrated that lexical dependencies are useful in  reentiating base NPs from noun phrases with phrasal  solving major classes of sentence ambiguities, and a  modifiers, and distinguishing sentences with empty  parser should make use of such information where  subjects from those where there is an overt subject  We focus here on using unlexicalized,  NP.While he gives incomplete experimental results  structural context because we feel that this  inforas to their efficacy, we can assume that these features  mation has been underexploited and  underappreciwere incorporated because of beneficial effects on  ated.We see this investigation as only one part of  parsing that were complementary to lexicalization.the foundation for state-of-the-art parsing which  emIn this paper, we show that the parsing  perforploys both lexical and structural conditioning.mance that can be achieved by an unlexicalized  PCFG is far higher than has previously been  demonstrated, and is, indeed, much higher than community  To facilitate comparison with previous work, we  wisdom has thought possible.We describe several  trained our models on sections 2 21 of the WSJ  secsimple, linguistically motivated annotations which  tion of the Penn treebank.We used the first 20 files  do much to close the gap between a vanilla PCFG  (393 sentences) of section 22 as a development set  and state-of-the-art lexicalized models.Specifically,  ( devset).This set is small enough that there is no-we construct an unlexicalized PCFG which  outperticeable variance in individual results, but it allowed  forms the lexicalized PCFGs of Magerman (1995)  rapid search for good features via continually  reparsand Collins (1996) (though not more recent models,  ing the devset in a partially manual hill-climb. All of  such as Charniak (1997) or Collins (1999)).section 23 was used as a test set for the final model.One benefit of this result is a much-strengthened  For each model, input trees were annotated or  translower bound on the capacity of an unlexicalized  formed in some way, as in Johnson (1998).Given  PCFG.To the extent that no such strong baseline has  a set of transformed trees, we viewed the local trees  been provided, the community has tended to greatly  as grammar rewrite rules in the standard way, and  overestimate the beneficial effect of lexicalization in  probabilistic parsing, rather than looking critically  for rule probabilities.5 To parse the grammar, we  at where lexicalized probabilities are both needed to used a simple array-based Java implementation of  make the right decision and available in the training a generalized CKY parser, which, for our final best  data.Secondly, this result affirms the value of  linmodel, was able to exhaustively parse all sentences  guistic analysis for feature discovery.The result has  in section 23 in 1GB of memory, taking  approxiother uses and advantages: an unlexicalized PCFG is  mately 3 sec for average length sentences.6  easier to interpret, reason about, and improve than  the more complex lexicalized models.The grammar  The tagging probabilities were smoothed to accommodate  unknown words.The quantity P( t ag|w or d) was estimated representation is much more compact, no longer re-as follows: words were split into one of several categories quiring large structures that store lexicalized proba-w or dclass, based on capitalization, suffix, digit, and other bilities.The parsing algorithms have lower asymp-character features.For each of these categories, we took the totic complexity4 and have much smaller grammar  maximum-likelihood estimate of P( t ag|w or dclass).This distribution was used as a prior against which observed taggings, if any, were taken, giving P( t ag|w or d) = [ c( t ag, w or d) +  egory is divided into several subcategories, for example  divid P( t ag|w or dclass)]/[ c(w or d)+ ].This was then inverted to ing verb phrases into finite and non-finite verb phrases, rather give P(w or d| t ag).The quality of this tagging model impacts than in the modern restricted usage where the term refers only all numbers; for example the raw treebank grammar's devset F1  to the syntactic argument frames of predicators.  is 72.62 with it and 72.09 without it.4 O( n 3) vs. O( n 5) for a naive implementation, or vs. O( n 4) 6The parser is available for download as open source at:  if using the clever approach of Eisner and Satta (1999).Figure 1: The v=1, h=1 markovization of VP VBZ NP PP.Figure 2: Markovizations: F1 and grammar size.The traditional starting point for unlexicalized  parsing is the raw n-ary treebank grammar read from  child always matters).It is a historical accident that  training trees (after removing functional tags and  the default notion of a treebank PCFG grammar takes  null elements).This basic grammar is imperfect in  v = 1 (only the current node matters vertically) and  two well-known ways.First, the category symbols  h = (rule right hand sides do not decompose at  are too coarse to adequately render the expansions  all).On this view, it is unsurprising that increasing  independent of the contexts.For example, subject  v and decreasing h have historically helped.NP expansions are very different from object NP  exAs an example, consider the case of v = 1,  pansions: a subject NP is 8.7 times more likely than  h = 1.If we start with the rule VP VBZ NP  an object NP to expand as just a pronoun.Having  PP PP, it will be broken into several stages, each a  separate symbols for subject and object NPs allows  binary or unary rule, which conceptually represent  this variation to be captured and used to improve  a head-outward generation of the right hand size, as  parse scoring.One way of capturing this kind of  shown in figure 1.The bottom layer will be a unary  external context is to use parent annotation, as pre-over the head declaring the goal: hVP: [VBZ]i  sented in Johnson (1998).For example, NPs with S  VBZ.The square brackets indicate that the VBZ is  parents (like subjects) will be marked NP S, while  the head, while the angle brackets hXi indicates that  NPs with VP parents (like objects) will be NP VP.the symbol hXi is an intermediate symbol  (equivThe second basic deficiency is that many rule  alently, an active or incomplete state).The next  types have been seen only once (and therefore have  layer up will generate the first rightward sibling of  their probabilities overestimated), and many rules  the head child: hVP: [VBZ]...NPi hVP: [VBZ]i  which occur in test sentences will never have been  NP.Next, the PP is generated: hVP: [VBZ]...PPi  seen in training (and therefore have their  probabilihVP: [VBZ]...NPi PP.We would then branch off left  ties underestimated see Collins (1999) for  analysiblings if there were any.7 Finally, we have another  sis).Note that in parsing with the unsplit grammar,  unary to finish the VP.Note that while it is  convenient to think of this as a head-outward process,  failure, but rather a possibly very weird parse  (Charthese are just PCFG rewrites, and so the actual scores  niak, 1996).One successful method of combating  attached to each rule will correspond to a downward  sparsity is to markovize the rules (Collins, 1999).In generation order.particular, we follow that work in markovizing out  Figure 2 presents a grid of horizontal and  vertifrom the head child, despite the grammar being  uncal markovizations of the grammar.The raw  treelexicalized, because this seems the best way to  capbank grammar corresponds to v = 1, h = (the  ture the traditional linguistic insight that phrases are  upper right corner), while the parent annotation in  organized around a head (Radford, 1988).(Johnson, 1998) corresponds to v = 2, h = , and  Both parent annotation (adding context) and RHS  the second-order model in Collins (1999), is broadly  markovization (removing it) can be seen as two  ina smoothed version of v = 2, h = 2.In  addistances of the same idea.In parsing, every node has  tion to exact n th-order models, we tried  variablea vertical history, including the node itself, parent,  grandparent, and so on.A reasonable assumption is  7In our system, the last few right children carry over as pre-that only the past v vertical ancestors matter to the  ceding context for the left children, distinct from common practice.We found this wrapped horizon to be beneficial, and it current expansion.Similarly, only the previous h  also unifies the infinite order model with the unmarkovized raw horizontal ancestors matter (we assume that the head  ROOT  Revenue was  CD  CD  including  CONJP  SPLIT-AUX  SPLIT-CC  down slightly from $  CD  CD  Figure 4: An error which can be resolved with the  UNARYINTERNAL annotation (incorrect baseline parse shown).  grammar.Although it does not necessarily jump out  RIGHT-REC-NP  of the grid at first glance, this point represents the  best compromise between a compact grammar and  Figure 3: Size and devset performance of the cumulatively an-useful markov histories.notated models, starting with the markovized baseline.The  right two columns show the change in F1 from the baseline for each annotation introduced, both cumulatively and for each sin-3  External vs. Internal Annotation  gle annotation applied to the baseline in isolation.The two major previous annotation strategies,  parent annotation and head lexicalization, can be seen  history models similar in intent to those described  as instances of external and internal annotation,  rein Ron et al.(1994).For variable horizontal  hisspectively.Parent annotation lets us indicate an  important feature of the external environment of a  occurrences of a symbol.For example, if the symbol  node which influences the internal expansion of that  hVP: [VBZ]...PP PPi were too rare, we would  colnode.On the other hand, lexicalization is a  (radilapse it to hVP: [VBZ]...PPi.For vertical histories,  cal) method of marking a distinctive aspect of the  we used a cutoff which included both frequency and  otherwise hidden internal contents of a node which  mutual information between the history and the  exinfluence the external distribution.Both kinds of  anpansions (this was not appropriate for the horizontal  notation can be useful.To identify split states, we  case because MI is unreliable at such low counts).add suffixes of the form -X to mark internal content  Figure 2 shows parsing accuracies as well as the  features, and X to mark external features.number of symbols in each markovization.These  To illustrate the difference, consider unary  prosymbol counts include all the intermediate states  ductions.In the raw grammar, there are many  unarwhich represent partially completed constituents.ies, and once any major category is constructed over  The general trend is that, in the absence of further  a span, most others become constructible as well  usannotation, more vertical annotation is better even  ing unary chains (see Klein and Manning (2001) for  exhaustive grandparent annotation. This is not true  discussion).Such chains are rare in real treebank  for horizontal markovization, where the  variabletrees: unary rewrites only appear in very specific  order second-order model was superior.The best  contexts, for example S complements of verbs where  entry, v = 3, h 2, has an F1 of 79.74, already  the S has an empty, controlled subject.Figure 4  a substantial improvement over the baseline.shows an erroneous output of the parser, using the  In the remaining sections, we discuss other  anbaseline markovized grammar.Intuitively, there are  notations which increasingly split the symbol space.several reasons this parse should be ruled out, but  Since we expressly do not smooth the grammar, not  one is that the lower S slot, which is intended  priall splits are guaranteed to be beneficial, and not all  marily for S complements of communication verbs,  sets of useful splits are guaranteed to co-exist well.  is not a unary rewrite position (such complements  In particular, while v = 3, h 2 markovization is  usually have subjects).It would therefore be natural  good on its own, it has a large number of states and  to annotate the trees so as to confine unary  producdoes not tolerate further splitting well.Therefore,  tions to the contexts in which they are actually  apwe base all further exploration on the v 2, h 2  propriate.We tried two annotations.First,  UNARYINTERNAL marks (with a -U) any nonterminal node  which has only one child.In isolation, this resulted  in an absolute gain of 0.55% (see figure 3).The  to VB  to  same sentence, parsed using only the baseline and  UNARY-INTERNAL, is parsed correctly, because the  VP rewrite in the incorrect parse ends with an S  VPU with very low probability.8  Alternately, UNARY-EXTERNAL, marked nodes  which had no siblings with  It was similar to  Figure 5: An error resolved with the TAG-PA annotation (of the but provided far less marginal benefit on top of  IN tag): (a) the incorrect baseline parse and (b) the correct TAG-other later features (none at all on top of  UNARYPA parse.SPLIT-IN also resolves this error.INTERNAL for our top models), and was discarded.9  One restricted place where external unary  annotasomewhat regularly occurs in a non-canonical  position was very useful, however, was at the  pretermition, its distribution is usually distinct.For example,  nal level, where internal annotation was  meaningthe most common adverbs directly under ADVP are  less.One distributionally salient tag conflation in  also (1599) and now (544).Under VP, they are n't the Penn treebank is the identification of  demonstra(3779) and not (922).Under NP, only (215) and just tives ( that, those) and regular determiners ( the, a).(132), and so on.TAG-PA brought F  Splitting  DT tags based on whether they were only  tially, to 80.62%.children (UNARY-DT) captured this distinction.The  In addition to the adverb case, the Penn tag set  same external unary annotation was even more  efconflates various grammatical distinctions that are  fective when applied to adverbs (UNARY-RB),  discommonly made in traditional and generative  gramtinguishing, for example, as well from also).Be-mar, and from which a parser could hope to get  useyond these cases, unary tag marking was  detrimenful information.For example, subordinating  contal.The F1 after UNARY-INTERNAL, UNARY-DT,  junctions ( while, as, if ), complementizers ( that, for), and UNARY-RB was 78.86%.  and prepositions ( of, in, from) all get the tag IN.Many of these distinctions are captured by  PA (subordinating conjunctions occur under S and  The idea that part-of-speech tags are not fine-grained  prepositions under PP), but are not (both  suborenough to abstract away from specific-word  bedinating conjunctions and complementizers appear  haviour is a cornerstone of lexicalization.The  Also, there are exclusively  nounUNARY-DT annotation, for example, showed that the  modifying prepositions ( of ), predominantly  verbdeterminers which occur alone are usefully  distinmodifying ones ( as), and so on.The annotation  guished from those which occur with other  nomiSPLIT-IN does a linguistically motivated 6-way split  nal material.This marks the DT nodes with a single  of the IN tag, and brought the total to 81.19%.bit about their immediate external context: whether  Figure 5 shows an example error in the baseline  there are sisters.Given the success of parent  annowhich is equally well fixed by either TAG-PA or  tation for nonterminals, it makes sense to parent  anSPLIT-IN.In this case, the more common nominal  notate tags, as well (TAG-PA).In fact, as figure 3  use of works is preferred unless the IN tag is anno-shows, exhaustively marking all preterminals with  tated to allow if to prefer S complements.their parent category was the most effective single  We also got value from three other annotations  annotation we tried.Why should this be useful?which subcategorized tags for specific lexemes.Most tags have a canonical category.For example,  First we split off auxiliary verbs with the  SPLITNNS tags occur under NP nodes (only 234 of 70855  AUX annotation, which appends BE to all forms  do not, mostly mistakes).However, when a tag  of be and HAVE to all forms of have.10 More minorly, SPLIT-CC marked conjunction tags to indicate  8Note that when we show such trees, we generally only  show one annotation on top of the baseline at a time.More10This is an extended uniform version of the partial  auxilover, we do not explicitly show the binarization implicit by the iary annotation of Charniak (1997), wherein all auxiliaries are horizontal markovization.marked as AUX and a -G is added to gerund auxiliaries and  9These two are not equivalent even given infinite data.  whether or not they were the strings [ Bb] ut or &, VP S  each of which have distinctly different distributions  from other conjunctions.Finally, we gave the  perto  cent sign (%) its own tag, in line with the dollar sign  appear  appear  ($) already having its own.Together these three  anCD NNS IN  CD NNS IN NP PP last  notations brought the F  three times on NNP JJ  1 to 81.81%.three times on NNP  What is an Unlexicalized Grammar?Around this point, we must address exactly what we  Figure 6: An error resolved with the TMP-NP annotation: (a) mean by an unlexicalized PCFG.To the extent that  the incorrect baseline parse and (b) the correct TMP-NP parse.we go about subcategorizing POS categories, many  of them might come to represent a single word.One  tively means that the subcategories that we break off  might thus feel that the approach of this paper is to  must themselves be very frequent in the language.walk down a slippery slope, and that we are merely  In such a framework, if we try to annotate  catearguing degrees.However, we believe that there is a  gories with any detailed lexical information, many  fundamental qualitative distinction, grounded in  linsentences either entirely fail to parse, or have only  guistic practice, between what we see as permitted  extremely weird parses.The resulting battle against  in an unlexicalized PCFG as against what one finds  sparsity means that we can only afford to make a few  and hopes to exploit in lexicalized PCFGs.The  didistinctions which have major distributional impact.vision rests on the traditional distinction between  Even with the individual-lexeme annotations in this  function words (or closed-class words) and content section, the grammar still has only 9255 states com-words (or open class or lexical words).It is  stanpared to the 7619 of the baseline model.dard practice in linguistics, dating back decades,  to annotate phrasal nodes with important  functionAnnotations Already in the Treebank  word distinctions, for example to have a CP[ for]  or a PP[ to], whereas content words are not part of  At this point, one might wonder as to the wisdom  grammatical structure, and one would not have  speof stripping off all treebank functional tags, only  cial rules or constraints for an NP[ stocks], for exam-to heuristically add other such markings back in to  ple.We follow this approach in our model: various  the grammar.By and large, the treebank out-of-the  closed classes are subcategorized to better represent  package tags, such as PP-LOC or ADVP-TMP, have  important distinctions, and important features  comnegative utility.Recall that the raw treebank  grammonly expressed by function words are annotated  mar, with no annotation or markovization, had an F1  onto phrasal nodes (such as whether a VP is finite,  of 72.62% on our development set.With the  funcor a participle, or an infinitive clause).However, no  tional annotation left in, this drops to 71.49%.The  use is made of lexical class words, to provide either  dropped even further, all the way to 72.87%, when  At any rate, we have kept ourselves honest by  esthese annotations were included.timating our models exclusively by maximum  likeNonetheless, some distinctions present in the raw  lihood estimation over our subcategorized  gramtreebank trees were valuable.For example, an NP  mar, without any form of interpolation or  shrinkwith an S parent could be either a temporal NP or a  age to unsubcategorized categories (although we do  subject.For the annotation TMP-NP, we retained the  markovize rules, as explained above).This  effecoriginal -TMP tags on NPs, and, furthermore,  propagated the tag down to the tag of the head of the NP.11It should be noted that we started with four tags in the Penn This is illustrated in figure 6, which also shows an  treebank tagset that rewrite as a single word: EX ( there), WP$  example of its utility, clarifying that CNN last night  ( whose), # (the pound sign), and TO), and some others such as  is not a plausible compound and facilitating the  othWP, POS, and some of the punctuation tags, which rewrite  as barely more.To the extent that we subcategorize tags, there erwise unusual high attachment of the smaller NP.will be more such cases, but many of them already exist in other TMP-NP brought the cumulative F1 to 82.25%.Note  tag sets.For instance, many tag sets, such as the Brown and that this technique of pushing the functional tags  CLAWS (c5) tagsets give a separate sets of tags to each form of the verbal auxiliaries be, do, and have, most of which rewrite as down to preterminals might be useful more gener-only a single word (and any corresponding contractions).ally; for example, locative PPs expand roughly the  ROOT  ROOT  Error analysis at this point suggested that many  remaining errors were attachment level and  conjunction scope.While these kinds of errors are  undoubtThis  This  edly profitable targets for lexical preference, most  attachment mistakes were overly high attachments,  indicating that the overall right-branching tendency  of English was not being captured.Indeed, this  tenFigure 7: An error resolved with the SPLIT-VP annotation: (a) dency is a difficult trend to capture in a PCFG be-the incorrect baseline parse and (b) the correct SPLIT-VP parse.cause often the high and low attachments involve the  very same rules.Even if not, attachment height is  same way as all other PPs (usually as IN NP), but  not modeled by a PCFG unless it is somehow  exthey do tend to have different prepositions below IN.plicitly encoded into category labels.More  comA second kind of information in the original  plex parsing models have indirectly overcome this  trees is the presence of empty elements.Following  by modeling distance (rather than height).Collins (1999), the annotation GAPPED-S marks S  Linear distance is difficult to encode in a PCFG  nodes which have an empty subject (i.e., raising and  marking nodes with the size of their yields  mascontrol constructions).This brought F  sively multiplies the state space.13 Therefore, we  1 to 82.28%.wish to find indirect indicators that distinguish high  Head Annotation  attachments from low ones.In the case of two PPs  following a NP, with the question of whether the  The notion that the head word of a constituent can  second PP is a second modifier of the leftmost NP  affect its behavior is a useful one.However, often  or should attach lower, inside the first PP, the  imthe head tag is as good (or better) an indicator of how  portant distinction is usually that the lower site is a  a constituent will behave.12 We found several head  non-recursive base NP.Collins (1999) captures this  annotations to be particularly effective.First,  posnotion by introducing the notion of a base NP, in  sessive NPs have a very different distribution than  which any NP which dominates only preterminals is  other NPs in particular, NP NP rules are only  marked with a -B.Further, if an NP-B does not have  used in the treebank when the leftmost child is  posa non-base NP parent, it is given one with a unary  sessive (as opposed to other imaginable uses like for  production.This was helpful, but substantially less  New York lawyers, which is left flat).To address this, effective than marking base NPs without introducing POSS-NP marked all possessive NPs.This brought  the unary, whose presence actually erased a useful  the total F1 to 83.06%.Second, the VP symbol is  internal indicator base NPs are more frequent in  very overloaded in the Penn treebank, most severely  subject position than object position, for example.In  in that there is no distinction between finite and  inisolation, the Collins method actually hurt the  basefinitival VPs.An example of the damage this  conline (absolute cost to F1 of 0.37%), while skipping  flation can do is given in figure 7, where one needs  the unary insertion added an absolute 0.73% to the  to capture the fact that present-tense verbs do not  baseline, and brought the cumulative F1 to 86.04%.generally take bare infinitive VP complements.To  In the case of attachment of a PP to an NP  eiallow the finite/non-finite distinction, and other verb  ther above or inside a relative clause, the high NP  type distinctions, SPLIT-VP annotated all VP nodes  is distinct from the low one in that the already  modwith their head tag, merging all finite forms to a  sinified one contains a verb (and the low one may be  gle tag VBF.In particular, this also accomplished  a base NP as well).This is a partial explanation of  Charniak's gerund-VP marking.This was extremely  the utility of verbal distance in Collins (1999).To  useful, bringing the cumulative F1 to 85.72%, 2.66%  absolute improvement (more than its solo  improve13The inability to encode distance naturally in a naive PCFG  ment over the baseline).  is somewhat ironic.In the heart of any PCFG parser, the fundamental table entry or chart item is a label over a span, for ex-12This is part of the explanation of why (Charniak, 2000)  ample an NP from position 0 to position 5.The concrete use of finds that early generation of head tags as in (Collins, 1999) a grammar rule is to take two adjacent span-marked labels and is so beneficial.The rest of the benefit is presumably in the combine them (for example NP[0,5] and VP[5,12] into S[0,12]).availability of the tags for smoothing purposes.Yet, only the labels are used to score the combination.Length 40  CB  0 CB  Acknowledgements  Collins (1996)  This paper is based on work supported in part by the  this paper  National Science Foundation under Grant No.  IISCollins (1999)  0085896, and in part by an IBM Faculty Partnership  Award to the second author.Length 100  CB  0 CB  this paper  Figure 8: Results of the final model on the test set (section 23).James K. Baker.1979. Trainable grammars for speech recognition.In D.H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society capture this, DOMINATES-V marks all nodes which  of America, pages 547 550.dominate any verbal node (V*, MD) with a -V.This  Taylor L. Booth and Richard A. Thomson.1973. Applying  brought the cumulative F  probability measures to abstract languages.IEEE Transac-1 to 86.91%.We also tried  tions on Computers, C-22:442 450.marking nodes which dominated prepositions and/or  Sharon A. Caraballo and Eugene Charniak.1998. New figures  conjunctions, but these features did not help the  cuof merit for best-first probabilistic chart parsing.Computa-mulative hill-climb.tional Linguistics, 24:275 298.Eugene Charniak, Sharon Goldwater, and Mark Johnson.1998.  The final distance/depth feature we used was an  Edge-based best-first chart parsing.In Proceedings of the explicit attempt to model depth, rather than use  Sixth Workshop on Very Large Corpora, pages 127 133.distance and linear intervention as a proxy.With  Eugene Charniak.1996. Tree-bank grammars.In Proc. of  the 13th National Conference on Artificial Intelligence, pp.RIGHT-REC-NP, we marked all NPs which contained  another NP on their right periphery (i.e., as a  rightEugene Charniak.1997. Statistical parsing with a context-free most descendant).This captured some further at-grammar and word statistics.In Proceedings of the 14th Na-tachment trends, and brought us to a final  developEugene Charniak.2000. A maximum-entropy-inspired parser.1 of 87.04%.Eugene Charniak.2001. Immediate-head parsing for language  Noam Chomsky.1965. Aspects of the Theory of Syntax.MIT  We took the final model and used it to parse  secMichael John Collins.1996. A new statistical parser based on tion 23 of the treebank.Figure 8 shows the  results.The test set F1 is 86.32% for 40 words,  M.Collins.1999. Head-Driven Statistical Models for Natural already higher than early lexicalized models, though  Language Parsing.Ph.D. thesis, Univ. of Pennsylvania.  of course lower than the state-of-the-art parsers.Jason Eisner and Giorgio Satta.1999. Efficient parsing for bilexical context-free grammars and head-automaton grammars.In ACL 37, pages 457 464.Conclusion  Marilyn Ford, Joan Bresnan, and Ronald M. Kaplan.1982. A  competence-based theory of syntactic closure.In Joan  BresThe advantages of unlexicalized grammars are clear  nan, editor, The Mental Representation of Grammatical Relations, pages 727 796.MIT Press, Cambridge, MA.enough easy to estimate, easy to parse with, and  Daniel Gildea.2001. Corpus variation and parser performance.  and space-efficient.However, the dismal  perIn 2001 Conference on Empirical Methods in Natural  Lanformance of basic unannotated unlexicalized  gramguage Processing (EMNLP).mars has generally rendered those advantages  irrelDonald Hindle and Mats Rooth.1993. Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103 120.  evant.Here, we have shown that, surprisingly, the  Mark Johnson.1998. PCFG models of linguistic tree represen-maximum-likelihood estimate of a compact  unlexitations.Computational Linguistics, 24:613 632.calized PCFG can parse on par with early lexicalized  Dan Klein and Christopher D. Manning.2001. Parsing with  parsers.We do not want to argue that lexical  setreebank grammars: Empirical bounds, theoretical models,  and the structure of the Penn treebank.In ACL 39/EACL 10.lection is not a worthwhile component of a  state-ofDavid M.Magerman.1995. Statistical decision-tree models for the-art parser certain attachments, at least, require  it though perhaps its necessity has been overstated.Andrew Radford.1988. Transformational Grammar.Cam-Rather, we have shown ways to improve parsing,  bridge University Press, Cambridge.Dana Ron, Yoram Singer, and Naftali Tishby.1994. The power some easier than lexicalization, and others of which  of amnesia.Advances in Neural Information Processing Sys-are orthogonal to it, and could presumably be used  to benefit lexicalized parsers as well."
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables  National Institute of Information and Communications Technology  Tohoku University  Kyoto University  2002), p.168). For example, when a document  contains some domain-specific words, the document  In this paper, we present a dependency  treebased method for sentiment classification of  will probably belong to the domain. However, in  Japanese and English subjective sentences  using conditional random fields with hidden  reversed. For example, let us consider the sentence  Subjective sentences often  con The medicine kills cancer cells. While the phrase  tain words which reverse the sentiment  pocancer cells has negative polarity, the word kills re-larities of other words.  Therefore,  interacverses the polarity, and the whole sentence has  postions between words need to be considered  itive polarity. Thus, in sentiment classification, a  in sentiment classification, which is difficult  sentence which contains positive (or negative)  polarto be handled with simple bag-of-words  approaches, and the syntactic dependency  strucity words does not necessarily have the same  polartures of subjective sentences are exploited in  ity as a whole, and we need to consider interactions  our method. In the method, the sentiment  polarity of each dependency subtree in a  sendently.  tence, which is not observable in training data,  Recently, several methods have been proposed to  is represented by a hidden variable. The  pocope with the problem (Zaenen, 2004; Ikeda et al.,  larity of the whole sentence is calculated in  consideration of interactions between the  hid2008). However, these methods are based on flat  bag-of-features representation, and do not consider  tion is used for inference. Experimental  resyntactic structures which seem essential to infer  sults of sentiment classification for Japanese  the polarity of a whole sentence. Other methods  and English subjective sentences showed that  have been proposed which utilize composition of  the method performs better than other  methsentences (Moilanen and Pulman, 2007; Choi and  ods based on bag-of-features.  Cardie, 2008; Jia et al., 2009), but these methods  use rules to handle polarity reversal, and whether  po",0,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables 2002), p.168). For example, when a document  contains some domain-specific words, the document  In this paper, we present a dependency  treebased method for sentiment classification of  will probably belong to the domain.However, in  Japanese and English subjective sentences  using conditional random fields with hidden  reversed.For example, let us consider the sentence  Subjective sentences often  con The medicine kills cancer cells.While the phrase  tain words which reverse the sentiment  pocancer cells has negative polarity, the word kills re-larities of other words.Therefore,  interacverses the polarity, and the whole sentence has  postions between words need to be considered  itive polarity.Thus, in sentiment classification, a  in sentiment classification, which is difficult  sentence which contains positive (or negative)  polarto be handled with simple bag-of-words  approaches, and the syntactic dependency  strucity words does not necessarily have the same  polartures of subjective sentences are exploited in  ity as a whole, and we need to consider interactions  our method.In the method, the sentiment  polarity of each dependency subtree in a  sendently.tence, which is not observable in training data,  Recently, several methods have been proposed to  is represented by a hidden variable.The  pocope with the problem (Zaenen, 2004; Ikeda et al.,  larity of the whole sentence is calculated in  consideration of interactions between the  hid2008).However, these methods are based on flat  bag-of-features representation, and do not consider  tion is used for inference.Experimental  resyntactic structures which seem essential to infer  sults of sentiment classification for Japanese  the polarity of a whole sentence.Other methods  and English subjective sentences showed that  have been proposed which utilize composition of  the method performs better than other  methsentences (Moilanen and Pulman, 2007; Choi and  ods based on bag-of-features.Cardie, 2008; Jia et al., 2009), but these methods  use rules to handle polarity reversal, and whether  po"
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Dependency-Based Construction of Semantic  Saarland University  University of Edinburgh  Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art.  1. ",1,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Dependency-Based Construction of Semantic Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account.We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process.We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation.In all cases, our framework obtains results that are comparable or superior to the state of the art.1."
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales  Bo Pang and Lillian Lee  (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract  become aware of the scientific challenges posed and  the scope of new applications enabled by the  proWe address the rating-inference problem,  cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.) Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ). This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivassiloglou, 2003). But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.  the task.  Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.  We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.  mission is mediocre (middling rating). Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.  be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1 ",0,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales become aware of the scientific challenges posed and the scope of new applications enabled by the proWe address the rating-inference problem, cessing of subjective language. (The papers  colwherein rather than simply decide whether  lected by Qu, Shanahan, and Wiebe (2004) form a  a review is thumbs up or thumbs  representative sample of research in the area.)Most  down , as in previous sentiment  analyprior work on the specific problem of categorizing  sis work, one must determine an author's  expressly opinionated text has focused on the  bievaluation with respect to a multi-point  nary distinction of positive vs. negative (Turney,  scale (e.g., one to five stars ).This task  2002; Pang, Lee, and Vaithyanathan, 2002; Dave,  represents an interesting twist on  stanLawrence, and Pennock, 2003; Yu and  Hatzivassiloglou, 2003).But it is often helpful to have more cause there are several different degrees  information than this binary distinction provides, es-of similarity between class labels; for  expecially if one is ranking items by recommendation  ample, three stars is intuitively closer to  or comparing several reviewers' opinions: example  four stars than to one star .  applications include collaborative filtering and  deWe first evaluate human performance at  ciding which conference submissions to accept.the task.Then, we apply a  metaTherefore, in this paper we consider generalizing  algorithm, based on a metric labeling  forto finer-grained scales: rather than just determine mulation of the problem, that alters a  whether a review is thumbs up or not, we attempt  given -ary classifier's output in an  exto infer the author's implied numerical rating, such  plicit attempt to ensure that similar items  as three stars or four stars . Note that this differs receive similar labels.We show that  from identifying opinion strength (Wilson, Wiebe, the meta-algorithm can provide signifi-and Hwa, 2004): rants and raves have the same  cant improvements over both multi-class  strength but represent opposite evaluations, and  refand regression versions of SVMs when we  eree forms often allow one to indicate that one is  employ a novel similarity measure  approvery confident (high strength) that a conference  subpriate to the problem.mission is mediocre (middling rating).Also, our  Publication info: Proceedings of the  task differs from ranking not only because one can ACL, 2005.be given a single item to classify (as opposed to a  set of items to be ordered relative to one another),  but because there are settings in which classification 1"
" Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts Stanford University, Stanford, CA 94305, USA  richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu  {jeaneis,manning,cgpotts}@stanford.edu  Semantic word spaces have been very  useThis  ful but cannot express the meaning of longer  phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank. It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.  To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.  trained on the new treebank, this model  outperforms all previous methods on several  metrics. It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%. The accuracy of predicting  nomena presented in such data. To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines. Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.  for both positive and negative phrases.  The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1  "," United we stand: improving sentiment analysis  by joining machine learning and rule based methods  National Centre for Scientific Research Demokritos , Inst. of Informatics and Telecommunications, Athens Greece  University of the Aegean, Dpt. of Information and Communication System Enginneering Samos, Greece  Institute of Computational Linguistics, University of Zurich,  In the past, we have succesfully used machine learning approaches for sentiment analysis. In the course of those experiments, we observed that our machine learning method, although able to cope well with figurative language could not always reach a certain decision about the polarity orientation of sentences, yielding erroneous evaluations. We support the conjecture that these cases bearing mild figurativeness could be better handled by a rule-based system. These two systems, acting complementarily, could bridge the gap between machine learning and rule-based approaches. Experimental results using the corpus of the Affective Text Task of SemEval '07, provide evidence in favor of this direction.  ",1,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu {jeaneis,manning,cgpotts}@stanford.edu Semantic word spaces have been very ful but cannot express the meaning of longer phrases in a principled way. Further progress  towards understanding compositionality in  tasks such as sentiment detection requires  about +  richer supervised training and evaluation  reor  sources and more powerful models of  comwit any  of  To remedy this, we introduce a  cleverness ,  other kind  Sentiment Treebank.It includes fine grained  sentiment labels for 215,154 phrases in the  Figure 1: Example of the Recursive Neural Tensor Net-parse trees of 11,855 sentences and presents  work accurately predicting 5 sentiment classes, very neg-new challenges for sentiment  compositionative to very positive ( , , 0, +, + +), at every node of a ality.To address them, we introduce the  parse tree and capturing the negation and its scope in this Recursive Neural Tensor Network.trained on the new treebank, this model  outperforms all previous methods on several  metrics.It pushes the state of the art in single  models to accurately capture the underlying  phe80% up to 85.4%.The accuracy of predicting  nomena presented in such data.To address this need, fine-grained sentiment labels for all phrases  we introduce the Stanford Sentiment Treebank and  reaches 80.7%, an improvement of 9.7% over  bag of features baselines.Lastly, it is the only  a powerful Recursive Neural Tensor Network that  model that can accurately capture the effects  can accurately predict the compositional semantic  of negation and its scope at various tree levels  effects present in this new corpus.for both positive and negative phrases.The Stanford Sentiment Treebank is the first  corpus with fully labeled parse trees that allows for a 1","by joining machine learning and rule based methods In the past, we have succesfully used machine learning approaches for sentiment analysis. In the course of those experiments, we observed that our machine learning method, although able to cope well with figurative language could not always reach a certain decision about the polarity orientation of sentences, yielding erroneous evaluations.We support the conjecture that these cases bearing mild figurativeness could be better handled by a rule-based system.These two systems, acting complementarily, could bridge the gap between machine learning and rule-based approaches.Experimental results using the corpus of the Affective Text Task of SemEval '07, provide evidence in favor of this direction."
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.  Association for Computational Linguistics.  Thumbs up? Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.  San Jose, CA 95120 USA  use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al. (1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.  deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  sentiment categorization. Moreover, there are also  perform human-produced baselines.  Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem. A challenging aspect of this categorization. We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.  ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the  ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","79-86.Association for Computational Linguistics.Thumbs up?Sentiment Classification using Machine Learning Techniques  Bo Pang and Lillian Lee  Department of Computer Science  IBM Almaden Research Center  Cornell University  650 Harry Rd.San Jose, CA 95120 USA  use.Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye's We consider the problem of classifying doc-Lexant system1) and recommender systems (e.g.,  uments not by topic, but by overall  sentiTerveen et al.(1997), Tatemura (2000)), where user  ment, e.g., determining whether a review  input and feedback could be quickly summarized;  inis positive or negative.deed, in general, free-form survey responses given in views as data, we find that standard ma-natural language format could be processed using  sentiment categorization.Moreover, there are also  perform human-produced baselines.Howpotential applications to message filtering; for exam-ever, the three machine learning methods  ple, one might be able to use sentiment information  we employed (Naive Bayes, maximum  entropy classification, and support vector  maIn this paper, we examine the effectiveness of  apchines) do not perform as well on sentiment  plying machine learning techniques to the sentiment  classification as on traditional topic-based  classification problem.A challenging aspect of this categorization.We conclude by examining  problem that seems to distinguish it from traditional factors that make the sentiment classifica-topic-based classification is that while topics are of-tion problem more challenging.ten identifiable by keywords alone, sentiment can be expressed in a more subtle manner.For example, the"
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. ", Squibs and Discussions Co-occurrence Patterns among Collocations: A Tool for Corpus-Based Lexical Knowledge Acquisition  Douglas Biber Northern Arizona University  1. ,1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.",Squibs and Discussions Co-occurrence Patterns among Collocations: A Tool for Corpus-Based Lexical Knowledge Acquisition 1. 
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," A Simple Rule-Based Part of Speech Tagger  Depart ment of Computer Science University of Pennsylvania Philadelphia, Pennsylvania 19104 U.S.A.  Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule based met hods. In this paper, we present a sim ple rule-based part of speech tagger which au tomatically acquires its rules and tags with ac curacy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, cor pus genre or language to another. Perhaps the biggest contribution of t his work is in demon strating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encou ragement for researchers to further explore rule-based tagging, search ing for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.  1 Introduct ion  There has been a dramatic increase in the application of probabilistic models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease wit h which the necessary statistics can be automat ically acquired and the fact that very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are ty pically not very robust.  One area in which the statistical approach has done particularly well is automatic part of speech tagging, as signing each word in an input sentence its proper par t of speech [Church 88; Cutting et al. 92; DeRose 88; Deroualt and Merialdo 86; Garside et al. 87; Jelinek 85;  The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-C0031 PRI.  Kupiec 89; Meteer et al. 91]. Stochastic taggers have ob tained a high degree of accuracy without perforing any syntactic analysis on the input. These stochastic. part of speech taggers make use of a Markov model which cap tures lexical and contextual information. The parame ters of the model can be estimated from tagged ([Church 88; DeRose 88; Deroualt and Merialdo 86; Garside et al. 87; Meteer et al. 91]) or untagged ([Cutting et al. 92; J elinek 85; Kupiec 89]) text. Once the parameters of the model are estimated a sentence can then be automat ically tagged by assining it the tag sequence which is assigned the highest probability by _the mod_el. Prfor mance is often enhanced with the aid of various higher level and post processing procedures or by manually tuning the model.  A number of rule-based taggers have been built [Klein and Simmons 63; Green and Rubin 71; Hindle 89]. [Klein and Simmons 63] and [Green and Rubin 71] both have error rates substantially higher than state of the art stochastic taggers. [Hindle 89] disambiguates wods within a deterministic parser. We wanted to determme whether a simple rule-based tagger without any knowl edge of syntax can perform as well as a stochtic tagg_er , or if part of speech tagging really is a domam to which stochastic techniques are better suited.  In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models. The rule-based tagger overcomes the limitations common in rule-based approaches to language process ing: it is robust, and the rules are automatically ac quired. In addition, the tagger has many adva:r_itag s over stochastic taggers, including: a vast reduction m stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of find ing and implementing improvements to the tagger, and better portability from one tag set or corpus genre to another.  2 The Tagger  The tagger works by automatically recognizin and re.m edying its weaknesses, thereby incrementally impovng its performance. The tagger initially tags by assigning each word its most likely tag, estimated by examining a large tagged corpus, without regard to context. In both sentences below, run would be tagged as a verb:  The run lasted thirty minutes. We run three miles every day.  The initial tagger has two procedures built in to im prove performance; both make use of no contextual in formation . One procedure is provided with information that words that were not in the training corpus and are capitalized tend to be proper nouns, and attempts to fix tagging mistakes accordingly. This information could be acquired automatically (see below), but is prespecified in the current implementation. In addition, there is a procedure which attempts to tag words not seen in the training corpus by assigning such words the tag most common for words ending in the same three letters. For example, blahblahous would be tagged as an adjective, because this is the most common tag for words ending in ous. This information is derived automatically from the training corpus.  This very simple algorithm has an error rate of abou t 7.9% when trained on 90% of the tagged Brown Corpus 1 [Francis and Kucera 82], and tested on a separate 5% of the corpus.2 Training consists of compiling a list of the most common tag for each word in t he training corpus.  The tagger then acquires patches to improve its per formance. Patch templates are of the form:  If a word is tagged a and it is in context C, then change that tag to b, or  If a word is tagged a and it has lexical property P,  then change that tag to b, or  If a word is tagged a and a word in region R has lexical property P, then change that tag to b.  The initial tagger was trained on 90% of the corpus ( the training corpus). 5% was held back to be used for the patch acquisition procedure (the patch corpus) and 5% for testing. Once the initial tagger is trained, it is used to tag the patch corpus. A list of tagging errors is compiled by comparing the output of the tagger to the correct tagging of the patch corpus. This list consists  of triples < taga , tagb , numbe r >, indicating the number  of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch cor pus. Next , for each error t riple, it is determined which instantiation of a template from the prespecified set of pdtch templates results in the greatest error reduction. Currently, t he patch templates are:  Change tag a to tag b when:  1. The preceding (following) word is tagged z.  2. The word two before (after) is tagged z.  1The Brown Corpus contains about 1.1 million words from a variety of genres of written English. There are 192 tags in the tag set, 96 of which occur more than one hu nd red times in the corpus.  2 The test set contained text from all genres in the Brown Corpus.  One of the two preceding (following) words is tagged  4. One of the th ree preceding (following) words is tagged z.  5. The preceding word is tagged z and the following word is tagged w.  6. The preceding (following) word is tagged z and the word two before (after) is tagged w.  7. The current word is (is not) capitalized.  8. The previous word is (is not) capitalized.  For each error triple < taga , tagb , number > and patch, we compute the reduction in error which results from applying the patch to remedy the mistagging of a word as taga when it should have been tagged tagb . We then compute t he number of new errors caused by ap plying the patch; that is, the number of times the patch results in a word being tagged as tagb when it should be tagged taga . The net improvement is calculated by subtracting the latter val ue from the former.  For example, when the initial tagger tags the patch corpus, it mistags 159 words as verbs when they should be nouns. If the patch change the tag from verb to noun if one of the two preceding word s is tagged as a deter miner is applied, it corrects 98 of t he 159 errors. How ever, it results in an additional 18 errors from changing tags which really should have been verb to noun. This patch results in a net decrease of 80 errors on the patch corpus.  The patch which results in the greatest improvement to the patch corpus is added to the list of patches. The patch is then applied in order to improve the tagging of the patch corpus, and the patch acquisition procedure continues.  The first ten patches found by the system are listed below 3 .  (1) TO iN NEXT-TAG AT  (6) TO IN N EXT-WORD-IS-CAP YES  (10) NP N N CURRENT-WORD-IS-CAP NO  The first patch states that if a word is tagged TO and the following word is tagged AT, then switch t he tag from TO to IN. This is because a noun phrase is  3 AT == article, HV D == had, IN == preposition, M D ==  modal, N N == sing. noun, N P == proper noun, PPS == 3rd sing. nom. pronoun, PPO == obj. personal pronoun, TO =  infinitive to, VB == verb, VBN == past part. verb, VBD ==  much more likely to immediately follow a preposition than to immediately follow infinitive TO. The second patch states that a tag should be switched from VBN to VBD if the preceding word is capitalized. This patch arises from two facts: the past verb tag is more likely than the past participle verb tag after a proper noun , a11d is also the more likely tag for the second word of the sentence.4 The third patch states that VBD should be changed to V BN if a ny of t he preceding th ree words are tagged HVD.  Once t he l ist of patches has been acquired , new text can be tagged as follows. First, tag the text using the basic lexical tagger. Next, apply each patch in turn to the corpus to decrease t he error rate. A patch which  changes the tagging of a word from a to b only applies  if the word has been tagged b somewhere in the training w  Note t hat one need not be too careful when construct ing the list of pat.ch templates. Adding a bad template to the list will not worsen performance. If a template is bad , then no rules which are instantiations of that template will appear in the final list of patches learned by the tagger. This makes it easy to experiment with extensions to the tagger.  Patch Application and Error Reduction  The tagger was tested on 5% of the Brown Corpus in cluding sections from every genre. First , the test corpus was t agged by the simple lexical tagger. Next , each of the patches was in t u rn applied to t he cor pus. Below is a graph showing the improvement in accu racy from apply  ing patches. It is significant that with only 71 patches, an error rate of 5.1% was obtained 5 . Of the 71 patches, 66 resulted in a reduction in the number of errors in the  test cor pus, 3 resulted in no net change, and 2 resulted in a higher number of errors. Almost all patches which were effective on the training corpus were also effective on the test corpus.  U nfort unately, it is difficult to compare our results with other published results. In [Meteer et al. 9 I], an error rate of 3-4% on one domain , Wall Street Journal articles and 5.6% on another domain , texts on terrorism in Latin American countries, is quoted. However , bot h the domains and the tag set are different from what we use. [Church 88] reports an accuracy of ""95-99% cor rect , depending on the definition of cor rect"" . We imple mented a version of t he algorithm described by Church. When trained and tested on the same samples used in our experiment , we found the error rate to be about 4.5%. [DeRose 88] quotes a 4% error rate; however, the sample used for testing was par t of the training corpus. [Garside et al. 87] reports an accura<: y of 96-97%. Their probabilistic t agger has been augmented with a hand crafted procedure to pretag problematic ""idioms"". This procedure, which requires that a list of idioms be  la 4 Both the first word of a sentence and proper nouns are capitalized.  Number of Patches  boriously created by hand , contributes 3% toward the accuracy of their tagger , according to [DeRose 88]. The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus. It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule-based tagger. For example, their tagger had diffi culty tagging as old as. An explicit rule was written to pretag as old as with the proper tags. According to the tagging scheme of the Brown Corpus, t he first as shoul be tagged as a qualifier, and the second as a subordi nating conjunction. In the rule-based t.agge, the ot common tag for as is subordinating conju nction. So tially, the second as is tagged correctly and the first a.s is tagged incorrectly. To remedy this, the system acquires the patch: if the curre nt word is tagged as a subordinat  ing conjunction, and so is the word two positions ahead then change the tag of the current word to qualifier.6  The rule-based tagger has automatically learned how to properly tag this ""idiom.""  Regardless of the precise rankings of the various tag gers, we have demonstrated that a simple rule-based ta? ger with very few rules performs on par with stochastic taggers.  5 We ran the experiment three times. Each time we divided  the corpus into training, patch and test sets in a different way. All three ru ns gave an error rate of 5%.  6 This was one of the 71 patches acquired by the rule-based  We have presented a simple part of speech tagger which perfor ms as well as existing stochastic taggers, but has significant advantages over these taggers.  The tagger is extremely portable. Many of the higher level proced ures used to improve the performance of stochastic taggers would not readily transfer over to a different tag set or genre, and certainly would not trans fer over to a different language. Everything except for the proper nou n discovery procedure is automatically ac quired by the rule-based tagger 7 , making it much more portable than a stochastic tagger. If the tagger were trained on a different corpus, a different set of patches suitable for that corpus would be found automatically.  Large tables of statistics are not needed for the rule based tagger. In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture contextual information. This information is usually a ta ble of trigram statistics, indicating for all tags tag a , tagb and t agc the probability that tagc follows taga and tagb . In the rule-based tagger, contextual information is cap tured in fewer than eighty rules. This makes for a much more perspicuous tagger, aiding in better understanding and simplifying further development of the tagger. Con textual information is expressed in a much more compact and understandable form. As can be seen from compar ing error rates, this compact representation of contextual information is just as effective as the information hidden in the large tables of contextual probabilities.  Perhaps the biggest cont ribu tion of t his work is in demonstrating that the stochastic method is not the only viable approach for part of speech tagging. The fact that the simple rule-based tagger can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more ex pressive set of patch templates and other variations on this simple bu t effective theme.  [Church 88) Church , K. A Stochastic Parts Program and Noun Phrase Parser for Unrest ricted Text. In Proceed ings of the Second Conference on A pplied N atural La nguage Processing, AGL , 136-143, 1988.  [Cutting et al. 92) Cutting, D., Kupiec, J ., Pederson, J. and Sibun , P. A Practical Part-of-Speech Tagger. In Proceedings of the Third Confer ence on Ap plied N at ural Language Process ing, AGL , 1992.  [DeRose 88) De Rose, S.J . Grammatical Category Dis ambiguation by Statistical Optimization. Computational Linguistics 14: 31-39, 1988.  [Deroualt and Merialdo 86) Deroualt , A. and Merialdo,  B. Natural language modeling for phoneme to-text transcription. IEEE Transactions on Patt ern Analysis and M achine Int ellige nce, Vol. PAMI-8, No. 6, 742-749, 1986.  7 A nd even this could be learned by the tagger.  (Francis and Kucera 82) Francis,  W. Nelson and Kucera, Henry, Frequenc y analysis of English usage. Le xicon and gram mar. Houghton Mifflin, Boston, 1982.  G. The Computational Analysis of English: A Corpus-Based Approa ch. Longman: Lon don , 1987.  [Green and Ru bin 71) Green, B. and Ru bin , G. Au to mated Grammatical Tagging of English. De par tment of Linguistics, Brown University, 1971.  [Hindle 89) Hindle, D. Acquiring disambiguation rules from text. Proceedings of the 27th Annual M eeting of the Association for Computa tional Lin guistics, 1989.  [Jelinek 85) Jelinek, F. Markov source modeling of text generation. In J . K. Skwirzinski, ed., Im pact of Processin g Techniques on Commu nication, Dordrecht, 1985.  (Klein and Simmons 63) Klein , S. and Simmons, R.F. A Computational Approach to Grammatical Coding of English Words. JACM 10: 334-47. 1963.  [Kupiec 89) Kupiec, J . Augmenting a hidden Markov model for phrase-dependent word tagging. In Proceedings of the DA RPA Speech a nd Natural Langua ge Workshop, Morgan Kauf mann , 1989.  [Meteer et al. 91) Meteer, M., Schwartz, R., and Weischedel, R. Empirical Studies in Part of Speech Labelling, Proceedings of the DARPA Speech and N atural Language Workshop, Morgan Kaufmann , 1991. ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","A Simple Rule-Based Part of Speech Tagger Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule based met hods.In this paper, we present a sim ple rule-based part of speech tagger which au tomatically acquires its rules and tags with ac curacy comparable to stochastic taggers.The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, cor pus genre or language to another.Perhaps the biggest contribution of t his work is in demon strating that the stochastic method is not the only viable method for part of speech tagging.The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encou ragement for researchers to further explore rule-based tagging, search ing for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.1 Introduct ion  There has been a dramatic increase in the application of probabilistic models to natural language processing over the last few years.The appeal of stochastic techniques over traditional rule-based techniques comes from the ease wit h which the necessary statistics can be automat ically acquired and the fact that very little handcrafted knowledge need be built into the system.In contrast, the rules in rule-based systems are usually difficult to construct and are ty pically not very robust.One area in which the statistical approach has done particularly well is automatic part of speech tagging, as signing each word in an input sentence its proper par t of speech [Church 88; Cutting et al.92; DeRose 88; Deroualt and Merialdo 86; Garside et al.87; Jelinek 85;  The author would like to thank Mitch Marcus and Rich Pito for valuable input.This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-C0031 PRI.Kupiec 89; Meteer et al.91].Stochastic taggers have ob tained a high degree of accuracy without perforing any syntactic analysis on the input.These stochastic.part of speech taggers make use of a Markov model which cap tures lexical and contextual information.The parame ters of the model can be estimated from tagged ([Church 88; DeRose 88; Deroualt and Merialdo 86; Garside et al. 87; Meteer et al. 91]) or untagged ([Cutting et al. 92; J elinek 85; Kupiec 89]) text.Once the parameters of the model are estimated a sentence can then be automat ically tagged by assining it the tag sequence which is assigned the highest probability by _the mod_el.Prfor mance is often enhanced with the aid of various higher level and post processing procedures or by manually tuning the model.A number of rule-based taggers have been built [Klein and Simmons 63; Green and Rubin 71; Hindle 89].[Klein and Simmons 63] and [Green and Rubin 71] both have error rates substantially higher than state of the art stochastic taggers.[Hindle 89] disambiguates wods within a deterministic parser.We wanted to determme whether a simple rule-based tagger without any knowl edge of syntax can perform as well as a stochtic tagg_er , or if part of speech tagging really is a domam to which stochastic techniques are better suited.In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models.The rule-based tagger overcomes the limitations common in rule-based approaches to language process ing: it is robust, and the rules are automatically ac quired.In addition, the tagger has many adva:r_itag s over stochastic taggers, including: a vast reduction m stored information required, the perspicuity of a small set of meaningful rules as opposed to the large tables of statistics needed for stochastic taggers, ease of find ing and implementing improvements to the tagger, and better portability from one tag set or corpus genre to another.2 The Tagger  The tagger works by automatically recognizin and re.m edying its weaknesses, thereby incrementally impovng its performance.The tagger initially tags by assigning each word its most likely tag, estimated by examining a large tagged corpus, without regard to context.In both sentences below, run would be tagged as a verb:  The run lasted thirty minutes.We run three miles every day.The initial tagger has two procedures built in to im prove performance; both make use of no contextual in formation . One procedure is provided with information that words that were not in the training corpus and are capitalized tend to be proper nouns, and attempts to fix tagging mistakes accordingly.This information could be acquired automatically (see below), but is prespecified in the current implementation.In addition, there is a procedure which attempts to tag words not seen in the training corpus by assigning such words the tag most common for words ending in the same three letters.For example, blahblahous would be tagged as an adjective, because this is the most common tag for words ending in ous.This information is derived automatically from the training corpus.This very simple algorithm has an error rate of abou t 7.9% when trained on 90% of the tagged Brown Corpus 1 [Francis and Kucera 82], and tested on a separate 5% of the corpus.2 Training consists of compiling a list of the most common tag for each word in t he training corpus.The tagger then acquires patches to improve its per formance.Patch templates are of the form:  If a word is tagged a and it is in context C, then change that tag to b, or  If a word is tagged a and it has lexical property P,  then change that tag to b, or  If a word is tagged a and a word in region R has lexical property P, then change that tag to b.The initial tagger was trained on 90% of the corpus ( the training corpus).5% was held back to be used for the patch acquisition procedure (the patch corpus) and 5% for testing.Once the initial tagger is trained, it is used to tag the patch corpus.A list of tagging errors is compiled by comparing the output of the tagger to the correct tagging of the patch corpus.This list consists  of triples < taga , tagb , numbe r >, indicating the number  of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch cor pus.Next , for each error t riple, it is determined which instantiation of a template from the prespecified set of pdtch templates results in the greatest error reduction.Currently, t he patch templates are:  Change tag a to tag b when:  1.The preceding (following) word is tagged z.2. The word two before (after) is tagged z.1The Brown Corpus contains about 1.1 million words from a variety of genres of written English.There are 192 tags in the tag set, 96 of which occur more than one hu nd red times in the corpus.2 The test set contained text from all genres in the Brown Corpus.One of the two preceding (following) words is tagged  4.One of the th ree preceding (following) words is tagged z.5. The preceding word is tagged z and the following word is tagged w.6. The preceding (following) word is tagged z and the word two before (after) is tagged w.7. The current word is (is not) capitalized.8. The previous word is (is not) capitalized.For each error triple < taga , tagb , number > and patch, we compute the reduction in error which results from applying the patch to remedy the mistagging of a word as taga when it should have been tagged tagb . We then compute t he number of new errors caused by ap plying the patch; that is, the number of times the patch results in a word being tagged as tagb when it should be tagged taga . The net improvement is calculated by subtracting the latter val ue from the former.For example, when the initial tagger tags the patch corpus, it mistags 159 words as verbs when they should be nouns.If the patch change the tag from verb to noun if one of the two preceding word s is tagged as a deter miner is applied, it corrects 98 of t he 159 errors.How ever, it results in an additional 18 errors from changing tags which really should have been verb to noun.This patch results in a net decrease of 80 errors on the patch corpus.The patch which results in the greatest improvement to the patch corpus is added to the list of patches.The patch is then applied in order to improve the tagging of the patch corpus, and the patch acquisition procedure continues.The first ten patches found by the system are listed below 3 .  (1) TO iN NEXT-TAG AT  (6) TO IN N EXT-WORD-IS-CAP YES  (10) NP N N CURRENT-WORD-IS-CAP NO  The first patch states that if a word is tagged TO and the following word is tagged AT, then switch t he tag from TO to IN.This is because a noun phrase is  3 AT == article, HV D == had, IN == preposition, M D ==  modal, N N == sing.noun, N P == proper noun, PPS == 3rd sing. nom.pronoun, PPO == obj.personal pronoun, TO =  infinitive to, VB == verb, VBN == past part.verb, VBD ==  much more likely to immediately follow a preposition than to immediately follow infinitive TO.The second patch states that a tag should be switched from VBN to VBD if the preceding word is capitalized.This patch arises from two facts: the past verb tag is more likely than the past participle verb tag after a proper noun , a11d is also the more likely tag for the second word of the sentence.4 The third patch states that VBD should be changed to V BN if a ny of t he preceding th ree words are tagged HVD.Once t he l ist of patches has been acquired , new text can be tagged as follows.First, tag the text using the basic lexical tagger.Next, apply each patch in turn to the corpus to decrease t he error rate.A patch which  changes the tagging of a word from a to b only applies  if the word has been tagged b somewhere in the training w  Note t hat one need not be too careful when construct ing the list of pat.ch templates.Adding a bad template to the list will not worsen performance.If a template is bad , then no rules which are instantiations of that template will appear in the final list of patches learned by the tagger.This makes it easy to experiment with extensions to the tagger.Patch Application and Error Reduction  The tagger was tested on 5% of the Brown Corpus in cluding sections from every genre.First , the test corpus was t agged by the simple lexical tagger.Next , each of the patches was in t u rn applied to t he cor pus.Below is a graph showing the improvement in accu racy from apply  ing patches.It is significant that with only 71 patches, an error rate of 5.1% was obtained 5 . Of the 71 patches, 66 resulted in a reduction in the number of errors in the  test cor pus, 3 resulted in no net change, and 2 resulted in a higher number of errors.Almost all patches which were effective on the training corpus were also effective on the test corpus.U nfort unately, it is difficult to compare our results with other published results.In [Meteer et al. 9 I], an error rate of 3-4% on one domain , Wall Street Journal articles and 5.6% on another domain , texts on terrorism in Latin American countries, is quoted.However , bot h the domains and the tag set are different from what we use.[Church 88] reports an accuracy of ""95-99% cor rect , depending on the definition of cor rect"" . We imple mented a version of t he algorithm described by Church.When trained and tested on the same samples used in our experiment , we found the error rate to be about 4.5%.[DeRose 88] quotes a 4% error rate; however, the sample used for testing was par t of the training corpus.[Garside et al. 87] reports an accura<: y of 96-97%.Their probabilistic t agger has been augmented with a hand crafted procedure to pretag problematic ""idioms"".This procedure, which requires that a list of idioms be  la 4 Both the first word of a sentence and proper nouns are capitalized.Number of Patches  boriously created by hand , contributes 3% toward the accuracy of their tagger , according to [DeRose 88].The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus.It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule-based tagger.For example, their tagger had diffi culty tagging as old as.An explicit rule was written to pretag as old as with the proper tags.According to the tagging scheme of the Brown Corpus, t he first as shoul be tagged as a qualifier, and the second as a subordi nating conjunction.In the rule-based t.agge, the ot common tag for as is subordinating conju nction.So tially, the second as is tagged correctly and the first a.s is tagged incorrectly.To remedy this, the system acquires the patch: if the curre nt word is tagged as a subordinat  ing conjunction, and so is the word two positions ahead then change the tag of the current word to qualifier.6  The rule-based tagger has automatically learned how to properly tag this ""idiom.""Regardless of the precise rankings of the various tag gers, we have demonstrated that a simple rule-based ta?ger with very few rules performs on par with stochastic taggers.5 We ran the experiment three times.Each time we divided  the corpus into training, patch and test sets in a different way.All three ru ns gave an error rate of 5%.6 This was one of the 71 patches acquired by the rule-based  We have presented a simple part of speech tagger which perfor ms as well as existing stochastic taggers, but has significant advantages over these taggers.The tagger is extremely portable.Many of the higher level proced ures used to improve the performance of stochastic taggers would not readily transfer over to a different tag set or genre, and certainly would not trans fer over to a different language.Everything except for the proper nou n discovery procedure is automatically ac quired by the rule-based tagger 7 , making it much more portable than a stochastic tagger.If the tagger were trained on a different corpus, a different set of patches suitable for that corpus would be found automatically.Large tables of statistics are not needed for the rule based tagger.In a stochastic tagger, tens of thousands of lines of statistical information are needed to capture contextual information.This information is usually a ta ble of trigram statistics, indicating for all tags tag a , tagb and t agc the probability that tagc follows taga and tagb . In the rule-based tagger, contextual information is cap tured in fewer than eighty rules.This makes for a much more perspicuous tagger, aiding in better understanding and simplifying further development of the tagger.Con textual information is expressed in a much more compact and understandable form.As can be seen from compar ing error rates, this compact representation of contextual information is just as effective as the information hidden in the large tables of contextual probabilities.Perhaps the biggest cont ribu tion of t his work is in demonstrating that the stochastic method is not the only viable approach for part of speech tagging.The fact that the simple rule-based tagger can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more ex pressive set of patch templates and other variations on this simple bu t effective theme.[Church 88) Church , K. A Stochastic Parts Program and Noun Phrase Parser for Unrest ricted Text.In Proceed ings of the Second Conference on A pplied N atural La nguage Processing, AGL , 136-143, 1988.[Cutting et al. 92) Cutting, D., Kupiec, J ., Pederson, J. and Sibun , P. A Practical Part-of-Speech Tagger.In Proceedings of the Third Confer ence on Ap plied N at ural Language Process ing, AGL , 1992.[DeRose 88) De Rose, S.J . Grammatical Category Dis ambiguation by Statistical Optimization.Computational Linguistics 14: 31-39, 1988.[Deroualt and Merialdo 86) Deroualt , A. and Merialdo,  B. Natural language modeling for phoneme to-text transcription.IEEE Transactions on Patt ern Analysis and M achine Int ellige nce, Vol. PAMI-8, No. 6, 742-749, 1986.7 A nd even this could be learned by the tagger.(Francis and Kucera 82) Francis,  W. Nelson and Kucera, Henry, Frequenc y analysis of English usage.Le xicon and gram mar.Houghton Mifflin, Boston, 1982.G. The Computational Analysis of English: A Corpus-Based Approa ch.Longman: Lon don , 1987.[Green and Ru bin 71) Green, B. and Ru bin , G. Au to mated Grammatical Tagging of English.De par tment of Linguistics, Brown University, 1971.[Hindle 89) Hindle, D. Acquiring disambiguation rules from text.Proceedings of the 27th Annual M eeting of the Association for Computa tional Lin guistics, 1989.[Jelinek 85) Jelinek, F. Markov source modeling of text generation.In J . K. Skwirzinski, ed., Im pact of Processin g Techniques on Commu nication, Dordrecht, 1985.(Klein and Simmons 63) Klein , S. and Simmons, R.F. A Computational Approach to Grammatical Coding of English Words.JACM 10: 334-47.1963.  [Kupiec 89) Kupiec, J . Augmenting a hidden Markov model for phrase-dependent word tagging.In Proceedings of the DA RPA Speech a nd Natural Langua ge Workshop, Morgan Kauf mann , 1989.[Meteer et al. 91) Meteer, M., Schwartz, R., and Weischedel, R. Empirical Studies in Part of Speech Labelling, Proceedings of the DARPA Speech and N atural Language Workshop, Morgan Kaufmann , 1991."
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory  Information Sciences Institute  University of S. California  corpora (Moser and Moore, 1995; Marcu et al., Abstract  In this paper, we recount our experience in developing a discourse-annotated  developing a large resource with discourse-level corpus for community-wide use.  annotation for NLP research. Our main goal in Working in the framework of  undertaking this effort was to create a reference Rhetorical Structure Theory, we were  corpus for community-wide use. Two essential able to create a large annotated  considerations from the outset were that the resource with very high consistency,  corpus needed to be consistently annotated, and using a well-defined methodology and  that it would be made publicly available through protocol. This resource is made  the Linguistic Data Consortium for a nominal publicly available through the  fee to cover distribution costs. The paper Linguistic Data Consortium to enable  describes the challenges we faced in building a researchers to develop empirically  corpus of this level of complexity and scope  grounded, discourse-specific  including selection of theoretical approach, applications.  annotation methodology, training, and quality assurance. The resulting corpus contains 385  ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory California  corpora (Moser and Moore, 1995; Marcu et al., Abstract  In this paper, we recount our experience in developing a discourse-annotated  developing a large resource with discourse-level corpus for community-wide use.annotation for NLP research.Our main goal in Working in the framework of  undertaking this effort was to create a reference Rhetorical Structure Theory, we were  corpus for community-wide use.Two essential able to create a large annotated  considerations from the outset were that the resource with very high consistency,  corpus needed to be consistently annotated, and using a well-defined methodology and  that it would be made publicly available through protocol.This resource is made  the Linguistic Data Consortium for a nominal publicly available through the  fee to cover distribution costs.The paper Linguistic Data Consortium to enable  describes the challenges we faced in building a researchers to develop empirically  corpus of this level of complexity and scope  grounded, discourse-specific  including selection of theoretical approach, applications.annotation methodology, training, and quality assurance.The resulting corpus contains 385"
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Kenneth Ward Church  Bell Laboratories Murray Hill, N.J.  Collins Publishers Glasgow, Scotland  The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.  It is common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their co-occurrence with other words. Running through the whole Firthian tradition, for example, is the theme that ""You shall know a word by the company it keeps"" (Firth, 1957).  On the one hand, bank co-occurs with words and sion such as money, notes, loan, account, investment, clerk, official, manager, robbery, vaults, working in a, its actions, First National, of England, and so forth. On the other hand, we find bank co-occurring with river, swim, boat, east (and of course West and South, which have acquired special meanings of their own), on top of the, and of the Rhine. (Hanks 1987, p. 127)  The search for increasingly delicate word classes is not new. In lexicography, for example, it goes back at least to the ""verb patterns"" described in Hornby's Advanced Learner's Dictionary (first edition 1948). What is new is that ties for the computational storage and analysis of large bodies of natural language have developed significantly in recent years, so that it is now becoming possible to test and apply informal assertions of this kind in a more rigorous way, and to see what company our words do keep.  The proposed statistical description has a large number of potentially important applications, including: (a) ing the language model both for speech recognition and optical character recognition (OCR), (b) providing biguation cues for parsing highly ambiguous syntactic tures such as noun compounds, conjunctions, and tional phrases, (c) retrieving texts from large databases  (e.g. newspapers, patents), (d) enhancing the productivity of computational linguists in compiling lexicons of synWctic facts, and (e) enhancing the productivity of cographers in identifying normal and conventional usage.  Consider the optical character recognizer (OCR) cation. Suppose that we have an OCR device as in Kahan et al. (1987), and it has assigned about equal probability to having recognized farm and form, where the context is either: (1) federal credit or (2) some of.  federal ~form ] credit  The proposed association measure can make use of the fact that farm is much more likely in the first context and form is much more likely in the second to resolve the ambiguity. Note that alternative disambiguation methods based on syntactic constraints such as part of speech are unlikely to help in this case since both form and farm are commonly used as nouns.  Word association norms are well known to be an important factor in psycholinguistic research, especially in the area of lexical retrieval. Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.  Some results and implications are summarized from  reaction-time experiments in which subjects either (a)  classified successive strings of letters as words and  nonwords, or (b) pronounced the strings. Both types of  response to words (e.g. BUTTER) were consistently  faster when preceded by associated words (e.g. BREAD)  rather than unassociated words (e.g. NURSE) (Meyer  et al. 1975, p. 98) Much of this psycholinguistic research is based on cal estimates of word association norms as in Palermo and Jenkins (1964), perhaps the most influential study of its kind, though extremely small and somewhat dated. This study measured 200 words by asking a few thousand jects to write down a word after each of the 200 words to be measured. Results are reported in tabular form, indicating which words were written down, and by how many subjects, factored by grade level and sex. The word doctor, for example, is reported on pp. 98-100 to be most often ated with nurse, followed by sick, health, medicine, tal, man, sickness, lawyer, and about 70 more words.  We propose an alternative measure, the association ratio, for measuring word association norms, based on the mation theoretic concept of mutual information. 1 The proposed measure is more objective and less costly than the subjective method employed in Palermo and Jenkins (1964). The association ratio can be scaled up to provide robust estimates of word association norms for a large portion of the language. Using the association ratio measure, the five most associated words are, in order: dentists, nurses, ing, treat, and hospitals.  What is ""mutual information?"" According to Fano (1961), if two points (words), x and y, have probabilities P(x) and P(y), then their mutual information, I(x,y), is defined to be  Informally, mutual information compares the probability of observing x and y together (the joint probability) with the probabilities of observing x and y independently (chance). If there is a genuine association between x and y, then the joint probability P(x,y) will be much larger than chance P(x) P(y), and consequently I(x,y) >> 0. If there is no interesting relationship between x and y, then P(x,y) P(x) P(y), and thus, I(x,y) ~ O. If x and y are in mentary distribution, then P(x,y) will be much less than P(x) P(y), forcing I(x,y) << 0.  In our application, word probabilities P(x) and P(y) are estimated by counting the number of observations of x and y in a corpus, f (x) andf(y), and normalizing by N, the size of the corpus. (Our examples use a number of different corpora with different sizes: 15 million words for the 1987 AP corpus, 36 million words for the 1988 AP corpus, and  8.6 million tokens for the tagged corpus.) Joint ties, P(x,y), are estimated by counting the number of times that x is followed by y in a window of w words, fw (x,y), and normalizing by N.  The window size parameter allows us to look at different scales. Smaller window sizes will identify fixed expressions (idioms such as bread and butter) and other relations that hold over short ranges; larger window sizes will highlight semantic concepts and other relationships that hold over larger scales.  Table 1 may help show the contrast. 2 In fixed sions, such as bread and butter and drink and drive, the words of interest are separated by a fixed number of words and there is very little variance. In the 1988 AP, it was found that the two words are always exactly two words apart whenever they are found near each other (within five words). That is, the mean separation is two, and the variance is zero.  Compounds also have very fixed word order (little ance), but the average separation is closer to one word rather than two. In contrast, relations such as man/woman are less fixed, as indicated by a larger variance in their separation. (The nearly zero value for the mean separation for man/women indicates the words appear about equally  Table 1. Mean and Variance of the Separation Between X and Y  Separation often in either order.) Lexical relations come in several varieties. There are some like refraining from that are fairly fixed, others such as coming from that may be separated by an argument, and still others like keeping from that are almost certain to be separated by an ment.  Relation Word x Word y Mean Variance  Compound computer scientist 1.12 O. I 0  United States 0.98 0.14  Semantic man woman 1.46 8.07  Lexical refraining from 1.11 0.20  coming from 0.83 2.89  The ideal window size is different in each case. For the  remainder of this paper, the window size, w, will be set to  five words as a compromise; this setting is large enough to  show some of the constraints between verbs and arguments,  but not so large that it would wash out constraints that  make use of strict adjacency)  Since the association ratio becomes unstable when the counts are very small, we will not discuss word pairs with f(x,y) _< 5. An improvement would make use of t-scores, and throw out pairs that were not significant. nately, this requires an estimate of the variance off(x,y), which goes beyond the scope of this paper. For the der of this paper, we will adopt the simple but arbitrary threshold, and ignore pairs with small counts.  Technically, the association ratio is different from  mutual information in two respects. First, joint probabilities  are supposed to be symmetric: P(x,y) = P(y, x), and  thus, mutual information is also symmetric: I(x,y) =  I(y, x). However, the association ratio is not symmetric,  sincef(x, y) encodes linear precedence. (Recall thatf(x, y)  denotes the number of times that word x appears before y  in the window of w words, not the number of times the two  words appear in either order.) Although we could fix this  problem by redefiningf(x, y) to be symmetric (by  averaging the matrix with its transpose), we have decided not to  do so, since order information appears to be very  interesting. Notice the asymmetry in the pairs in Table 2  (computed from 44 million words of 1988 AP text), illustrating a  wide variety of biases ranging from sexism to syntax.  Second, one might expect f(x, y) <_ f(x) and f(x, y) <_ f(y), but the way we have been counting, this needn't be the case if x and y happen to appear several times in the window. For example, given the sentence, ""Library ers were prohibited from saving books from this heap of ruins,"" which appeared in an AP story on April 1, 1988, f(prohibited) = 1 and f(prohibited, from) = 2. This problem can be fixed by dividingf(x, y) by w -1 (which has the consequence of subtracting log2 (w -1) = 2 from our association ratio scores). This adjustment has the  addiTable 2. Asymmetry in 1988 AP Corpus (N = 44 million)  doctors lawyers 29 19  save money 187 11  tional beneft of assuring that Z f(x,y) = ~ f(x) = Zf(y) = N. When I(x, y) is large, the association ratio produces very credible results not unlike those reported in Palermo and Jenkins (1964), as illustrated in Table 3. In contrast, when I(x, y) ---: 0, the pairs are less interesting. (As a very rough rule; of thumb, we have observed that pairs with I(x, y) > 3 tend to be interesting, and pairs with smaller I(x, y) are generally not. One can make this statement precise by calibrating the measure with subjective measures.  Alternatively, one could make estimates of the variance and then make statements about confidence levels, e.g. with 95% confidence, P(x, y) > e(x) P(y).)  If I(x, y) << 0, we would predict that x and y are in complementary distribution. However, we are rarely able to observe I(x, y) << 0 because our corpora are too small (and our measurement techniques are too crude). Suppose, for example, that both x and y appear about 10 times per million words of text. Then, P(x) = P(y) = 10 -5 and chance is P(x) P(x) = 10 -I . Thus, to say that I(x, y) is much less than 0, we need to say that P(x, y) is much less than 10 -t , a statement that is hard to make with much confidence given the size of presently available corpora. In fact, we cannot (easily) observe a probability less than  1/N ~ 10 -7, and therefore it is hard to know if I(x, y) is  much less than chance or not, unless chance is very large.  (In fact, the pair a... doctors in Table 3, appears  significantly less often than chance. But to justify this statement,  we need to compensate for the window size (which shifts  the score downward by 2.0, e.g. from 0.96 down to -1.04),  and we need to estimate the standard deviation, using a  method such as Good (1953). 4  Although the psycholinguistic literature documents the significance of noun/noun word associations such as doctor/ nurse in considerable detail, relatively little is said about  Table 3. Some interesting Associations with ""Doctor"" in the  1987 AP Corpus (N = 15 million)  11.3 12 111 honorary 621 doctor  9.4 8 1105 doctors 154 treating  9.0 6 275 examined 621 doctor  8.9 11 1105 doctors 317 treat  Some Uninteresting Associations with ""Doctor""  0.96 6 621 doctor 73785 with  0.93 12 84716 is 1105 doctors  associations among verbs, function words, adjectives, and other non-nouns. In addition to identifying semantic tions of the doctor/nurse variety, we believe the association ratio can also be used to search for interesting lexico-syntactic relationships between verbs and typical argu-ments/adjuncts. The proposed association ratio can be viewed as a formalization of Sinclair's argument:  How common are the phrasal verbs with set? Set is particularly rich in making combinations with words like about, in, up, out, on, off, and these words are themselves very common. How likely is set offto occur? Both are frequent words [set occurs approximately 250 times in a million words and off occurs approximately 556 times in a million words... [T]he question we are asking can be roughly rephrased as follows: how likely is off to occur immediately after set?... This is 0.00025 x 0.00055 [P(x) P(y)], which gives us the tiny figure of 0.0000001375 ... The assumption behind this tion is that the words are distributed at random in a text [at chance, in our terminology]. It is obvious to a linguist that this is not so, and a rough measure of how much set and offattract each other is to compare the probability with what actually happens ... Set off occurs nearly 70 times in the 7.3 million word corpus [P(x, y) = 70/(7.3 x 106) >> P(x) P(y)]. That is enough to show its main patterning and it suggests that in currently-held corpora there will be found sufficient evidence for the description of a substantial collection of phrases ... (Sinclair 1987c, pp. 151-152).  Using Sinclair's estimates P(set) ~ 250 x 10 -6, P(off) ~-556 x 10 -6, and P(set, off) ~ 70/(7.3 x 106), we would estimate the mutual information to be I(set; off) = log2P(set, off)/(P(set) P(off)) ~ 6.1. In the 1988 AP corpus (N = 44,344,077), we estimate P(set) ~ 13,046/N, P(off) ~ 20,693/N, and P(set, off) ~ 463/N. Given these estimates, we would compute the mutual information to be l(set; off) ~ 6.2.  In this example, at least, the values seem to be fairly comparable across corpora. In other examples, we will see some differences due to sampling. Sinclair's corpus is a fairly balanced sample of (mainly British) text; the AP corpus is an unbalanced sample of American journalese.  This association between set and offis relatively strong; the joint probability is more than 26 = 64 times larger than chance. The other particles that Sinclair mentions have association ratios that can be seen in Table 4.  The first three, set up, set off, and set out, are clearly  Table 4. Some Phrasal Verbs in 1988 AP Corpus (N = 44 million)  set up 13,046 64,601 2713 7.3 set off 13,046 20,693 463 6.2 set out 13,046 47,956 301 4.4 set on 13,046 258,170 162 1.1 set in 13,046 739,932 795 1.8 set about 13,046 82,319 16 -0.6  associated; the last three are not so clear. As Sinclair suggests, the approach is well suited for identifying the phrasal verbs, at least in certain cases.  6 PREPROCESSING WITH A PART OF SPEECH TAGGER  Phrasal verbs involving the preposition to raise an ing problem because of the possible confusion with the infinitive marker to. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988), and then measure tions between tagged words, we can identify interesting contrasts between verbs associated with a following sition to~in and verbs associated with a following infinitive marker to~to. (Part of speech notation is borrowed from Francis and Kucera (1982); in = preposition; to = infini-tive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.) The association ratio identifies quite a number of verbs ated in an interesting way with to; restricting our attention to pairs with a score of 3.0 or more, there are 768 verbs associated with the preposition to~in and 551 verbs with the infinitive marker to/to. The ten verbs found to be most associated before to/in are:  to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/ vbg, amounting/vbg, revert/vb, reverted/vbn, resorting/ vbg, relegated/vbn  to~to: obligated/vbn, trying/vbg, compelled/vbn, en-ables/vbz, supposed/vbn, intends/vbz, vowing/vbg, tried/vbd, enabling/vbg, tends/vbz, tend/vb, intend/vb, tries/vbz  Thus, we see there is considerable leverage to be gained by preprocessing the corpus and manipulating the inventory of tokens.  Hindle (Church et al. 1989) has found it helpful to cess the input with the Fidditch parser (Hindle 1983a, 1983b) to identify associations between verbs and ments, and postulate semantic classes for nouns on this basis. Hindle's method is able to find some very interesting associations, as Tables 5 and 6 demonstrate.  After running his parser over the 1988 AP corpus (44 million words), Hindle found N = 4,112,943 subject/verb/ object (SVO) triples. The mutual information between a verb and its object was computed from these 4 million triples by counting how often the verb and its object were found in the same triple and dividing by chance. Thus, for example, disconnect/V and telephone/0 have a joint ability of 7/N. In this case, chance is 84/N x 481/N because there are 84 SVO triples with the verb disconnect, and 481 SVO triples with the object telephone. The mutual information is log z 7N/(84 481) = 9.48. Similarly, the mutual information for drink/Vbeer/O is 9.9 = log 2 29N/ (660 195). (drink/V and beer/O are found in 660 and Table 5. What Can You Drink?  195 SVO triples, respectively; they are found together in 29 of these triples).  This application of Hindle's parser illustrates a second example of preprocessing the input to highlight certain constraints of interest. For measuring syntactic constraints, it may be useful to include some part of speech information and to exclude much of the internal structure of noun phrases. For other purposes, it may be helpful to tag items and/or phrases with semantic labels such as *person*, *place*, *time*, *body part*, *bad*, and so on.  Large machine-readable corpora are only just now ing available to lexicographers. Up to now, lexicographers have been reliant either on citations collected by human  Table 6. What Can You Do to a Telephone?  Verb Object Mutual Info Joint Freq  sit_by/V telephone/O 11.78 7 disconnect/V telephone/O 9.48 7 answer/V telephone/O 8.80 98 hang_up]V telephone/O 7.87 3 tap/V telephone/O 7.69 15 pick_up/V telephone/O 5.63 11 return/V telephone/O 5.01 19 be_by/V telephone/O 4.93 2 spot/V telephone/O 4.43 2 repeat/V telephone/O 4.39 3 place/V telephone/O 4.23 7 receive/V telephone/O 4.22 28 install/V telephone/O 4.20 2 be_on/V telephone/O 4.05 15 come_to/V telephone/O 3.63 6 use/V telephone/O 3.59 29 operate/V telephone/O 3.16 4  readers, which introduced an element of selectivity and so inevitably distortion (rare words and uses were collected but common uses of common words were not), or on small corpora of only a million words or so, which are reliably informative for only the most common uses of the few most frequent words of English. (A million-word corpus such as the Brown Corpus is reliable, roughly, for only some uses of only some of the forms of around 4000 dictionary entries. But standard dictionaries typically contain twenty times this number of entries.)  The computational tools available for studying readable corpora are at present still rather primitive. These are concordancing programs (see Figure 1), which are basically KWIC (key word in context; Aho et al. 1988) indexes with additional features such as the ability to extend the context, sort leftward as well as rightward, and so on. There is very little interactive software. In a typical situation in the lexicography of the 1980s, a lexicographer is giwen the concordances for a word, marks up the printout with colored pens to identify the salient senses, and then writes syntactic descriptions and definitions.  Although this technology is a great improvement on using human readers to collect boxes of citation index cards (tlhe method Murray used in constructing The Oxford English Dictionary a century ago), it works well if there are no more than a few dozen concordance lines for a word, and only two or three main sense divisions. In analyzing a complex word such as take, save, or from, the pher is trying to pick out significant patterns and subtle distinctions that are buried in literally thousands of dance lines: pages and pages of computer printout. The unaided human mind simply cannot discover all the  signifiIs Su~Say, calling for ~x~ater economic reforms to save Oatha ~ poveay.  mmi~:ion asseaed that "" the Postal Se~wice could save enormous sums of money in conwacling out individual e  Then. sl0e said, the family hopes to save enough for a down payment on a boule.  e out-of-work steelworker, "" because that doesn't save jobs, that costs jobs. ""  .... We suspend reality when we say we'll save money by spending $10,000 in wage~ for a public work~  sclent~ts has won the first round in an effort to save one of Egypt's great m:Lsxtre.s, the decaying tomb of R  about three children in a mining town who plot to save the "" pit ponies "" doomed to be slaughtered.  GM executives say the slmtdow~ will save the automaker $500 million a year in operating e~ts a  rtr~ent as receiver, lilstracted officials to U3, to save the m3pany rather than liquidate it and then declared  The package, which is to save the counW/nearly $2 billion, also includes a program  newly enhanced image as the moderate who moved to save the counw/.  mffiina offer from chairman Victor Posner to help save the financially troubled company, but said Pc~er stil  after tellinga delivery-room doctor not to try to save the infant by imsertlnli a tube in its throat to belp i  h bliffiday Tmr~day, cheered by those who fought to save the majestic Beaux Arts arcl~tecmralmE~-telpiece.  at be ~sl formed an alliance with Moslem rebels to save ate nation from commumsm.  ' Basically we could save the operating costs of the Pershing, s and ground-launch  We worked for a year to save the ~te at enormous expense to us, "" said Leveil]ee.  their expet~ive mirrors, just like in wartime, to save them fi~m diamken yankee brawlel~, "" Ta~ said.  ald of many who risked their Own lives in order to save those who were p~=aengers. ""  We must increase tile amount Americans save. ""  Figure 1 Short Sample of the Concordance to ""save"" from the AP 1987 Corpus.  cant patterns, let alone group them and rank them in order of importance.  The AP 1987 concordance to save is many pages long; there are 666 lines for the base form alone, and many more for the inflected forms saved, saves, saving, and savings. In the discussion that follows, we shall, for the sake of ity, not analyze the inflected forms and we shall only look at the patterns to the right of save (see Table 7).  It is hard to know what is important in such a dance and what is not. For example, although it is easy to see from the concordance selection in Figure 1 that the word ""to"" often comes before ""save"" and the word ""the"" often comes after ""save,"" it is hard to say from examination of a concordance alone whether either or both of these co-occurrences have any significance.  Two examples will illustrate how the association ratio measure helps make the analysis both quicker and more accurate.  8.1 EXAMPLE 1: ""SAVE ... FROM""  The association ratios in Table 7 show that association norms apply to function words as well as content words. For example, one of the words significantly associated with save is from. Many dictionaries, for example Webster's Ninth New Collegiate Dictionary (Merriam Webster), make no explicit mention of from in the entry for save, although  Table 7. Words Often Co-Occurring to the Right of""Save""  8.3 7 724 save 447 annually  7.6 64 724 save 6776 money  6.4 6 724 save 1481 thousands  5.0 7 724 save 4590 own  4.6 7 724 save 5798 world  4.6 15 724 save 13010 them  4.5 8 724 save 7434 country  4.2 25 724 save 27367 their  4.1 8 724 save 9249 company  4.1 6 724 save 7114 month  Computational Linguistics Volume 16, Number 1, March 1990  British learners' dictionaries do make specific mention of  from in connection with save. These learners' dictionaries  pay more attention to language structure and collocation  than do American collegiate dictionaries, and  lexicographers trained in the British tradition are often fairly skilled  at spotting these generalizations. However, teasing out  such facts and distinguishing true intuitions from false  intuitions takes a lot of time and hard work, and there is a  high probability of inconsistencies and omissions.  Which other verbs typically associate with from, and  where does save rank in such a list? The association ratio  identified 1530 words that are associated with from; 911 of  them were tagged as verbs. The first 100 verbs are:  fited/vbn, excused/vbd, arising/vbg, range/vb, exempts/  vbz, suffering/vbg, excluded/vbn, marks/vbz, profiting/  vary/vb, exempted/vbn, separate/vb, banished/vbn,  withdrawing/vbg, ferry/vb, prevented/vbn, profit/vb,  exempt/vb, expelled/vbn, withdraw/vb, stem/vb,  separated/vbn, judging/vbg, adapted/vbn, escaping/vbg,  inherited/vbn, differed/vbd, emerged/vbd, withheld/vbd,  leaked/vbn, strip/vb, resulting/vbg, discourage/vb,  prevent/vb, withdrew/vbd, prohibits/vbz, borrowing/vbg,  preventing/vbg, prohibit/vb, resulted/vbd (6.0),  preclude/vb, divert/vb, distinguish/vb, pulled/vbn, fell/  vbg, extract/vb, subtract/vb, recover/vb, paralyzed/  vbn, stole/vbd, departing/vbg, escaped/vbn, prohibited/  Save...from is a good example for illustrating the  advantages of the association ratio. Save is ranked 319th in this  list, indicating that the association is modest, strong enough  to be important (21 times more likely than chance), but not  so strong that it would pop out at us in a concordance, or  that it would be one of the first things to come to mind.  If the dictionary is going to list save.., from, then, for consistency's sake, it ought to consider listing all of the more important associations as well. Of the 27 bare verbs (tagged 'vb') in the list above, all but seven are listed in Collins Cobuild English Language Dictionary as occurring with from. However, this dictionary does not note that vary, ferry, strip, divert, forbid, and reap occur with from. If the Cobuild lexicographers had had access to the posed measure, they could possibly have obtained better coverage at less cost.  8.2 EXAMPLE 2: IDENTIFYING SEMANTIC CLASSES  Having established the relative importance of save ... from, and having noted that the two words are rarely  adjacent, we would now like to speed up the labor-intensive task of categorizing the concordance lines. Ideally, we would like to develop a set of semi-automatic tools that would help a lexicographer produce something like Figure 2, which provides an annotated summary of the 65 dance lines for save ... from. 5 The save ... from pattern occurs in about 10% of the 666 concordance lines for save.  Traditionally, semantic categories have been only vaguely recognized, and to date little effort has been devoted to a systematic classification of a large corpus. Lexicographers have tended to use concordances impressionistically; tic theorists, AI-ers, and others have concentrated on a few interesting examples, e.g. bachelor, and have not given much thought to how the results might be scaled up.  With this concern in mind, it seems reasonable to ask how well these 65 lines for save...from fit in with all other uses of save A laborious concordance analysis was taken to answer this question. When it was nearing tion, we noticed that the tags that we were inventing to capture the generalizations could in most cases have been suggested by looking at the lexical items listed in the association ratio table for save. For example, we had failed to notice the significance of time adverbials in our analysis of save, and no dictionary records this. Yet it should be  ( Robert DeNiro ) to save Indian tribes(PERSON] from genocide[DESTRUCT[BAD]] at the hands of  "" We wanted to save him(PERSON] ~orn undue ~ouble[BAD] and loss(BAD] of money , ""  Murphy was sacrificed to save more powerful Democrats(PERSON] from harm(BAD] .  "" God sent this man to save my five children(PERSON] from being burned to death(DESTRUCT(BAD]] and  Pope John Paul I] to "" save us(PERSON] fl~m sin(BAD] . ""  rescuers who helped save the toddler(PERSON] from an abandoned weU[LOC] will be feted with a parade  while attempting to save two drowning hoys[PERSON] from a turbulent(BAD] creeklLOC] in Otdo[LOC]  member states to help save the EEC[INSTI from possible bankaxlptcy[BCON][BAD] this year.  should be sought "" to save the compeny[CORP[1NST]] from bankmptfy[BCON][BAD].  law was necessary to save the counffy[NATIOlq[lNST]] flora disaster(BAD].  operation "" to save the nation(NATION(INS'r]] from COmmUnL~n[BAD][POL1TICAL] .  were not needed to save the system from benkauptcy[ECON][BAD].  his efforts to save the wodd[INST] from the like~ of Lothax and the Spider Woman  give them the money to save the dogs(ANIMAL] from being destroyed(DESTRUCT] ,  program intended to save the giant birds(ANIMAL] ~om extinction[DESTRUCTI,  walnut and ash tx~es to save them from the axes and saws of a logging company.  after the a~aek to save the ship from a temble[BAD] fire, Navy reports concluded Thursday.  cemficates that would save shopper~[pERSON] anywhere f~m $50[MONEY] [NUMBER] to $500[MONEY] (/flu  Figure 2 Some AP 1987 Concordance Lines to ""save...from, "" Roughly Sorted into Categories.  clear fi'om the association ratio table above that annually and month 6 are commonly found with save. More detailed inspection shows that the time adverbials correlate ingly with just one group of save objects, namely those tagged [MONEY]. The AP wire is full of discussions of saving $1.2 billion per month; computational lexicography should measure and record such patterns if they are eral, even when traditional dictionaries do not.  A,; another example illustrating how the association ratio tables would have helped us analyze the save concordance lines, we found ourselves contemplating the semantic tag ENV(IRONMENT) to analyze lines such as:  the trend to save the forests[ENV]  it's our turn to save the lake[ENV],  joined a fight to save their forests[ENV],  can we get busy to save the planet[ENV] ?  If we had looked at the association ratio tables before labC.ing the 65 lines for save ... from, we might have noticed the very large value for save.., forests, suggesting that there may be an important pattern here. In fact, this pattern probably subsumes most of the occurrences of the ""save [ANIMAL]"" pattern noticed in Figure 2. Thus, these tables do not provide semantic tags, but they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags.  It may be that everything said here about save and other  words is true only of 1987 American journalese. Intuitively,  however, many of the patterns discovered seem to be good  candidates for conventions of general English. A future  step would be to examine other more balanced corpora and  test how well the patterns hold up.  9 CONCLUSIONS  We began this paper with the psycholinguistic notion of  word association norm, and extended that concept toward  the information theoretic definition of mutual information.  This provided a precise statistical calculation that could be  applied to a very large corpus of text to produce a table of  associations for tens of thousands of words. We were then  able to show that the table encoded a number of very  interesting patterns ranging from doctor.., nurse to save  ....from. We finally concluded by showing how the  patterns in the association ratio table might help a  lexicographer organize a concordance.  In point of fact, we actually developed these results in  basically the reverse order. Concordance analysis is still  extremely labor-intensive and prone to errors of omission.  The ways that concordances are sorted don't adequately  support current lexicographic practice. Despite the fact  that a concordance is indexed by a single word, often  lexicographers actually use a second word such as from or  an equally common semantic concept such as a time  adverbial to decide how to categorize concordance lines. In other  words, they use two words to triangulate in on a word sense.  This triangulation approach clusters concordance lines  together into word senses based primarily on usage  (distributional evidence), as opposed to intuitive notions of meaning.  Thus, the question of what is a word sense can be addressed  with syntactic methods (symbol pushing), and need not  address semantics (interpretation), even though the  inventory of tags may appear to have semantic values.  The triangulation approach requires ""art."" How does the  lexicographer decide which potential cut points are  ""interesting"" and which are merely due to chance? The  proposed association ratio score provides a practical and  objective measure that is often a fairly good approximation  to the ""art."" Since the proposed measure is objective, it can  be applied in a systematic way over a large body of  material, steadily improving consistency and productivity.  But on the other hand, the objective score can be ing. The score takes only distributional evidence into ac-count. For example, the measure favors set ... for over set ... down; it doesn't know that the former is less interesting because its semantics are compositional. In addition, the measure is extremely superficial; it cannot cluster words into appropriate syntactic classes without an explicit preprocess such as Church's parts program or Hindle's parser. Neither of these preprocesses, though, can help highlight the ""natural"" similarity between nouns such as picture and photograph. Although one might imagine a preprocess that would help in this particular case, there will probably always be a class of generalizations that are obvious to an intelligent lexicographer, but lie hopelessly beyond the objectivity of a computer.  Despite these problems, the association ratio could be an important tool to aid the lexicographer, rather like an index to the concordances. It can help us decide what to look for; it provides a quick summary of what company our words do keep.  Church, K. 1988 ""A Stochastic Parts Program and Noun Phrase Parser  for Unrestricted Text,"" Second Conference on Applied Natural  Language Processing, Austin, TX. Church, K.; Gale, W.; Hanks, P.; and Hindle, D. 1989 ""Parsing, Word  Associations and Typical Predicate-Argument Relations,""  International Workshop on Parsing Technologies, CMU.  Fano, R. 1961 Transmission of Information: A Statistical Theory of Communications. MIT Press, Cambridge, MA. Firth, J. 1957 ""A Synopsis of Linguistic Theory 1930-1955,"" in Studies in Linguistic Analysis, Philological Society, Oxford; reprinted in Palmer,  F. (ed.) 1968 Selected Papers of J. R. Firth, Longman, Harlow. Francis, W. and Ku~era, H. 1982 Frequency Analysis of English Usage. Houghton Mifflin Company, Boston, MA. Good, I. J. 1953 The Population Frequencies of Species and the  Estimation of Population Parameters. Biometrika, Vol. 40, 237-264. Hanks, P. 1987 ""Definitions and Explanations,"" in J. Sinclair (ed.),  Looking Up: An Account of the COBUILD Project in Lexical ing. Collins, London and Glasgow.  Hindle, D. 1983a ""Deterministic Parsing of Syntactic Non-fluencies."" In  Proceedings of the 23rd Annual Meeting of the Association for tational Linguistics.  Hindle, D. 1983b ""User Manual for Fidditch, a Deterministic Parser."" Naval Research Laboratory Technical Memorandum #7590-142. Hornby, A. 1948 The Advanced Learner's Dictionary, Oxford University  Press, Oxford, U.K. Jelinek, F. 1982. (personal communication) Kahan, S.; Pavlidis, T.; and Baird, H. 1987 ""On the Recognition of  Printed Characters of any Font or Size,"" IEEE Transactions PAMI, 274-287.  Meyer, D.; Schvaneveldt, R.; and Ruddy, M. 1975 ""Loci of Contextual Effects on Visual Word-Recognition,"" in P. Rabbitt and S. Dornic (eds.), Attention and Performance V, Academic Press, New York.  Palermo, D. and Jenkins, J. 1964 ""Word AssociationNorms."" University of Minnesota Press, Minneapolis, MN.  Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; and Stock, P. (eds.) 1987a Collins Cobuild English Language Dictionary. Collins, London and Glasgow.  Sinclair, J. 1987b ""The Nature of the Evidence,"" in J. Sinclair (ed.),  Looking Up: An Account of the COBUILD Project in Lexical  Computing. Collins, London and Glasgow.  Smadja, F. In press. ""Microcoding the Lexicon with Co-Occurrence Knowledge,"" in Zernik (ed.), Lexical Acquisition: Using On-Line sources to Build a Lexicon, MIT Press, Cambridge, MA.  This statistic has also been used by the IBM speech group (Jelinek 1982) for constructing language models for applications in speech recognition.  Smadja (in press) discusses the separation between collocates in a very similar way.  This definition fw(x,y) uses a rectangular window. It might be interesting to consider alternatives (e.g. a triangular window or a decaying exponential) that would weight words less and less as they are separated by more and more words. Other windows are also possible. For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest.  Although the Good-Turing Method (Good 1953) is more than 35 years old, it is still heavily cited. For example, Katz (1987) uses the method in order to estimate trigram probabilities in the IBM speech recognizer. The Good-Turing Method is helpful for trigrams that have not been seen very often in the training corpus.  The last unclassified line .... save shoppers anywhere from $50... raises interesting problems. Syntactic ""chunking"" shows that, in spite of its co-occurrence of from with save, this line does not belong here. An intriguing exercise, given the lookup table we are trying to construct, is how to guard against false inferences such as that since shoppers is tagged [PERSON], $50 to $500 must here count as either BAD or a LOCATION. Accidental coincidences of this kind do not have a significant effect on the measure, however, although they do serve as a reminder of the probabilistic nature of the findings.  The word time itself also occurs significantly in the table, but on closer examination it is clear that this use of time (e.g. to save time) counts as something like a commodity or resource, not as part of a time adjunct. Such are the pitfalls of lexicography (obvious when they are pointed out). ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.)We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.(The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.)The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.It is common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their co-occurrence with other words.Running through the whole Firthian tradition, for example, is the theme that ""You shall know a word by the company it keeps"" (Firth, 1957).On the one hand, bank co-occurs with words and sion such as money, notes, loan, account, investment, clerk, official, manager, robbery, vaults, working in a, its actions, First National, of England, and so forth.On the other hand, we find bank co-occurring with river, swim, boat, east (and of course West and South, which have acquired special meanings of their own), on top of the, and of the Rhine.(Hanks 1987, p. 127)  The search for increasingly delicate word classes is not new.In lexicography, for example, it goes back at least to the ""verb patterns"" described in Hornby's Advanced Learner's Dictionary (first edition 1948).What is new is that ties for the computational storage and analysis of large bodies of natural language have developed significantly in recent years, so that it is now becoming possible to test and apply informal assertions of this kind in a more rigorous way, and to see what company our words do keep.The proposed statistical description has a large number of potentially important applications, including: (a) ing the language model both for speech recognition and optical character recognition (OCR), (b) providing biguation cues for parsing highly ambiguous syntactic tures such as noun compounds, conjunctions, and tional phrases, (c) retrieving texts from large databases  (e.g. newspapers, patents), (d) enhancing the productivity of computational linguists in compiling lexicons of synWctic facts, and (e) enhancing the productivity of cographers in identifying normal and conventional usage.Consider the optical character recognizer (OCR) cation.Suppose that we have an OCR device as in Kahan et al.(1987), and it has assigned about equal probability to having recognized farm and form, where the context is either: (1) federal credit or (2) some of.  federal ~form ] credit  The proposed association measure can make use of the fact that farm is much more likely in the first context and form is much more likely in the second to resolve the ambiguity.Note that alternative disambiguation methods based on syntactic constraints such as part of speech are unlikely to help in this case since both form and farm are commonly used as nouns.Word association norms are well known to be an important factor in psycholinguistic research, especially in the area of lexical retrieval.Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.Some results and implications are summarized from  reaction-time experiments in which subjects either (a)  classified successive strings of letters as words and  nonwords, or (b) pronounced the strings.Both types of  response to words (e.g. BUTTER) were consistently  faster when preceded by associated words (e.g. BREAD)  rather than unassociated words (e.g. NURSE) (Meyer  et al. 1975, p.98) Much of this psycholinguistic research is based on cal estimates of word association norms as in Palermo and Jenkins (1964), perhaps the most influential study of its kind, though extremely small and somewhat dated.This study measured 200 words by asking a few thousand jects to write down a word after each of the 200 words to be measured.Results are reported in tabular form, indicating which words were written down, and by how many subjects, factored by grade level and sex.The word doctor, for example, is reported on pp.98-100 to be most often ated with nurse, followed by sick, health, medicine, tal, man, sickness, lawyer, and about 70 more words.We propose an alternative measure, the association ratio, for measuring word association norms, based on the mation theoretic concept of mutual information.1 The proposed measure is more objective and less costly than the subjective method employed in Palermo and Jenkins (1964).The association ratio can be scaled up to provide robust estimates of word association norms for a large portion of the language.Using the association ratio measure, the five most associated words are, in order: dentists, nurses, ing, treat, and hospitals.What is ""mutual information?""According to Fano (1961), if two points (words), x and y, have probabilities P(x) and P(y), then their mutual information, I(x,y), is defined to be  Informally, mutual information compares the probability of observing x and y together (the joint probability) with the probabilities of observing x and y independently (chance).If there is a genuine association between x and y, then the joint probability P(x,y) will be much larger than chance P(x) P(y), and consequently I(x,y) >> 0.If there is no interesting relationship between x and y, then P(x,y) P(x) P(y), and thus, I(x,y) ~ O.If x and y are in mentary distribution, then P(x,y) will be much less than P(x) P(y), forcing I(x,y) << 0.In our application, word probabilities P(x) and P(y) are estimated by counting the number of observations of x and y in a corpus, f (x) andf(y), and normalizing by N, the size of the corpus.(Our examples use a number of different corpora with different sizes: 15 million words for the 1987 AP corpus, 36 million words for the 1988 AP corpus, and  8.6 million tokens for the tagged corpus.)Joint ties, P(x,y), are estimated by counting the number of times that x is followed by y in a window of w words, fw (x,y), and normalizing by N.  The window size parameter allows us to look at different scales.Smaller window sizes will identify fixed expressions (idioms such as bread and butter) and other relations that hold over short ranges; larger window sizes will highlight semantic concepts and other relationships that hold over larger scales.Table 1 may help show the contrast.2 In fixed sions, such as bread and butter and drink and drive, the words of interest are separated by a fixed number of words and there is very little variance.In the 1988 AP, it was found that the two words are always exactly two words apart whenever they are found near each other (within five words).That is, the mean separation is two, and the variance is zero.Compounds also have very fixed word order (little ance), but the average separation is closer to one word rather than two.In contrast, relations such as man/woman are less fixed, as indicated by a larger variance in their separation.(The nearly zero value for the mean separation for man/women indicates the words appear about equally  Table 1.Mean and Variance of the Separation Between X and Y  Separation often in either order.)Lexical relations come in several varieties.There are some like refraining from that are fairly fixed, others such as coming from that may be separated by an argument, and still others like keeping from that are almost certain to be separated by an ment.Relation Word x Word y Mean Variance  Compound computer scientist 1.12 O.I 0  United States 0.98 0.14  Semantic man woman 1.46 8.07  Lexical refraining from 1.11 0.20  coming from 0.83 2.89  The ideal window size is different in each case.For the  remainder of this paper, the window size, w, will be set to  five words as a compromise; this setting is large enough to  show some of the constraints between verbs and arguments,  but not so large that it would wash out constraints that  make use of strict adjacency)  Since the association ratio becomes unstable when the counts are very small, we will not discuss word pairs with f(x,y) _< 5.An improvement would make use of t-scores, and throw out pairs that were not significant.nately, this requires an estimate of the variance off(x,y), which goes beyond the scope of this paper.For the der of this paper, we will adopt the simple but arbitrary threshold, and ignore pairs with small counts.Technically, the association ratio is different from  mutual information in two respects.First, joint probabilities  are supposed to be symmetric: P(x,y) = P(y, x), and  thus, mutual information is also symmetric: I(x,y) =  I(y, x).However, the association ratio is not symmetric,  sincef(x, y) encodes linear precedence.(Recall thatf(x, y)  denotes the number of times that word x appears before y  in the window of w words, not the number of times the two  words appear in either order.)Although we could fix this  problem by redefiningf(x, y) to be symmetric (by  averaging the matrix with its transpose), we have decided not to  do so, since order information appears to be very  interesting.Notice the asymmetry in the pairs in Table 2  (computed from 44 million words of 1988 AP text), illustrating a  wide variety of biases ranging from sexism to syntax.Second, one might expect f(x, y) <_ f(x) and f(x, y) <_ f(y), but the way we have been counting, this needn't be the case if x and y happen to appear several times in the window.For example, given the sentence, ""Library ers were prohibited from saving books from this heap of ruins,"" which appeared in an AP story on April 1, 1988, f(prohibited) = 1 and f(prohibited, from) = 2.This problem can be fixed by dividingf(x, y) by w -1 (which has the consequence of subtracting log2 (w -1) = 2 from our association ratio scores).This adjustment has the  addiTable 2.Asymmetry in 1988 AP Corpus (N = 44 million)  doctors lawyers 29 19  save money 187 11  tional beneft of assuring that Z f(x,y) = ~ f(x) = Zf(y) = N.When I(x, y) is large, the association ratio produces very credible results not unlike those reported in Palermo and Jenkins (1964), as illustrated in Table 3.In contrast, when I(x, y) ---: 0, the pairs are less interesting.(As a very rough rule; of thumb, we have observed that pairs with I(x, y) > 3 tend to be interesting, and pairs with smaller I(x, y) are generally not.One can make this statement precise by calibrating the measure with subjective measures.Alternatively, one could make estimates of the variance and then make statements about confidence levels, e.g. with 95% confidence, P(x, y) > e(x) P(y).)If I(x, y) << 0, we would predict that x and y are in complementary distribution.However, we are rarely able to observe I(x, y) << 0 because our corpora are too small (and our measurement techniques are too crude).Suppose, for example, that both x and y appear about 10 times per million words of text.Then, P(x) = P(y) = 10 -5 and chance is P(x) P(x) = 10 -I . Thus, to say that I(x, y) is much less than 0, we need to say that P(x, y) is much less than 10 -t , a statement that is hard to make with much confidence given the size of presently available corpora.In fact, we cannot (easily) observe a probability less than  1/N ~ 10 -7, and therefore it is hard to know if I(x, y) is  much less than chance or not, unless chance is very large.(In fact, the pair a...doctors in Table 3, appears  significantly less often than chance.But to justify this statement,  we need to compensate for the window size (which shifts  the score downward by 2.0, e.g. from 0.96 down to -1.04),  and we need to estimate the standard deviation, using a  method such as Good (1953).4  Although the psycholinguistic literature documents the significance of noun/noun word associations such as doctor/ nurse in considerable detail, relatively little is said about  Table 3.Some interesting Associations with ""Doctor"" in the  1987 AP Corpus (N = 15 million)  11.3 12 111 honorary 621 doctor  9.4 8 1105 doctors 154 treating  9.0 6 275 examined 621 doctor  8.9 11 1105 doctors 317 treat  Some Uninteresting Associations with ""Doctor""  0.96 6 621 doctor 73785 with  0.93 12 84716 is 1105 doctors  associations among verbs, function words, adjectives, and other non-nouns.In addition to identifying semantic tions of the doctor/nurse variety, we believe the association ratio can also be used to search for interesting lexico-syntactic relationships between verbs and typical argu-ments/adjuncts.The proposed association ratio can be viewed as a formalization of Sinclair's argument:  How common are the phrasal verbs with set?Set is particularly rich in making combinations with words like about, in, up, out, on, off, and these words are themselves very common.How likely is set offto occur?Both are frequent words [set occurs approximately 250 times in a million words and off occurs approximately 556 times in a million words...[T]he question we are asking can be roughly rephrased as follows: how likely is off to occur immediately after set?...This is 0.00025 x 0.00055 [P(x) P(y)], which gives us the tiny figure of 0.0000001375 ...The assumption behind this tion is that the words are distributed at random in a text [at chance, in our terminology].It is obvious to a linguist that this is not so, and a rough measure of how much set and offattract each other is to compare the probability with what actually happens ...Set off occurs nearly 70 times in the 7.3 million word corpus [P(x, y) = 70/(7.3 x 106) >> P(x) P(y)].That is enough to show its main patterning and it suggests that in currently-held corpora there will be found sufficient evidence for the description of a substantial collection of phrases ...(Sinclair 1987c, pp. 151-152).Using Sinclair's estimates P(set) ~ 250 x 10 -6, P(off) ~-556 x 10 -6, and P(set, off) ~ 70/(7.3 x 106), we would estimate the mutual information to be I(set; off) = log2P(set, off)/(P(set) P(off)) ~ 6.1.In the 1988 AP corpus (N = 44,344,077), we estimate P(set) ~ 13,046/N, P(off) ~ 20,693/N, and P(set, off) ~ 463/N.Given these estimates, we would compute the mutual information to be l(set; off) ~ 6.2.In this example, at least, the values seem to be fairly comparable across corpora.In other examples, we will see some differences due to sampling.Sinclair's corpus is a fairly balanced sample of (mainly British) text; the AP corpus is an unbalanced sample of American journalese.This association between set and offis relatively strong; the joint probability is more than 26 = 64 times larger than chance.The other particles that Sinclair mentions have association ratios that can be seen in Table 4.The first three, set up, set off, and set out, are clearly  Table 4.Some Phrasal Verbs in 1988 AP Corpus (N = 44 million)  set up 13,046 64,601 2713 7.3 set off 13,046 20,693 463 6.2 set out 13,046 47,956 301 4.4 set on 13,046 258,170 162 1.1 set in 13,046 739,932 795 1.8 set about 13,046 82,319 16 -0.6  associated; the last three are not so clear.As Sinclair suggests, the approach is well suited for identifying the phrasal verbs, at least in certain cases.6 PREPROCESSING WITH A PART OF SPEECH TAGGER  Phrasal verbs involving the preposition to raise an ing problem because of the possible confusion with the infinitive marker to.We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988), and then measure tions between tagged words, we can identify interesting contrasts between verbs associated with a following sition to~in and verbs associated with a following infinitive marker to~to.(Part of speech notation is borrowed from Francis and Kucera (1982); in = preposition; to = infini-tive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.)The association ratio identifies quite a number of verbs ated in an interesting way with to; restricting our attention to pairs with a score of 3.0 or more, there are 768 verbs associated with the preposition to~in and 551 verbs with the infinitive marker to/to.The ten verbs found to be most associated before to/in are:  to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/ vbg, amounting/vbg, revert/vb, reverted/vbn, resorting/ vbg, relegated/vbn  to~to: obligated/vbn, trying/vbg, compelled/vbn, en-ables/vbz, supposed/vbn, intends/vbz, vowing/vbg, tried/vbd, enabling/vbg, tends/vbz, tend/vb, intend/vb, tries/vbz  Thus, we see there is considerable leverage to be gained by preprocessing the corpus and manipulating the inventory of tokens.Hindle (Church et al. 1989) has found it helpful to cess the input with the Fidditch parser (Hindle 1983a, 1983b) to identify associations between verbs and ments, and postulate semantic classes for nouns on this basis.Hindle's method is able to find some very interesting associations, as Tables 5 and 6 demonstrate.After running his parser over the 1988 AP corpus (44 million words), Hindle found N = 4,112,943 subject/verb/ object (SVO) triples.The mutual information between a verb and its object was computed from these 4 million triples by counting how often the verb and its object were found in the same triple and dividing by chance.Thus, for example, disconnect/V and telephone/0 have a joint ability of 7/N.In this case, chance is 84/N x 481/N because there are 84 SVO triples with the verb disconnect, and 481 SVO triples with the object telephone.The mutual information is log z 7N/(84 481) = 9.48.Similarly, the mutual information for drink/Vbeer/O is 9.9 = log 2 29N/ (660 195).(drink/V and beer/O are found in 660 and Table 5.What Can You Drink?195 SVO triples, respectively; they are found together in 29 of these triples).This application of Hindle's parser illustrates a second example of preprocessing the input to highlight certain constraints of interest.For measuring syntactic constraints, it may be useful to include some part of speech information and to exclude much of the internal structure of noun phrases.For other purposes, it may be helpful to tag items and/or phrases with semantic labels such as *person*, *place*, *time*, *body part*, *bad*, and so on.Large machine-readable corpora are only just now ing available to lexicographers.Up to now, lexicographers have been reliant either on citations collected by human  Table 6.What Can You Do to a Telephone?Verb Object Mutual Info Joint Freq  sit_by/V telephone/O 11.78 7 disconnect/V telephone/O 9.48 7 answer/V telephone/O 8.80 98 hang_up]V telephone/O 7.87 3 tap/V telephone/O 7.69 15 pick_up/V telephone/O 5.63 11 return/V telephone/O 5.01 19 be_by/V telephone/O 4.93 2 spot/V telephone/O 4.43 2 repeat/V telephone/O 4.39 3 place/V telephone/O 4.23 7 receive/V telephone/O 4.22 28 install/V telephone/O 4.20 2 be_on/V telephone/O 4.05 15 come_to/V telephone/O 3.63 6 use/V telephone/O 3.59 29 operate/V telephone/O 3.16 4  readers, which introduced an element of selectivity and so inevitably distortion (rare words and uses were collected but common uses of common words were not), or on small corpora of only a million words or so, which are reliably informative for only the most common uses of the few most frequent words of English.(A million-word corpus such as the Brown Corpus is reliable, roughly, for only some uses of only some of the forms of around 4000 dictionary entries.But standard dictionaries typically contain twenty times this number of entries.)The computational tools available for studying readable corpora are at present still rather primitive.These are concordancing programs (see Figure 1), which are basically KWIC (key word in context; Aho et al.1988) indexes with additional features such as the ability to extend the context, sort leftward as well as rightward, and so on.There is very little interactive software.In a typical situation in the lexicography of the 1980s, a lexicographer is giwen the concordances for a word, marks up the printout with colored pens to identify the salient senses, and then writes syntactic descriptions and definitions.Although this technology is a great improvement on using human readers to collect boxes of citation index cards (tlhe method Murray used in constructing The Oxford English Dictionary a century ago), it works well if there are no more than a few dozen concordance lines for a word, and only two or three main sense divisions.In analyzing a complex word such as take, save, or from, the pher is trying to pick out significant patterns and subtle distinctions that are buried in literally thousands of dance lines: pages and pages of computer printout.The unaided human mind simply cannot discover all the  signifiIs Su~Say, calling for ~x~ater economic reforms to save Oatha ~ poveay.mmi~:ion asseaed that "" the Postal Se~wice could save enormous sums of money in conwacling out individual e  Then.sl0e said, the family hopes to save enough for a down payment on a boule.e out-of-work steelworker, "" because that doesn't save jobs, that costs jobs.""  ....We suspend reality when we say we'll save money by spending $10,000 in wage~ for a public work~  sclent~ts has won the first round in an effort to save one of Egypt's great m:Lsxtre.s, the decaying tomb of R  about three children in a mining town who plot to save the "" pit ponies "" doomed to be slaughtered.GM executives say the slmtdow~ will save the automaker $500 million a year in operating e~ts a  rtr~ent as receiver, lilstracted officials to U3, to save the m3pany rather than liquidate it and then declared  The package, which is to save the counW/nearly $2 billion, also includes a program  newly enhanced image as the moderate who moved to save the counw/.mffiina offer from chairman Victor Posner to help save the financially troubled company, but said Pc~er stil  after tellinga delivery-room doctor not to try to save the infant by imsertlnli a tube in its throat to belp i  h bliffiday Tmr~day, cheered by those who fought to save the majestic Beaux Arts arcl~tecmralmE~-telpiece.at be ~sl formed an alliance with Moslem rebels to save ate nation from commumsm.' Basically we could save the operating costs of the Pershing, s and ground-launch  We worked for a year to save the ~te at enormous expense to us, "" said Leveil]ee.  their expet~ive mirrors, just like in wartime, to save them fi~m diamken yankee brawlel~, "" Ta~ said.ald of many who risked their Own lives in order to save those who were p~=aengers.""  We must increase tile amount Americans save.""  Figure 1 Short Sample of the Concordance to ""save"" from the AP 1987 Corpus.cant patterns, let alone group them and rank them in order of importance.The AP 1987 concordance to save is many pages long; there are 666 lines for the base form alone, and many more for the inflected forms saved, saves, saving, and savings.In the discussion that follows, we shall, for the sake of ity, not analyze the inflected forms and we shall only look at the patterns to the right of save (see Table 7).It is hard to know what is important in such a dance and what is not.For example, although it is easy to see from the concordance selection in Figure 1 that the word ""to"" often comes before ""save"" and the word ""the"" often comes after ""save,"" it is hard to say from examination of a concordance alone whether either or both of these co-occurrences have any significance.Two examples will illustrate how the association ratio measure helps make the analysis both quicker and more accurate.8.1 EXAMPLE 1: ""SAVE ...FROM""  The association ratios in Table 7 show that association norms apply to function words as well as content words.For example, one of the words significantly associated with save is from.Many dictionaries, for example Webster's Ninth New Collegiate Dictionary (Merriam Webster), make no explicit mention of from in the entry for save, although  Table 7.Words Often Co-Occurring to the Right of""Save""  8.3 7 724 save 447 annually  7.6 64 724 save 6776 money  6.4 6 724 save 1481 thousands  5.0 7 724 save 4590 own  4.6 7 724 save 5798 world  4.6 15 724 save 13010 them  4.5 8 724 save 7434 country  4.2 25 724 save 27367 their  4.1 8 724 save 9249 company  4.1 6 724 save 7114 month  Computational Linguistics Volume 16, Number 1, March 1990  British learners' dictionaries do make specific mention of  from in connection with save.These learners' dictionaries  pay more attention to language structure and collocation  than do American collegiate dictionaries, and  lexicographers trained in the British tradition are often fairly skilled  at spotting these generalizations.However, teasing out  such facts and distinguishing true intuitions from false  intuitions takes a lot of time and hard work, and there is a  high probability of inconsistencies and omissions.Which other verbs typically associate with from, and  where does save rank in such a list?The association ratio  identified 1530 words that are associated with from; 911 of  them were tagged as verbs.The first 100 verbs are:  fited/vbn, excused/vbd, arising/vbg, range/vb, exempts/  vbz, suffering/vbg, excluded/vbn, marks/vbz, profiting/  vary/vb, exempted/vbn, separate/vb, banished/vbn,  withdrawing/vbg, ferry/vb, prevented/vbn, profit/vb,  exempt/vb, expelled/vbn, withdraw/vb, stem/vb,  separated/vbn, judging/vbg, adapted/vbn, escaping/vbg,  inherited/vbn, differed/vbd, emerged/vbd, withheld/vbd,  leaked/vbn, strip/vb, resulting/vbg, discourage/vb,  prevent/vb, withdrew/vbd, prohibits/vbz, borrowing/vbg,  preventing/vbg, prohibit/vb, resulted/vbd (6.0),  preclude/vb, divert/vb, distinguish/vb, pulled/vbn, fell/  vbg, extract/vb, subtract/vb, recover/vb, paralyzed/  vbn, stole/vbd, departing/vbg, escaped/vbn, prohibited/  Save...from is a good example for illustrating the  advantages of the association ratio.Save is ranked 319th in this  list, indicating that the association is modest, strong enough  to be important (21 times more likely than chance), but not  so strong that it would pop out at us in a concordance, or  that it would be one of the first things to come to mind.If the dictionary is going to list save.., from, then, for consistency's sake, it ought to consider listing all of the more important associations as well.Of the 27 bare verbs (tagged 'vb') in the list above, all but seven are listed in Collins Cobuild English Language Dictionary as occurring with from.However, this dictionary does not note that vary, ferry, strip, divert, forbid, and reap occur with from.If the Cobuild lexicographers had had access to the posed measure, they could possibly have obtained better coverage at less cost.8.2 EXAMPLE 2: IDENTIFYING SEMANTIC CLASSES  Having established the relative importance of save ... from, and having noted that the two words are rarely  adjacent, we would now like to speed up the labor-intensive task of categorizing the concordance lines.Ideally, we would like to develop a set of semi-automatic tools that would help a lexicographer produce something like Figure 2, which provides an annotated summary of the 65 dance lines for save ... from.5 The save ... from pattern occurs in about 10% of the 666 concordance lines for save.Traditionally, semantic categories have been only vaguely recognized, and to date little effort has been devoted to a systematic classification of a large corpus.Lexicographers have tended to use concordances impressionistically; tic theorists, AI-ers, and others have concentrated on a few interesting examples, e.g. bachelor, and have not given much thought to how the results might be scaled up.With this concern in mind, it seems reasonable to ask how well these 65 lines for save...from fit in with all other uses of save A laborious concordance analysis was taken to answer this question.When it was nearing tion, we noticed that the tags that we were inventing to capture the generalizations could in most cases have been suggested by looking at the lexical items listed in the association ratio table for save.For example, we had failed to notice the significance of time adverbials in our analysis of save, and no dictionary records this.Yet it should be  ( Robert DeNiro ) to save Indian tribes(PERSON] from genocide[DESTRUCT[BAD]] at the hands of  "" We wanted to save him(PERSON] ~orn undue ~ouble[BAD] and loss(BAD] of money , ""  Murphy was sacrificed to save more powerful Democrats(PERSON] from harm(BAD] .  "" God sent this man to save my five children(PERSON] from being burned to death(DESTRUCT(BAD]] and  Pope John Paul I] to "" save us(PERSON] fl~m sin(BAD] . ""  rescuers who helped save the toddler(PERSON] from an abandoned weU[LOC] will be feted with a parade  while attempting to save two drowning hoys[PERSON] from a turbulent(BAD] creeklLOC] in Otdo[LOC]  member states to help save the EEC[INSTI from possible bankaxlptcy[BCON][BAD] this year.should be sought "" to save the compeny[CORP[1NST]] from bankmptfy[BCON][BAD].law was necessary to save the counffy[NATIOlq[lNST]] flora disaster(BAD].operation "" to save the nation(NATION(INS'r]] from COmmUnL~n[BAD][POL1TICAL] .  were not needed to save the system from benkauptcy[ECON][BAD].his efforts to save the wodd[INST] from the like~ of Lothax and the Spider Woman  give them the money to save the dogs(ANIMAL] from being destroyed(DESTRUCT] ,  program intended to save the giant birds(ANIMAL] ~om extinction[DESTRUCTI,  walnut and ash tx~es to save them from the axes and saws of a logging company.after the a~aek to save the ship from a temble[BAD] fire, Navy reports concluded Thursday.cemficates that would save shopper~[pERSON] anywhere f~m $50[MONEY] [NUMBER] to $500[MONEY] (/flu  Figure 2 Some AP 1987 Concordance Lines to ""save...from, "" Roughly Sorted into Categories.clear fi'om the association ratio table above that annually and month 6 are commonly found with save.More detailed inspection shows that the time adverbials correlate ingly with just one group of save objects, namely those tagged [MONEY].The AP wire is full of discussions of saving $1.2 billion per month; computational lexicography should measure and record such patterns if they are eral, even when traditional dictionaries do not.A,; another example illustrating how the association ratio tables would have helped us analyze the save concordance lines, we found ourselves contemplating the semantic tag ENV(IRONMENT) to analyze lines such as:  the trend to save the forests[ENV]  it's our turn to save the lake[ENV],  joined a fight to save their forests[ENV],  can we get busy to save the planet[ENV] ?  If we had looked at the association ratio tables before labC.ing the 65 lines for save ... from, we might have noticed the very large value for save.., forests, suggesting that there may be an important pattern here.In fact, this pattern probably subsumes most of the occurrences of the ""save [ANIMAL]"" pattern noticed in Figure 2.Thus, these tables do not provide semantic tags, but they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags.It may be that everything said here about save and other  words is true only of 1987 American journalese.Intuitively,  however, many of the patterns discovered seem to be good  candidates for conventions of general English.A future  step would be to examine other more balanced corpora and  test how well the patterns hold up.9 CONCLUSIONS  We began this paper with the psycholinguistic notion of  word association norm, and extended that concept toward  the information theoretic definition of mutual information.This provided a precise statistical calculation that could be  applied to a very large corpus of text to produce a table of  associations for tens of thousands of words.We were then  able to show that the table encoded a number of very  interesting patterns ranging from doctor.., nurse to save  ....from.We finally concluded by showing how the  patterns in the association ratio table might help a  lexicographer organize a concordance.In point of fact, we actually developed these results in  basically the reverse order.Concordance analysis is still  extremely labor-intensive and prone to errors of omission.The ways that concordances are sorted don't adequately  support current lexicographic practice.Despite the fact  that a concordance is indexed by a single word, often  lexicographers actually use a second word such as from or  an equally common semantic concept such as a time  adverbial to decide how to categorize concordance lines.In other  words, they use two words to triangulate in on a word sense.This triangulation approach clusters concordance lines  together into word senses based primarily on usage  (distributional evidence), as opposed to intuitive notions of meaning.Thus, the question of what is a word sense can be addressed  with syntactic methods (symbol pushing), and need not  address semantics (interpretation), even though the  inventory of tags may appear to have semantic values.The triangulation approach requires ""art.""How does the  lexicographer decide which potential cut points are  ""interesting"" and which are merely due to chance?The  proposed association ratio score provides a practical and  objective measure that is often a fairly good approximation  to the ""art.""Since the proposed measure is objective, it can  be applied in a systematic way over a large body of  material, steadily improving consistency and productivity.But on the other hand, the objective score can be ing.The score takes only distributional evidence into ac-count.For example, the measure favors set ...for over set ...down; it doesn't know that the former is less interesting because its semantics are compositional.In addition, the measure is extremely superficial; it cannot cluster words into appropriate syntactic classes without an explicit preprocess such as Church's parts program or Hindle's parser.Neither of these preprocesses, though, can help highlight the ""natural"" similarity between nouns such as picture and photograph.Although one might imagine a preprocess that would help in this particular case, there will probably always be a class of generalizations that are obvious to an intelligent lexicographer, but lie hopelessly beyond the objectivity of a computer.Despite these problems, the association ratio could be an important tool to aid the lexicographer, rather like an index to the concordances.It can help us decide what to look for; it provides a quick summary of what company our words do keep.Church, K. 1988 ""A Stochastic Parts Program and Noun Phrase Parser  for Unrestricted Text,"" Second Conference on Applied Natural  Language Processing, Austin, TX.Church, K.; Gale, W.; Hanks, P.; and Hindle, D. 1989 ""Parsing, Word  Associations and Typical Predicate-Argument Relations,""  International Workshop on Parsing Technologies, CMU.Fano, R. 1961 Transmission of Information: A Statistical Theory of Communications.MIT Press, Cambridge, MA.Firth, J. 1957 ""A Synopsis of Linguistic Theory 1930-1955,"" in Studies in Linguistic Analysis, Philological Society, Oxford; reprinted in Palmer,  F.(ed.)1968 Selected Papers of J.R. Firth, Longman, Harlow.Francis, W. and Ku~era, H. 1982 Frequency Analysis of English Usage.Houghton Mifflin Company, Boston, MA.Good, I. J. 1953 The Population Frequencies of Species and the  Estimation of Population Parameters.Biometrika, Vol. 40, 237-264.Hanks, P. 1987 ""Definitions and Explanations,"" in J.Sinclair (ed.),  Looking Up: An Account of the COBUILD Project in Lexical ing.Collins, London and Glasgow.Hindle, D. 1983a ""Deterministic Parsing of Syntactic Non-fluencies.""In  Proceedings of the 23rd Annual Meeting of the Association for tational Linguistics.Hindle, D. 1983b ""User Manual for Fidditch, a Deterministic Parser.""Naval Research Laboratory Technical Memorandum #7590-142.Hornby, A. 1948 The Advanced Learner's Dictionary, Oxford University  Press, Oxford, U.K. Jelinek, F. 1982. (personal communication) Kahan, S.; Pavlidis, T.; and Baird, H. 1987 ""On the Recognition of  Printed Characters of any Font or Size,"" IEEE Transactions PAMI, 274-287.Meyer, D.; Schvaneveldt, R.; and Ruddy, M. 1975 ""Loci of Contextual Effects on Visual Word-Recognition,"" in P.Rabbitt and S. Dornic (eds.), Attention and Performance V, Academic Press, New York.Palermo, D. and Jenkins, J. 1964 ""Word AssociationNorms.""University of Minnesota Press, Minneapolis, MN.Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; and Stock, P. (eds.)1987a Collins Cobuild English Language Dictionary.Collins, London and Glasgow.Sinclair, J. 1987b ""The Nature of the Evidence,"" in J.Sinclair (ed.),  Looking Up: An Account of the COBUILD Project in Lexical  Computing.Collins, London and Glasgow.Smadja, F. In press.""Microcoding the Lexicon with Co-Occurrence Knowledge,"" in Zernik (ed.), Lexical Acquisition: Using On-Line sources to Build a Lexicon, MIT Press, Cambridge, MA.This statistic has also been used by the IBM speech group (Jelinek 1982) for constructing language models for applications in speech recognition.Smadja (in press) discusses the separation between collocates in a very similar way.This definition fw(x,y) uses a rectangular window.It might be interesting to consider alternatives (e.g. a triangular window or a decaying exponential) that would weight words less and less as they are separated by more and more words.Other windows are also possible.For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest.Although the Good-Turing Method (Good 1953) is more than 35 years old, it is still heavily cited.For example, Katz (1987) uses the method in order to estimate trigram probabilities in the IBM speech recognizer.The Good-Turing Method is helpful for trigrams that have not been seen very often in the training corpus.The last unclassified line ....save shoppers anywhere from $50...raises interesting problems.Syntactic ""chunking"" shows that, in spite of its co-occurrence of from with save, this line does not belong here.An intriguing exercise, given the lookup table we are trying to construct, is how to guard against false inferences such as that since shoppers is tagged [PERSON], $50 to $500 must here count as either BAD or a LOCATION.Accidental coincidences of this kind do not have a significant effect on the measure, however, although they do serve as a reminder of the probabilistic nature of the findings.The word time itself also occurs significantly in the table, but on closer examination it is clear that this use of time (e.g. to save time) counts as something like a commodity or resource, not as part of a time adjunct.Such are the pitfalls of lexicography (obvious when they are pointed out)."
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Similarity-Based Estimation of Word Cooccurrence  600 Mountain Ave.  Murray Hill, NJ 07974, USA  dagan research, att. com  pereira research, att. com  In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ""eat a peach"" and ""eat a beach"" is more likely. tical NLP methods determine the likelihood of a word combination according to its frequency in a training pus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for timating the probability of such previously unseen word combinations using available information on ""most ilar"" words.  We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word grams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity im-provement in the prediction of unseen bigrams and tistically significant reductions in speech-recognition ror.  ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","Similarity-Based Estimation of Word Cooccurrence Murray Hill, NJ 07974, USA  dagan research, att.com  pereira research, att.com  In many applications of natural language processing it is necessary to determine the likelihood of a given word combination.For example, a speech recognizer may need to determine which of the two word combinations ""eat a peach"" and ""eat a beach"" is more likely.tical NLP methods determine the likelihood of a word combination according to its frequency in a training pus.However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus.In this work we propose a method for timating the probability of such previously unseen word combinations using available information on ""most ilar"" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word grams in a variant of Katz's back-off model.The similarity-based method yields a 20% perplexity im-provement in the prediction of unseen bigrams and tistically significant reductions in speech-recognition ror."
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Recognizing Expressions of Commonsense Psychology in English Text Andrew Gordon, Abe Kazemzadeh, Anish Nair and Milena Petrova  University of Southern California  Within the field of computational linguistics,  the study of commonsense psychology has not  received special attention, and is generally viewed as  Many applications of natural language  just one of the many conceptual areas that must be  processing technologies involve analyzing  addressed in building large-scale lexical-semantic  texts that concern the psychological states  resources for language processing. Although there  and processes of people, including their  have been a number of projects that have included  beliefs, goals, predictions, explanations,  concepts of commonsense psychology as part of a  and plans. In this paper, we describe our  larger lexical-semantic resource, e.g. the Berkeley  efforts to create a robust, large-scale  lexical-semantic resource for the recognition  attempted to achieve a high degree of breadth or  and classification of expressions of  comdepth over the sorts of expressions that people use  monsense psychology in English Text.  to refer to mental states and processes.  We achieve high levels of precision and  The lack of a large-scale resource for the  analyrecall by hand-authoring sets of local  sis of language for commonsense psychological  grammars for commonsense psychology  concepts is seen as a barrier to the development of  concepts, and show that this approach can  a range of potential computer applications that  inachieve classification performance greater  volve text analysis, including the following:  than that obtained by using machine  Natural language interfaces to mixed-initiative  learning techniques. We demonstrate the  utility of this resource for large-scale  corTraum, 1993) require the ability to map  expus analysis by identifying references to  pressions of users' beliefs, goals, and plans  adversarial and competitive goals in  po(among other commonsense psychology  conlitical speeches throughout U.S. history.  cepts) onto formalizations that can be  manipulated by automated planning algorithms.  Automated question answering systems  1 Commonsense Psychology in Language  (Voorhees & Buckland, 2002) require the  abilAcross all text genres it is common to find words  ity to tag and index text corpora with the  releand phrases that refer to the mental states of people  vant commonsense psychology concepts in  (their beliefs, goals, plans, emotions, etc.) and their  order to handle questions concerning the  bemental processes (remembering, imagining,  prioriliefs, expectations, and intentions of people.  tizing, problem solving). These mental states and  Research efforts within the field of psychology  processes are among the broad range of concepts  that employ automated corpus analysis  techthat people reason about every day as part of their  commonsense understanding of human  psycholtal illness impacts on language production, e.g.  ogy. Commonsense psychology has been studied  Reboul & Sabatier's (2001) study of the  disin many fields, sometimes using the terms Folk  course of schizophrenic patients, require the  psychology or Theory of Mind, as both a set of  beability to identify all references to certain  psyliefs that people have about the mind and as a set  chological concepts in order to draw statistical  of everyday reasoning abilities.  In order to enable future applications, we  unmain of children's language acquisition, but rather  dertook a new effort to meet this need for a  linguistic resource. This paper describes our efforts in  We conducted a study to determine how  politibuilding a large-scale lexical-semantic resource for  cal speeches have been tailored over the course of  automated processing of natural language text  U.S. history throughout changing climates of  miliabout mental states and processes. Our aim was to  tary action. Specifically, we wondered if  politibuild a system that would analyze natural language  cians were more likely to talk about goals having  text and recognize, with high precision and recall,  to do with conflict, competition, and aggression  every expression therein related to commonsense  during wartime than in peacetime. In order to  psychology, even in the face of an extremely broad  automatically recognize references to goals of this  range of surface forms. Each recognized  expressort in text, we used a set of local grammars  sion would be tagged with an appropriate concept  authored using the methodology described in  Secfrom a broad set of those that participate in our  tion 3 of this paper. The corpus we selected to  apcommonsense psychological theories.  ply these concept recognizers was the U.S. State of  Section 2 demonstrates the utility of a  lexicalthe Union Addresses from 1790 to 2003. The  reasemantic resource of commonsense psychology in  sons for choosing this particular text corpus were  automated corpus analysis through a study of the  its uniform distribution over time and its easy  changes in mental state expressions over the course  availability in electronic form from Project  Gutenof over 200 years of U.S. Presidential  State-of-theberg (www.gutenberg. net). Our set of local  gramUnion Addresses. Section 3 of this paper describes  mars identified 4290 references to these goals in  the methodology that we followed to create this  this text corpus, the vast majority of them begin  resource, which involved the hand authoring of  references to goals of an adversarial nature (rather  than competitive). Examples of the references that  scribes a set of evaluations to determine the  perwere identified include the following:  formance levels that these local grammars could  They sought to use the rights and privileges  achieve and to compare these levels to those of  they had obtained in the United Nations, to  machine learning approaches. Section 5 concludes  frustrate its purposes [ adversarial-goal] and  this paper with a discussion of the relative merits  cut down its powers as an effective agent of  of this approach to the creation of lexical-semantic  world progress. ( Truman, 1953)  resources as compared to other approaches.  The nearer we come to vanquishing [  adversarial-goal] our enemies the more we  inevita2 Applications to corpus analysis  bly become conscious of differences among  the victors. ( Roosevelt, 1945)  One of the primary applications of a  lexicalMen have vied [ competitive-goal] with each  semantic resource for commonsense psychology is  other to do their part and do it well. ( Wilson,  toward the automated analysis of large text  corpora. The research value of identifying  commonI will submit to Congress comprehensive  legsense psychology expressions has been  islation to strengthen our hand in combating  demonstrated in work on children's language use,  [ adversarial-goal] terrorists. ( Clinton, 1995)  where researchers have manually annotated large  text corpora consisting of parent/child discourse  Figure 1 summarizes the results of applying our  transcripts (Barsch & Wellman, 1995) and  chillocal grammars for adversarial and competitive  dren's storybooks (Dyer et al., 2000). While these  goals to the U.S. State of the Union Addresses. For  previous studies have yielded interesting results,  each year, the value that is plotted represents the  they required enormous amounts of human effort  number of references to these concepts that were  to manually annotate texts. In this section we aim  identified per 100 words in the address. The  interto show how a lexical-semantic resource for  comesting result of this analysis is that references to  monsense psychology can be used to automate this  adversarial and competitive goals in this corpus  annotation task, with an example not from the  doincrease in frequency in a pattern that directly  corresponds to the major military conflicts that the  U.S. has participated in throughout its history.  Features per 100 words  Figure 1. Adversarial and competitive goals in the U.S. State of the Union Addresses from 1790-2003  Each numbered peak in Figure 1 corresponds to  els of each of these conceptual areas are being  a period in which the U.S. was involved in a  miliauthored to support automated inference about  tary conflict. These are: 1) 1813, War of 1812, US  commonsense psychology (Gordon & Hobbs,  and Britain; 2) 1847, Mexican American War; 3)  2003). We adopted this conceptual framework in  1864, Civil War; 4) 1898, Spanish American War;  our current project because of the broad scope of  5) 1917, World War I; 6) 1943, World War II; 7)  the concepts in this ontology and its potential for  future integration into computational reasoning  1991, Gulf War; 10) 2002, War on Terrorism.  The wide applicability of a lexical-semantic  reThe full list of the 30 concept areas identified is  source for commonsense psychology will require  as follows: 1) Managing knowledge, 2) Similarity  that the identified concepts are well defined and  comparison, 3) Memory retrieval, 4) Emotions, 5)  are of broad enough scope to be relevant to a wide  Explanations, 6) World envisionment, 7)  Execurange of tasks. Additionally, such a resource must  tion envisionment, 8) Causes of failure, 9)  Manachieve high levels of accuracy in identifying these  aging expectations, 10) Other agent reasoning, 11)  concepts in natural language text. The remainder of  Threat detection, 12) Goals, 13) Goal themes, 14)  this paper describes our efforts in authoring and  evaluating such a resource.  17) Planning modalities, 18) Planning goals, 19)  Plan construction, 20) Plan adaptation, 21) Design,  3 Authoring recognition rules  22) Decisions, 23) Scheduling, 24) Monitoring, 25)  The first challenge in building any lexical-semantic  Repetitive execution, 28) Plan following, 29)  Obresource is to identify the concepts that are to be  servation of execution, and 30) Body interaction.  recognized in text and used as tags for indexing or  Our aim for this lexical-semantic resource was  markup. For expressions of commonsense  psyto develop a system that could automatically  idenchology, these concepts must describe the broad  tify every expression of commonsense psychology  scope of people's mental states and processes. An  in English text, and assign to them a tag  correontology of commonsense psychology with a high  sponding to one of the 635 concepts in this  ontoldegree of both breadth and depth is described by  ogy. For example, the following passage (from  Gordon (2002). In this work, 635 commonsense  William Makepeace Thackeray's 1848 novel,  psychology concepts were identified through an  Vanity Fair) illustrates the format of the output of  analysis of the representational requirements of a  this system, where references to commonsense  corpus of 372 planning strategies collected from 10  psychology concepts are underlined and followed  real-world planning domains. These concepts were  by a tag indicating their specific concept type  degrouped into 30 conceptual areas, corresponding to  limited by square brackets:  various reasoning functions, and full formal  modPerhaps [ partially-justified-proposition] she  directly to one of these three concepts, which  had mentioned the fact [ proposition] already to  prompted us to elaborate the original sets of  conRebecca, but that young lady did not appear to  cepts to accommodate these and other distinctions  [ partially-justified-proposition] have  rememmade in language. In the case of the conceptual  bered it [ memory-retrieval]; indeed, vowed and  area of memory retrieval, a total of twelve unique  protested that she expected [ add-expectation] to  concepts were necessary to achieve coverage over  see a number of Amelia's nephews and nieces.  the distinctions evident in English.  She was quite disappointed [  disappointmentThese local grammars were authored one  conemotion] that Mr. Sedley was not married; she  ceptual area at a time. At the time of the writing of  was sure [ justified-proposition] Amelia had said  this paper, our group had completed 6 of the  origihe was, and she doted so on [ liking-emotion]  litnal 30 commonsense psychology conceptual areas.  tle children.  The remainder of this paper focuses on the first 4  The approach that we took was to author (by  of the 6 areas that were completed, which were  hand) a set of local grammars that could be used to  evaluated to determine the recall and precision  peridentify each concept. For this task we utilized the  formance of our hand-authored rules. These four  Intex Corpus Processor software developed by the  areas are Managing knowledge, Memory,  Explanations, and Similarity judgments. Figure 2  preguistique (LADL) of the University of Paris 7  (Silsents each of these four areas with a single  berztein, 1999). This software allowed us to author  fabricated example of an English expression for  a set of local grammars using a graphical user  ineach of the final set of concepts. Local grammars  terface, producing lexical/syntactic structures that  for the two additional conceptual areas, Goals (20  can be compiled into finite-state transducers. To  concepts) and Goal management (17 concepts),  simplify the authoring of these local grammars,  were authored using the same approach as the  othIntex includes a large-coverage English dictionary  ers, but were not completed in time to be included  compiled by Blandine Courtois, allowing us to  in our performance evaluation.  specify them at a level that generalized over noun  After authoring these local grammars using the  and verb forms. For example, there are a variety of  Intex Corpus Processor, finite-state transducers  ways of expressing in English the concept of  reafwere compiled for each commonsense psychology  firming a belief that is already held, as exemplified  concept in each of the different conceptual areas.  in the following sentences:  To simplify the application of these transducers to  text corpora and to aid in their evaluation,  trans1) The finding was confirmed by the new  ducers for individual concepts were combined into  data. 2) She told the truth, corroborating his  a single finite state machine (one for each  concepstory. 3) He reaffirms his love for her. 4) We  tual area). By examining the number of states and  need to verify the claim. 5) Make sure it is true.  transitions in the compiled finite state graphs, some  Although the verbs in these sentences differ in  indication of their relative size can be given for the  tense, the dictionaries in Intex allowed us to  recogfour conceptual areas that we evaluated: Managing  nize each using the following simple description:  knowledge (348 states / 932 transitions), Memory  (<confirm> by | <corroborate> | <reaffirm> |  (203 / 725), Explanations (208 / 530), and  Similarity judgments (121 / 500).  While constructing local grammars for each of  the concepts in the original ontology of  common4 Performance evaluation  sense psychology, we identified several conceptual  distinctions that were made in language that were  In order to evaluate the utility of our set of  handnot expressed in the specific concepts that Gordon  authored local grammars, we conducted a study of  had identified. For example, the original ontology  their precision and recall performance. In order to  included only three concepts in the conceptual area  calculate the performance levels, it was first  necesof memory retrieval (the sparsest of the 30 areas),  sary to create a test corpus that contained  refernamely memory, memory cue, and memory  reences to the sorts of commonsense psychological  trieval. English expressions such as to forget and  concepts that our rules were designed to recognize.  repressed memory could not be easily mapped  To accomplish this, we administered a survey to  1. Managing knowledge (37 concepts)  He's got a logical mind ( managing-knowledge-ability). She's very gullible ( bias-toward-belief). He's skepti-cal by nature ( bias-toward-disbelief). It is the truth ( true). That is completely false ( false). We need to know whether it is true or false ( truth-value). His claim was bizarre ( proposition). I believe what you are saying ( belief). I didn't know about that (unknown). I used to think like you do ( revealed-incorrect-belief). The assumption was widespread ( assumption). There is no reason to think that ( unjustified-proposition). There is some evidence you are right ( partially-justified-proposition). The fact is well established ( justified-proposition). As a rule, students are generally bright ( inference). The conclusion could not be otherwise ( consequence). What was the reason for your suspicion ( justification)? That isn't a good reason ( poor-justification). Your argument is circular ( circular-justification). One of these things must be false ( contradiction). His wisdom is vast ( knowledge). He knew all about history ( knowledge-domain). I know something about plumbing ( partial-knowledge-domain).  He's got a lot of real-world experience ( world-knowledge). He understands the theory behind it ( world-model-knowledge). That is just common sense ( shared-knowledge). I'm willing to believe that ( add-belief). I stopped believing it after a while ( remove-belief). I assumed you were coming ( add-assumption). You can't make that assumption here ( remove-assumption). Let's see what follows from that ( check-inferences). Disregard the consequences of the assumption ( ignore-inference). I tried not to think about it ( suppress-inferences). I concluded that one of them must be wrong ( realize-contradiction). I realized he must have been there ( realize). I can't think straight ( knowledge-management-failure). It just confirms what I knew all along ( reaffirm-belief).  2. Memory (12 concepts)  He has a good memory ( memory-ability). It was one of his fondest memories ( memory-item). He blocked out the memory of the tempestuous relationship ( repressed-memory-item). He memorized the words of the song ( memory-storage). She remembered the last time it rained ( memory-retrieval). I forgot my locker combination ( memory-retrieval-failure). He repressed the memories of his abusive father ( memory-repression). The widow was reminded of her late husband ( reminding). He kept the ticket stub as a memento ( memory-cue). He intended to call his brother on his birthday ( schedule-plan). He remembered to set the alarm before he fell asleep ( scheduled-plan-retrieval). I forgot to take out the trash ( scheduled-plan-retrieval-failure).  He's good at coming up with explanations ( explanation-ability). The cause was clear ( cause). Nobody knew how it had happened ( mystery). There were still some holes in his account ( explanation-criteria). It gave us the explanation we were looking for ( explanation). It was a plausible explanation ( candidate-explanation). It was the best explanation I could think of ( best-candidate-explanation). There were many contributing factors ( factor). I came up with an explanation ( explain). Let's figure out why it was so ( attempt-to-explain). He came up with a reasonable explanation ( generate-candidate-explanation). We need to consider all of the possible explanations ( assess-candidate-explanations). That is the explanation he went with ( adopt-explanation). We failed to come up with an explanation ( explanation-failure). I can't think of anything that could have caused it ( explanation-generation-failure). None of these explanations account for the facts ( explanation-satisfaction-failure).  Your account must be wrong ( unsatisfying-explanation). I prefer non-religious explanations (explanation-preference). You should always look for scientific explanations ( add-explanation-preference). We're not going to look at all possible explanations ( remove-explanation-preference).  4. Similarity judgments (13 concepts)  She's good at picking out things that are different ( similarity-comparison-ability). Look at the similarities between the two ( make-comparison). He saw that they were the same at an abstract level ( draw-analogy). She could see the pattern unfolding ( find-pattern). It depends on what basis you use for comparison ( comparison-metric). They have that in common ( same-characteristic). They differ in that regard ( different-characteristic). If a tree were a person, its leaves would correspond to fingers ( analogical-mapping). The pattern in the rug was intricate ( pattern). They are very much alike ( similar). It is completely different ( dissimilar). It was an analogous example ( analogous).  Figure 2. Example sentences referring to 92 concepts in 4 areas of commonsense psychology  collect novel sentences that could be used for this  area. The results show that the precision of our  system is very high, with marginal recall  performThis survey was administered over the course  ance.  of one day to anonymous adult volunteers who  The low recall scores raised a concern over the  stopped by a table that we had set up on our  uniquality of our test data. In reviewing the sentences  versity's campus. We instructed the survey taker to  that were collected, it was apparent that some  surauthor 3 sentences that included words or phrases  vey participants were not able to complete the task  related to a given concept, and 3 sentences that  as we had specified. To improve the validity of the  they felt did not contain any such references. Each  test data, we enlisted six volunteers (native English  survey taker was asked to generate these 6  senspeakers not members of our development team) to  tences for each of the 4 concept areas that we were  judge whether or not each sentence in the corpus  evaluating, described on the survey in the  followwas produced according to the instructions. The  corpus of sentences was divided evenly among  these six raters, and each sentence that the rater  Managing knowledge: Anything about the  judged as not satisfying the instructions was  filknowledge, assumptions, or beliefs that people  tered from the data set. In addition, each rater also  have in their mind  judged half of the sentences given to a different  Memory: When people remember things,  forrater in order to compute the degree of inter-rater  get things, or are reminded of things  Explanations: When people come up with  possentences from the corpus, a second  precisible explanations for unknown causes  sion/recall evaluation was performed. Table 2  pre Similarity judgments: When people find  simisents the results of our hand-authored local  larities or differences in things  grammars on the filtered data set, and lists the  inter-rater agreement for each conceptual area among  A total of 99 people volunteered to take our  our six raters. The results show that the system  survey, resulting in a corpus of 297 positive and  achieves a high level of precision, and the recall  297 negative sentences for each conceptual area,  performance is much better than earlier indicated.  with a few exceptions due to incomplete surveys.  The performance of our hand-authored local  Using this survey data, we calculated the  precigrammars was then compared to the performance  sion and recall performance of our hand-authored  that could be obtained using more traditional  malocal grammars. Every sentence that had at least  chine-learning approaches. In these comparisons,  one concept detected for the corresponding concept  the recognition of commonsense psychology  conarea was treated as a hit . Table 1 presents the  cepts was treated as a classification problem,  precision and recall performance for each concept  where the task was to distinguish between positive  Correct Hits  Wrong hits  Precision  Memory  Similarity judgments  Table 1. Precision and recall results on the unfiltered data set  Inter-rater  Correct  Precision  Hits (a)  hits (b)  Managing knowledge  Memory  Table 2. Precision and recall results on the filtered data set, with inter-rater agreement on filtering  A. Hand authored local  B. SVM with word level  C. SVM with word and  features  Memory  Similarity judgments  Table 3. Percent agreement ( Pa) and Kappa statistics ( K) for classification using hand-authored local grammars (A), SVMs with word features (B), and SVMs with word and concept features (C) and negative sentences for any given concept area.  Sentences in the filtered data sets were used as  training instances, and feature vectors for each  sentence were composed of word-level unigram  The most significant challenge facing developers  and bi-gram features, using no stop-lists and by  of large-scale lexical-semantic resources is coming  ignoring punctuation and case. By using a toolkit  to some agreement on the way that natural  lanof machine learning algorithms (Witten & Frank,  guage can be mapped onto specific concepts. This  1999), we were able to compare the performance  challenge is particularly evident in consideration of  of a wide range of different techniques, including  our survey data and subsequent filtering. The  Na ve Bayes, C4.5 rule induction, and Support  abilities that people have in producing and  recogVector Machines, through stratified  crossnizing sentences containing related words or  validation (10-fold) of the training data. The  highphrases differed significantly across concept areas.  est performance levels were achieved using a  seWhile raters could agree on what constitutes a  sentence containing an expression about memory  training a support vector classifier using  polyno(Kappa=.8069), the agreement on expressions of  mial kernels (Platt, 1998). These performance  remanaging knowledge is much lower than we  sults are presented in Table 3. The percentage  would hope for (Kappa=.5636). We would expect  correctness of classification ( Pa) of our  handmuch greater inter-rater agreement if we had  authored local grammars (column A) was higher  trained our six raters for the filtering task, that is,  than could be attained using this machine-learning  described exactly which concepts we were looking  approach (column B) in three out of the four  confor and gave them examples of how these concepts  can be realized in English text. However, this  apWe then conducted an additional study to  deproach would have invalidated our performance  termine if the two approaches (hand-authored local  results on the filtered data set, as the task of the  grammars and machine learning) could be  comraters would be biased toward identifying  examplimentary. The concepts that are recognized by  ples that our system would likely perform well on  our hand-authored rules could be conceived as  adrather than identifying references to concepts of  ditional bimodal features for use in machine  commonsense psychology.  learning algorithms. We constructed an additional  Our inter-rater agreement concern is indicative  set of support vector machine classifiers trained on  of a larger problem in the construction of  largethe filtered data set that included these additional  scale lexical-semantic resources. The deeper we  concept-level features in the feature vector of each  delve into the meaning of natural language, the less  instance along side the existing unigram and  biwe are likely to find strong agreement among  ungram features. Performance of these enhanced  trained people concerning the particular concepts  classifiers, also obtained through stratified  crossthat are expressed in any given text. Even with  validation (10-fold), are also reported in Table 3 as  well (column C). The results show that these  enknowledge (e.g. commonsense psychology), finer  hanced classifiers perform at a level that is the  distinctions in meaning will require the efforts of  greater of that of each independent approach.  trained knowledge engineers to successfully map  between language and concepts. While this will  sion/recall performance evaluations, the concern is  even more serious for other methodologies that  rely on large amounts of hand-tagged text data to  Baker, C., Fillmore, C., & Lowe, J. (1998) The  Berkeley FrameNet project. in Proceedings of the  create the recognition rules in the first place. We  expect that this problem will become more evident  as projects using algorithms to induce local  gramBartsch, K. & Wellman, H. (1995) Children talk about  mars from manually-tagged corpora, such as the  the mind. New York: Oxford University Press.  Dyer, J., Shatz, M., & Wellman, H. (2000) Young  chilbroaden and deepen their encodings in conceptual  dren's storybooks as a source of mental state  inforareas that are more abstract (e.g. commonsense  mation. Cognitive Development 15:17-37.  Ferguson, G. & Allen, J. (1993) Cooperative Plan  ReaThe approach that we have taken in our  resoning for Dialogue Systems, in AAAI-93 Fall  Symsearch does not offer a solution to the growing  posium on Human-Computer Collaboration:  problem of evaluating lexical-semantic resources.  Reconciling Theory, Synthesizing Practice, AAAI  However, by hand-authoring local grammars for  Technical Report FS-93-05. Menlo Park, CA: AAAI  specific concepts rather than inducing them from  Press.  tagged text, we have demonstrated a successful  Gordon, A. (2002) The Theory of Mind in Strategy  methodology for creating lexical-semantic  reRepresentations. 24th Annual Meeting of the  Cognisources with a high degree of conceptual breadth  tive Science Society. Mahwah, NJ: Lawrence  Erland depth. By employing linguistic and knowledge  baum Associates.  engineering skills in a combined manner we have  Gordon, A. & Hobbs (2003) Coverage and competency  been able to make strong ontological commitments  in formal theories: A commonsense theory of  memabout the meaning of an important portion of the  ory. AAAI Spring Symposium on Formal Theories of  English language. We have demonstrated that the  Commonsense knowledge, March 24-26, Stanford.  precision and recall performance of this approach  Platt, J. (1998). Fast Training of Support Vector  Mais high, achieving classification performance  chines using Sequential Minimal Optimization. In B.  greater than that of standard machine-learning  Sch lkopf, C. Burges, and A. Smola (eds.) Ad vances  techniques. Furthermore, we have shown that  in Kernel Methods Support Vector Learning,  Camhand-authored local grammars can be used to  bridge, MA: MIT Press.  identify concepts that can be easily combined with  Reboul A., Sabatier P., No l-Jorand M-C. (2001) Le  word-level features (e.g. unigrams, bi-grams) for  essing systems. Our early exploration of the  application of this work for corpus analysis (U.S. State  of the Union Addresses) has produced interesting  Computers and the Humanities 33(3).  results, and we expect that the continued  development of this resource will be important to the  sucTraum, D. (1993) Mental state in the TRAINS-92  diacess of future corpus analysis and human-computer  logue manager. In Working Notes of the AAAI Spring  interaction projects.  Symposium on Reasoning about Mental States:  Formal Theories and Applications, pages 143-149, 1993.  Acknowledgments  Menlo Park, CA: AAAI Press.  Voorhees, E. & Buckland, L. (2002) The Eleventh Text  This paper was developed in part with funds from  REtrieval Conference (TREC 2002). Washington,  the U.S. Army Research Institute for the  BehavDC: Department of Commerce, National Institute of  ioral and Social Sciences under ARO contract  Standards and Technology.  number DAAD 19-99-D-0046. Any opinions,  Witten, I. & Frank, E. (1999) Data Mining: Practical  findings and conclusions or recommendations  exMachine Learning Tools and Techniques with Java  pressed in this paper are those of the authors and  do not necessarily reflect the views of the  Department of the Army. ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","Within the field of computational linguistics, the study of commonsense psychology has not received special attention, and is generally viewed as Many applications of natural language just one of the many conceptual areas that must be processing technologies involve analyzing addressed in building large-scale lexical-semantic texts that concern the psychological states resources for language processing. Although there  and processes of people, including their  have been a number of projects that have included  beliefs, goals, predictions, explanations,  concepts of commonsense psychology as part of a  and plans.In this paper, we describe our  larger lexical-semantic resource, e.g. the Berkeley  efforts to create a robust, large-scale  lexical-semantic resource for the recognition  attempted to achieve a high degree of breadth or  and classification of expressions of  comdepth over the sorts of expressions that people use  monsense psychology in English Text.to refer to mental states and processes.We achieve high levels of precision and  The lack of a large-scale resource for the  analyrecall by hand-authoring sets of local  sis of language for commonsense psychological  grammars for commonsense psychology  concepts is seen as a barrier to the development of  concepts, and show that this approach can  a range of potential computer applications that  inachieve classification performance greater  volve text analysis, including the following:  than that obtained by using machine  Natural language interfaces to mixed-initiative  learning techniques.We demonstrate the  utility of this resource for large-scale  corTraum, 1993) require the ability to map  expus analysis by identifying references to  pressions of users' beliefs, goals, and plans  adversarial and competitive goals in  po(among other commonsense psychology  conlitical speeches throughout U.S. history.cepts) onto formalizations that can be  manipulated by automated planning algorithms.Automated question answering systems  1 Commonsense Psychology in Language  (Voorhees & Buckland, 2002) require the  abilAcross all text genres it is common to find words  ity to tag and index text corpora with the  releand phrases that refer to the mental states of people  vant commonsense psychology concepts in  (their beliefs, goals, plans, emotions, etc.) and their  order to handle questions concerning the  bemental processes (remembering, imagining,  prioriliefs, expectations, and intentions of people.  tizing, problem solving).These mental states and  Research efforts within the field of psychology  processes are among the broad range of concepts  that employ automated corpus analysis  techthat people reason about every day as part of their  commonsense understanding of human  psycholtal illness impacts on language production, e.g.  ogy.Commonsense psychology has been studied  Reboul & Sabatier's (2001) study of the  disin many fields, sometimes using the terms Folk  course of schizophrenic patients, require the  psychology or Theory of Mind, as both a set of  beability to identify all references to certain  psyliefs that people have about the mind and as a set  chological concepts in order to draw statistical  of everyday reasoning abilities.In order to enable future applications, we  unmain of children's language acquisition, but rather  dertook a new effort to meet this need for a  linguistic resource.This paper describes our efforts in  We conducted a study to determine how  politibuilding a large-scale lexical-semantic resource for  cal speeches have been tailored over the course of  automated processing of natural language text  U.S. history throughout changing climates of  miliabout mental states and processes.Our aim was to  tary action.Specifically, we wondered if  politibuild a system that would analyze natural language  cians were more likely to talk about goals having  text and recognize, with high precision and recall,  to do with conflict, competition, and aggression  every expression therein related to commonsense  during wartime than in peacetime.In order to  psychology, even in the face of an extremely broad  automatically recognize references to goals of this  range of surface forms.Each recognized  expressort in text, we used a set of local grammars  sion would be tagged with an appropriate concept  authored using the methodology described in  Secfrom a broad set of those that participate in our  tion 3 of this paper.The corpus we selected to  apcommonsense psychological theories.ply these concept recognizers was the U.S. State of  Section 2 demonstrates the utility of a  lexicalthe Union Addresses from 1790 to 2003.The  reasemantic resource of commonsense psychology in  sons for choosing this particular text corpus were  automated corpus analysis through a study of the  its uniform distribution over time and its easy  changes in mental state expressions over the course  availability in electronic form from Project  Gutenof over 200 years of U.S. Presidential  State-of-theberg (www.gutenberg. net).Our set of local  gramUnion Addresses.Section 3 of this paper describes  mars identified 4290 references to these goals in  the methodology that we followed to create this  this text corpus, the vast majority of them begin  resource, which involved the hand authoring of  references to goals of an adversarial nature (rather  than competitive).Examples of the references that  scribes a set of evaluations to determine the  perwere identified include the following:  formance levels that these local grammars could  They sought to use the rights and privileges  achieve and to compare these levels to those of  they had obtained in the United Nations, to  machine learning approaches.Section 5 concludes  frustrate its purposes [ adversarial-goal] and  this paper with a discussion of the relative merits  cut down its powers as an effective agent of  of this approach to the creation of lexical-semantic  world progress.( Truman, 1953)  resources as compared to other approaches.The nearer we come to vanquishing [  adversarial-goal] our enemies the more we  inevita2 Applications to corpus analysis  bly become conscious of differences among  the victors.( Roosevelt, 1945)  One of the primary applications of a  lexicalMen have vied [ competitive-goal] with each  semantic resource for commonsense psychology is  other to do their part and do it well.( Wilson,  toward the automated analysis of large text  corpora.The research value of identifying  commonI will submit to Congress comprehensive  legsense psychology expressions has been  islation to strengthen our hand in combating  demonstrated in work on children's language use,  [ adversarial-goal] terrorists.( Clinton, 1995)  where researchers have manually annotated large  text corpora consisting of parent/child discourse  Figure 1 summarizes the results of applying our  transcripts (Barsch & Wellman, 1995) and  chillocal grammars for adversarial and competitive  dren's storybooks (Dyer et al., 2000).While these  goals to the U.S. State of the Union Addresses.For  previous studies have yielded interesting results,  each year, the value that is plotted represents the  they required enormous amounts of human effort  number of references to these concepts that were  to manually annotate texts.In this section we aim  identified per 100 words in the address.The  interto show how a lexical-semantic resource for  comesting result of this analysis is that references to  monsense psychology can be used to automate this  adversarial and competitive goals in this corpus  annotation task, with an example not from the  doincrease in frequency in a pattern that directly  corresponds to the major military conflicts that the  U.S. has participated in throughout its history.Features per 100 words  Figure 1.Adversarial and competitive goals in the U.S. State of the Union Addresses from 1790-2003  Each numbered peak in Figure 1 corresponds to  els of each of these conceptual areas are being  a period in which the U.S. was involved in a  miliauthored to support automated inference about  tary conflict.These are: 1) 1813, War of 1812, US  commonsense psychology (Gordon & Hobbs,  and Britain; 2) 1847, Mexican American War; 3)  2003).We adopted this conceptual framework in  1864, Civil War; 4) 1898, Spanish American War;  our current project because of the broad scope of  5) 1917, World War I; 6) 1943, World War II; 7)  the concepts in this ontology and its potential for  future integration into computational reasoning  1991, Gulf War; 10) 2002, War on Terrorism.The wide applicability of a lexical-semantic  reThe full list of the 30 concept areas identified is  source for commonsense psychology will require  as follows: 1) Managing knowledge, 2) Similarity  that the identified concepts are well defined and  comparison, 3) Memory retrieval, 4) Emotions, 5)  are of broad enough scope to be relevant to a wide  Explanations, 6) World envisionment, 7)  Execurange of tasks.Additionally, such a resource must  tion envisionment, 8) Causes of failure, 9)  Manachieve high levels of accuracy in identifying these  aging expectations, 10) Other agent reasoning, 11)  concepts in natural language text.The remainder of  Threat detection, 12) Goals, 13) Goal themes, 14)  this paper describes our efforts in authoring and  evaluating such a resource.17) Planning modalities, 18) Planning goals, 19)  Plan construction, 20) Plan adaptation, 21) Design,  3 Authoring recognition rules  22) Decisions, 23) Scheduling, 24) Monitoring, 25)  The first challenge in building any lexical-semantic  Repetitive execution, 28) Plan following, 29)  Obresource is to identify the concepts that are to be  servation of execution, and 30) Body interaction.recognized in text and used as tags for indexing or  Our aim for this lexical-semantic resource was  markup.For expressions of commonsense  psyto develop a system that could automatically  idenchology, these concepts must describe the broad  tify every expression of commonsense psychology  scope of people's mental states and processes.An  in English text, and assign to them a tag  correontology of commonsense psychology with a high  sponding to one of the 635 concepts in this  ontoldegree of both breadth and depth is described by  ogy.For example, the following passage (from  Gordon (2002).In this work, 635 commonsense  William Makepeace Thackeray's 1848 novel,  psychology concepts were identified through an  Vanity Fair) illustrates the format of the output of  analysis of the representational requirements of a  this system, where references to commonsense  corpus of 372 planning strategies collected from 10  psychology concepts are underlined and followed  real-world planning domains.These concepts were  by a tag indicating their specific concept type  degrouped into 30 conceptual areas, corresponding to  limited by square brackets:  various reasoning functions, and full formal  modPerhaps [ partially-justified-proposition] she  directly to one of these three concepts, which  had mentioned the fact [ proposition] already to  prompted us to elaborate the original sets of  conRebecca, but that young lady did not appear to  cepts to accommodate these and other distinctions  [ partially-justified-proposition] have  rememmade in language.In the case of the conceptual  bered it [ memory-retrieval]; indeed, vowed and  area of memory retrieval, a total of twelve unique  protested that she expected [ add-expectation] to  concepts were necessary to achieve coverage over  see a number of Amelia's nephews and nieces.the distinctions evident in English.She was quite disappointed [  disappointmentThese local grammars were authored one  conemotion] that Mr. Sedley was not married; she  ceptual area at a time.At the time of the writing of  was sure [ justified-proposition] Amelia had said  this paper, our group had completed 6 of the  origihe was, and she doted so on [ liking-emotion]  litnal 30 commonsense psychology conceptual areas.tle children.The remainder of this paper focuses on the first 4  The approach that we took was to author (by  of the 6 areas that were completed, which were  hand) a set of local grammars that could be used to  evaluated to determine the recall and precision  peridentify each concept.For this task we utilized the  formance of our hand-authored rules.These four  Intex Corpus Processor software developed by the  areas are Managing knowledge, Memory,  Explanations, and Similarity judgments.Figure 2  preguistique (LADL) of the University of Paris 7  (Silsents each of these four areas with a single  berztein, 1999).This software allowed us to author  fabricated example of an English expression for  a set of local grammars using a graphical user  ineach of the final set of concepts.Local grammars  terface, producing lexical/syntactic structures that  for the two additional conceptual areas, Goals (20  can be compiled into finite-state transducers.To  concepts) and Goal management (17 concepts),  simplify the authoring of these local grammars,  were authored using the same approach as the  othIntex includes a large-coverage English dictionary  ers, but were not completed in time to be included  compiled by Blandine Courtois, allowing us to  in our performance evaluation.specify them at a level that generalized over noun  After authoring these local grammars using the  and verb forms.For example, there are a variety of  Intex Corpus Processor, finite-state transducers  ways of expressing in English the concept of  reafwere compiled for each commonsense psychology  firming a belief that is already held, as exemplified  concept in each of the different conceptual areas.in the following sentences:  To simplify the application of these transducers to  text corpora and to aid in their evaluation,  trans1) The finding was confirmed by the new  ducers for individual concepts were combined into  data.2) She told the truth, corroborating his  a single finite state machine (one for each  concepstory. 3) He reaffirms his love for her.4) We  tual area).By examining the number of states and  need to verify the claim.5) Make sure it is true.transitions in the compiled finite state graphs, some  Although the verbs in these sentences differ in  indication of their relative size can be given for the  tense, the dictionaries in Intex allowed us to  recogfour conceptual areas that we evaluated: Managing  nize each using the following simple description:  knowledge (348 states / 932 transitions), Memory  (<confirm> by | <corroborate> | <reaffirm> |  (203 / 725), Explanations (208 / 530), and  Similarity judgments (121 / 500).While constructing local grammars for each of  the concepts in the original ontology of  common4 Performance evaluation  sense psychology, we identified several conceptual  distinctions that were made in language that were  In order to evaluate the utility of our set of  handnot expressed in the specific concepts that Gordon  authored local grammars, we conducted a study of  had identified.For example, the original ontology  their precision and recall performance.In order to  included only three concepts in the conceptual area  calculate the performance levels, it was first  necesof memory retrieval (the sparsest of the 30 areas),  sary to create a test corpus that contained  refernamely memory, memory cue, and memory  reences to the sorts of commonsense psychological  trieval.English expressions such as to forget and  concepts that our rules were designed to recognize.repressed memory could not be easily mapped  To accomplish this, we administered a survey to  1.Managing knowledge (37 concepts)  He's got a logical mind ( managing-knowledge-ability).She's very gullible ( bias-toward-belief).He's skepti-cal by nature ( bias-toward-disbelief).It is the truth ( true).That is completely false ( false).We need to know whether it is true or false ( truth-value).His claim was bizarre ( proposition).I believe what you are saying ( belief).I didn't know about that (unknown).I used to think like you do ( revealed-incorrect-belief).The assumption was widespread ( assumption).There is no reason to think that ( unjustified-proposition).There is some evidence you are right ( partially-justified-proposition).The fact is well established ( justified-proposition).As a rule, students are generally bright ( inference).The conclusion could not be otherwise ( consequence).What was the reason for your suspicion ( justification)?That isn't a good reason ( poor-justification).Your argument is circular ( circular-justification).One of these things must be false ( contradiction).His wisdom is vast ( knowledge).He knew all about history ( knowledge-domain).I know something about plumbing ( partial-knowledge-domain).He's got a lot of real-world experience ( world-knowledge).He understands the theory behind it ( world-model-knowledge).That is just common sense ( shared-knowledge).I'm willing to believe that ( add-belief).I stopped believing it after a while ( remove-belief).I assumed you were coming ( add-assumption).You can't make that assumption here ( remove-assumption).Let's see what follows from that ( check-inferences).Disregard the consequences of the assumption ( ignore-inference).I tried not to think about it ( suppress-inferences).I concluded that one of them must be wrong ( realize-contradiction).I realized he must have been there ( realize).I can't think straight ( knowledge-management-failure).It just confirms what I knew all along ( reaffirm-belief).2. Memory (12 concepts)  He has a good memory ( memory-ability).It was one of his fondest memories ( memory-item).He blocked out the memory of the tempestuous relationship ( repressed-memory-item).He memorized the words of the song ( memory-storage).She remembered the last time it rained ( memory-retrieval).I forgot my locker combination ( memory-retrieval-failure).He repressed the memories of his abusive father ( memory-repression).The widow was reminded of her late husband ( reminding).He kept the ticket stub as a memento ( memory-cue).He intended to call his brother on his birthday ( schedule-plan).He remembered to set the alarm before he fell asleep ( scheduled-plan-retrieval).I forgot to take out the trash ( scheduled-plan-retrieval-failure).He's good at coming up with explanations ( explanation-ability).The cause was clear ( cause).Nobody knew how it had happened ( mystery).There were still some holes in his account ( explanation-criteria).It gave us the explanation we were looking for ( explanation).It was a plausible explanation ( candidate-explanation).It was the best explanation I could think of ( best-candidate-explanation).There were many contributing factors ( factor).I came up with an explanation ( explain).Let's figure out why it was so ( attempt-to-explain).He came up with a reasonable explanation ( generate-candidate-explanation).We need to consider all of the possible explanations ( assess-candidate-explanations).That is the explanation he went with ( adopt-explanation).We failed to come up with an explanation ( explanation-failure).I can't think of anything that could have caused it ( explanation-generation-failure).None of these explanations account for the facts ( explanation-satisfaction-failure).Your account must be wrong ( unsatisfying-explanation).I prefer non-religious explanations (explanation-preference).You should always look for scientific explanations ( add-explanation-preference).We're not going to look at all possible explanations ( remove-explanation-preference).4. Similarity judgments (13 concepts)  She's good at picking out things that are different ( similarity-comparison-ability).Look at the similarities between the two ( make-comparison).He saw that they were the same at an abstract level ( draw-analogy).She could see the pattern unfolding ( find-pattern).It depends on what basis you use for comparison ( comparison-metric).They have that in common ( same-characteristic).They differ in that regard ( different-characteristic).If a tree were a person, its leaves would correspond to fingers ( analogical-mapping).The pattern in the rug was intricate ( pattern).They are very much alike ( similar).It is completely different ( dissimilar).It was an analogous example ( analogous).Figure 2.Example sentences referring to 92 concepts in 4 areas of commonsense psychology  collect novel sentences that could be used for this  area.The results show that the precision of our  system is very high, with marginal recall  performThis survey was administered over the course  ance.  of one day to anonymous adult volunteers who  The low recall scores raised a concern over the  stopped by a table that we had set up on our  uniquality of our test data.In reviewing the sentences  versity's campus.We instructed the survey taker to  that were collected, it was apparent that some  surauthor 3 sentences that included words or phrases  vey participants were not able to complete the task  related to a given concept, and 3 sentences that  as we had specified.To improve the validity of the  they felt did not contain any such references.Each  test data, we enlisted six volunteers (native English  survey taker was asked to generate these 6  senspeakers not members of our development team) to  tences for each of the 4 concept areas that we were  judge whether or not each sentence in the corpus  evaluating, described on the survey in the  followwas produced according to the instructions.The  corpus of sentences was divided evenly among  these six raters, and each sentence that the rater  Managing knowledge: Anything about the  judged as not satisfying the instructions was  filknowledge, assumptions, or beliefs that people  tered from the data set.In addition, each rater also  have in their mind  judged half of the sentences given to a different  Memory: When people remember things,  forrater in order to compute the degree of inter-rater  get things, or are reminded of things  Explanations: When people come up with  possentences from the corpus, a second  precisible explanations for unknown causes  sion/recall evaluation was performed.Table 2  pre Similarity judgments: When people find  simisents the results of our hand-authored local  larities or differences in things  grammars on the filtered data set, and lists the  inter-rater agreement for each conceptual area among  A total of 99 people volunteered to take our  our six raters.The results show that the system  survey, resulting in a corpus of 297 positive and  achieves a high level of precision, and the recall  297 negative sentences for each conceptual area,  performance is much better than earlier indicated.  with a few exceptions due to incomplete surveys.The performance of our hand-authored local  Using this survey data, we calculated the  precigrammars was then compared to the performance  sion and recall performance of our hand-authored  that could be obtained using more traditional  malocal grammars.Every sentence that had at least  chine-learning approaches.In these comparisons,  one concept detected for the corresponding concept  the recognition of commonsense psychology  conarea was treated as a hit . Table 1 presents the  cepts was treated as a classification problem,  precision and recall performance for each concept  where the task was to distinguish between positive  Correct Hits  Wrong hits  Precision  Memory  Similarity judgments  Table 1.Precision and recall results on the unfiltered data set  Inter-rater  Correct  Precision  Hits (a)  hits (b)  Managing knowledge  Memory  Table 2.Precision and recall results on the filtered data set, with inter-rater agreement on filtering  A.Hand authored local  B.SVM with word level  C.SVM with word and  features  Memory  Similarity judgments  Table 3.Percent agreement ( Pa) and Kappa statistics ( K) for classification using hand-authored local grammars (A), SVMs with word features (B), and SVMs with word and concept features (C) and negative sentences for any given concept area.Sentences in the filtered data sets were used as  training instances, and feature vectors for each  sentence were composed of word-level unigram  The most significant challenge facing developers  and bi-gram features, using no stop-lists and by  of large-scale lexical-semantic resources is coming  ignoring punctuation and case.By using a toolkit  to some agreement on the way that natural  lanof machine learning algorithms (Witten & Frank,  guage can be mapped onto specific concepts.This  1999), we were able to compare the performance  challenge is particularly evident in consideration of  of a wide range of different techniques, including  our survey data and subsequent filtering.The  Na ve Bayes, C4.5 rule induction, and Support  abilities that people have in producing and  recogVector Machines, through stratified  crossnizing sentences containing related words or  validation (10-fold) of the training data.The  highphrases differed significantly across concept areas.est performance levels were achieved using a  seWhile raters could agree on what constitutes a  sentence containing an expression about memory  training a support vector classifier using  polyno(Kappa=.8069), the agreement on expressions of  mial kernels (Platt, 1998).These performance  remanaging knowledge is much lower than we  sults are presented in Table 3.The percentage  would hope for (Kappa=.5636).We would expect  correctness of classification ( Pa) of our  handmuch greater inter-rater agreement if we had  authored local grammars (column A) was higher  trained our six raters for the filtering task, that is,  than could be attained using this machine-learning  described exactly which concepts we were looking  approach (column B) in three out of the four  confor and gave them examples of how these concepts  can be realized in English text.However, this  apWe then conducted an additional study to  deproach would have invalidated our performance  termine if the two approaches (hand-authored local  results on the filtered data set, as the task of the  grammars and machine learning) could be  comraters would be biased toward identifying  examplimentary.The concepts that are recognized by  ples that our system would likely perform well on  our hand-authored rules could be conceived as  adrather than identifying references to concepts of  ditional bimodal features for use in machine  commonsense psychology.learning algorithms.We constructed an additional  Our inter-rater agreement concern is indicative  set of support vector machine classifiers trained on  of a larger problem in the construction of  largethe filtered data set that included these additional  scale lexical-semantic resources.The deeper we  concept-level features in the feature vector of each  delve into the meaning of natural language, the less  instance along side the existing unigram and  biwe are likely to find strong agreement among  ungram features.Performance of these enhanced  trained people concerning the particular concepts  classifiers, also obtained through stratified  crossthat are expressed in any given text.Even with  validation (10-fold), are also reported in Table 3 as  well (column C).The results show that these  enknowledge (e.g. commonsense psychology), finer  hanced classifiers perform at a level that is the  distinctions in meaning will require the efforts of  greater of that of each independent approach.trained knowledge engineers to successfully map  between language and concepts.While this will  sion/recall performance evaluations, the concern is  even more serious for other methodologies that  rely on large amounts of hand-tagged text data to  Baker, C., Fillmore, C., & Lowe, J. (1998) The  Berkeley FrameNet project.in Proceedings of the  create the recognition rules in the first place.We  expect that this problem will become more evident  as projects using algorithms to induce local  gramBartsch, K. & Wellman, H. (1995) Children talk about  mars from manually-tagged corpora, such as the  the mind.New York: Oxford University Press.Dyer, J., Shatz, M., & Wellman, H. (2000) Young  chilbroaden and deepen their encodings in conceptual  dren's storybooks as a source of mental state  inforareas that are more abstract (e.g. commonsense  mation.Cognitive Development 15:17-37.Ferguson, G. & Allen, J. (1993) Cooperative Plan  ReaThe approach that we have taken in our  resoning for Dialogue Systems, in AAAI-93 Fall  Symsearch does not offer a solution to the growing  posium on Human-Computer Collaboration:  problem of evaluating lexical-semantic resources.Reconciling Theory, Synthesizing Practice, AAAI  However, by hand-authoring local grammars for  Technical Report FS-93-05.Menlo Park, CA: AAAI  specific concepts rather than inducing them from  Press.tagged text, we have demonstrated a successful  Gordon, A. (2002) The Theory of Mind in Strategy  methodology for creating lexical-semantic  reRepresentations.24th Annual Meeting of the  Cognisources with a high degree of conceptual breadth  tive Science Society.Mahwah, NJ: Lawrence  Erland depth.By employing linguistic and knowledge  baum Associates.engineering skills in a combined manner we have  Gordon, A. & Hobbs (2003) Coverage and competency  been able to make strong ontological commitments  in formal theories: A commonsense theory of  memabout the meaning of an important portion of the  ory.AAAI Spring Symposium on Formal Theories of  English language.We have demonstrated that the  Commonsense knowledge, March 24-26, Stanford.precision and recall performance of this approach  Platt, J. (1998).Fast Training of Support Vector  Mais high, achieving classification performance  chines using Sequential Minimal Optimization.In B.  greater than that of standard machine-learning  Sch lkopf, C. Burges, and A. Smola (eds.)Ad vances  techniques.Furthermore, we have shown that  in Kernel Methods Support Vector Learning,  Camhand-authored local grammars can be used to  bridge, MA: MIT Press.identify concepts that can be easily combined with  Reboul A., Sabatier P., No l-Jorand M-C.(2001) Le  word-level features (e.g. unigrams, bi-grams) for  essing systems.Our early exploration of the  application of this work for corpus analysis (U.S. State  of the Union Addresses) has produced interesting  Computers and the Humanities 33(3).results, and we expect that the continued  development of this resource will be important to the  sucTraum, D. (1993) Mental state in the TRAINS-92  diacess of future corpus analysis and human-computer  logue manager.In Working Notes of the AAAI Spring  interaction projects.Symposium on Reasoning about Mental States:  Formal Theories and Applications, pages 143-149, 1993.Acknowledgments  Menlo Park, CA: AAAI Press.Voorhees, E. & Buckland, L. (2002) The Eleventh Text  This paper was developed in part with funds from  REtrieval Conference (TREC 2002).Washington,  the U.S. Army Research Institute for the  BehavDC: Department of Commerce, National Institute of  ioral and Social Sciences under ARO contract  Standards and Technology.number DAAD 19-99-D-0046.Any opinions,  Witten, I. & Frank, E. (1999) Data Mining: Practical  findings and conclusions or recommendations  exMachine Learning Tools and Techniques with Java  pressed in this paper are those of the authors and  do not necessarily reflect the views of the  Department of the Army."
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Automatic Detection of Text Genre  Xerox Palo Alto Research Center  Department of Linguistics  3333 Coyote Hill Road  Stanford University  Stanford CA 94305-2150 USA  genres than in formal ones). In information retrieval,  genre classification could enable users to sort search  As the text databases available to users  beresults according to their immediate interests.  People who go into a bookstore or library are not usually  becomes increasingly important for  comlooking simply for information about a particular  putational linguistics as a complement to  topic, but rather have requirements of genre as well:  topical and structural principles of  classifithey are looking for scholarly articles about  hypnocation. We propose a theory of genres as  tism, novels about the French Revolution, editorials  bundles of facets, which correlate with  varabout the supercollider, and so forth.  ious surface cues, and argue that genre  deIf genre classification is so useful, why hasn't it  figtection based on surface cues is as  successured much in computational linguistics before now?  ful as detection based on deeper structural  One important reason is that, up to now, the  digitized corpora and collections which are the subject  of much CL research have been for the most part  ",0,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","Automatic Detection of Text Genre genres than in formal ones). In information retrieval,  genre classification could enable users to sort search  As the text databases available to users  beresults according to their immediate interests.People who go into a bookstore or library are not usually  becomes increasingly important for  comlooking simply for information about a particular  putational linguistics as a complement to  topic, but rather have requirements of genre as well:  topical and structural principles of  classifithey are looking for scholarly articles about  hypnocation.We propose a theory of genres as  tism, novels about the French Revolution, editorials  bundles of facets, which correlate with  varabout the supercollider, and so forth.ious surface cues, and argue that genre  deIf genre classification is so useful, why hasn't it  figtection based on surface cues is as  successured much in computational linguistics before now?ful as detection based on deeper structural  One important reason is that, up to now, the  digitized corpora and collections which are the subject  of much CL research have been for the most part"
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. ", Building a Large Annotated Corpus of English: The Penn Treebank  Mitchell P. Marcus* Beatrice Santorini t  University of Pennsylvania Northwestern University  University of Pennsylvania  1. ,1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.",1. 
" Learning Subjective Language  Theresa Wilson  University of Pittsburgh  University of Pittsburgh  Matthew Bell  University of North Carolina  University of Pittsburgh  at Asheville  New Mexico State University  Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.  1. "," Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 417-424.  Thumbs Up or Thumbs Down? Semantic Orientation Applied to  Unsupervised Classification of Reviews  Peter D. Turney  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches. It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion. With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down). The  classifisuch summary statistics. This is the motivation for  cation of a review is predicted by the  the research described here. Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs. A phrase has a positive  ing new kinds of search tools (Hearst, 1992).  semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended. The algorithm  bad associations (e.g., very cavalier ). In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output. The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994). The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997). A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive. The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ). The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations). The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view. If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.  Otherwise, the prediction is that the item is not  recommended.  ",1,"Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.The goal of this work is learning subjective language from corpora.Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.The features are also examined working together in concert.The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.1.","417-424.Thumbs Up or Thumbs Down?Semantic Orientation Applied to  Unsupervised Classification of Reviews  Peter D. Turney  Institute for Information Technology  National Research Council of Canada  case, Google1 reports about 5,000 matches.It  would be useful to know what fraction of these  matches recommend Akumal as a travel  destinaThis paper presents a simple unsupervised  tion.With an algorithm for automatically  classifylearning algorithm for classifying reviews  ing a review as thumbs up or thumbs down , it  as recommended (thumbs up) or not  recwould be possible for a search engine to report  ommended (thumbs down).The  classifisuch summary statistics.This is the motivation for  cation of a review is predicted by the  the research described here.Other potential  appliaverage semantic orientation of the  cations include recognizing flames (abusive  phrases in the review that contain  adjecnewsgroup messages) (Spertus, 1997) and  developtives or adverbs.A phrase has a positive  ing new kinds of search tools (Hearst, 1992).semantic orientation when it has good  asIn this paper, I present a simple unsupervised  sociations (e.g., subtle nuances ) and a  learning algorithm for classifying a review as  recnegative semantic orientation when it has  ommended or not recommended.The algorithm  bad associations (e.g., very cavalier ).In  takes a written review as input and produces a  this paper, the semantic orientation of a  classification as output.The first step is to use a  phrase is calculated as the mutual  inforpart-of-speech tagger to identify phrases in the  inmation between the given phrase and the  put text that contain adjectives or adverbs (Brill,  word excellent minus the mutual  1994).The second step is to estimate the semantic  information between the given phrase and  orientation of each extracted phrase  (Hatzivassithe word poor . A review is classified as  loglou & McKeown, 1997).A phrase has a  posirecommended if the average semantic  oritive semantic orientation when it has good  entation of its phrases is positive.The  alassociations (e.g., romantic ambience ) and a  gorithm achieves an average accuracy of  negative semantic orientation when it has bad  as74% when evaluated on 410 reviews from  sociations (e.g., horrific events ).The third step is  Epinions, sampled from four different  to assign the given review to a class, recommended  domains (reviews of automobiles, banks,  or not recommended, based on the average  semanmovies, and travel destinations).The  actic orientation of the phrases extracted from the  recuracy ranges from 84% for automobile  view.If the average is positive, the prediction is  reviews to 66% for movie reviews.  that the review recommends the item it discusses.Otherwise, the prediction is that the item is not  recommended."
